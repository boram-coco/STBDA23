[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Special Topics in Big Data Analysis(2023)",
    "section": "",
    "text": "guebin Choi, Professor of Statistics, Jeonbuk National University\n2nd semester, 2023\ngithub\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 10, 2023\n\n\n[STBDA2023] 14wk-60: 자전거대여 / 하이퍼파라메터 튜닝\n\n\n김보람 \n\n\n\n\nDec 10, 2023\n\n\n[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 9, 2023\n\n\n[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-57: House Prices / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-56: 타이타닉 / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-55: Medical Cost / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-53: 취업(다중공선성) / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-52: 취업(오버피팅) / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-48: 아이스크림 / 자료분석(Autogluon)\n\n\n김보람 \n\n\n\n\nDec 3, 2023\n\n\n[STBDA2023] 13wk-47: 평가지표의 계산\n\n\n김보람 \n\n\n\n\nNov 27, 2023\n\n\n[STBDA2023] 12wk-046: 평가지표의 이해\n\n\n김보람 \n\n\n\n\nNov 27, 2023\n\n\n[STBDA2023] 12wk-045: 아이스크림 / 부스팅\n\n\n김보람 \n\n\n\n\nNov 27, 2023\n\n\n[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트\n\n\n김보람 \n\n\n\n\nNov 21, 2023\n\n\n[STBDA2023] 11wk-043: 아이스크림 판매량 / 배깅\n\n\n김보람 \n\n\n\n\nNov 21, 2023\n\n\n[STBDA2023] 11wk-042: Weighted_Data / 의사결정나무 weights\n\n\n김보람 \n\n\n\n\nNov 21, 2023\n\n\n[STBDA2023] 11wk-041: Medical Cost / 의사결정나무 max_feature,random_state\n\n\n김보람 \n\n\n\n\nNov 21, 2023\n\n\n[STBDA2023] 11wk-040: Medical Cost / 의사결정나무의 시각화\n\n\n김보람 \n\n\n\n\nNov 14, 2023\n\n\n[STBDA2023] A3: 개발환경의 변천사\n\n\n최규빈 \n\n\n\n\nNov 14, 2023\n\n\n[STBDA2023] 10wk-039: 의사결정나무 Discussion\n\n\n김보람 \n\n\n\n\nNov 14, 2023\n\n\n[STBDA2023] 10wk-038: 아이스크림 – 의사결정나무 원리\n\n\n김보람 \n\n\n\n\nNov 13, 2023\n\n\n[STBDA2023] 10wk-036: 애니메이션\n\n\n김보람 \n\n\n\n\nNov 13, 2023\n\n\n[STBDA2023] 09wk-mid (ver 1.0) – 풀이업로드\n\n\n최규빈 \n\n\n\n\nNov 13, 2023\n\n\n[STBDA2023] 10wk-037: 아이스크림 – 의사결정나무, max_depth\n\n\n김보람 \n\n\n\n\nNov 9, 2023\n\n\n[STBDA2023] 09wk-mid\n\n\n김보람 \n\n\n\n\nOct 22, 2023\n\n\n[STBDA2023] 07wk-035: 아이스크림(이상치) / 의사결정나무\n\n\n김보람 \n\n\n\n\nOct 22, 2023\n\n\n[STBDA2023] 07wk-034: 취업(오버피팅) / 의사결정나무\n\n\n김보람 \n\n\n\n\nOct 22, 2023\n\n\n[STBDA2023] 07wk-033: 취업(다중공선성) / 의사결정나무\n\n\n김보람 \n\n\n\n\nOct 22, 2023\n\n\n[STBDA2023] 07wk-032: 아이스크림(교호작용) / 의사결정나무\n\n\n김보람 \n\n\n\n\nOct 22, 2023\n\n\n[STBDA2023] 07wk-031: 체중감량(교호작용) / 의사결정나무\n\n\n김보람 \n\n\n\n\nOct 21, 2023\n\n\n[STBDA2023] 07wk-030: 아이스크림(교호작용) / 선형회귀\n\n\n김보람 \n\n\n\n\nOct 21, 2023\n\n\n[STBDA2023] 07wk-029: 체중감량(교호작용) / 회귀분석\n\n\n김보람 \n\n\n\n\nOct 21, 2023\n\n\n[STBDA2023] 07wk-028: 선형모형의 적\n\n\n김보람 \n\n\n\n\nOct 20, 2023\n\n\n[STBDA2023] 07wk-027: 아이스크림(이상치) / 회귀분석\n\n\n김보람 \n\n\n\n\nOct 14, 2023\n\n\n[STBDA2023] 06wk-026: 취업+각종영어점수, LassoCV\n\n\n김보람 \n\n\n\n\nOct 14, 2023\n\n\n[STBDA2023] 06wk-025: 취업+각종영어점수, Lasso\n\n\n김보람 \n\n\n\n\nOct 14, 2023\n\n\n[STBDA2023] 06wk-024: 취업+각종영어점수, RidgeCV\n\n\n김보람 \n\n\n\n\nOct 14, 2023\n\n\n[STBDA2023] 06wk-023: 취업+각종영어점수, Ridge\n\n\n김보람 \n\n\n\n\nOct 12, 2023\n\n\n[STBDA2023] 06wk-022: 취업+각종영어점수, 다중공선성\n\n\n김보람 \n\n\n\n\nOct 8, 2023\n\n\n[STBDA2023] 05wk-021: 취업+밸런스게임, 오버피팅\n\n\n김보람 \n\n\n\n\nOct 8, 2023\n\n\n[STBDA2023] 05wk-020: StandardScaler를 이용한 전처리\n\n\n김보람 \n\n\n\n\nOct 8, 2023\n\n\n[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리\n\n\n김보람 \n\n\n\n\nSep 28, 2023\n\n\n[STBDA2023] 04wk-018: Predictor 깊은 이해 + 기호정리\n\n\n김보람 \n\n\n\n\nSep 28, 2023\n\n\n[STBDA2023] 04wk-017: 취업, 로지스틱을 더 깊게\n\n\n김보람 \n\n\n\n\nSep 28, 2023\n\n\n[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱\n\n\n김보람 \n\n\n\n\nSep 28, 2023\n\n\n[STBDA2023] 04wk-015: 결측치 처리, sklearn.impute\n\n\n김보람 \n\n\n\n\nSep 28, 2023\n\n\n[STBDA2023] 04wk-014: 결측치 시각화, msno\n\n\n김보람 \n\n\n\n\nSep 28, 2023\n\n\n[STBDA2023] 03wk-013: 타이타닉, 로지스틱\n\n\n김보람 \n\n\n\n\nSep 28, 2023\n\n\n[STBDA2023] 03wk-012: 취업, 로지스틱\n\n\n김보람 \n\n\n\n\nSep 26, 2023\n\n\n[STBDA2023] 03wk-011: Medical Cost, 회귀분석\n\n\n김보람 \n\n\n\n\nSep 25, 2023\n\n\n[STBDA2023] 03wk-010: 아이스크림(초코/바닐라), 회귀분석\n\n\n김보람 \n\n\n\n\nSep 25, 2023\n\n\n[STBDA2023] 03wk-009: 아이스크림, 회귀분석\n\n\n김보람 \n\n\n\n\nSep 17, 2023\n\n\n[STBDA2023] 02wk-008: 타이타닉, Autogluon(best_quality)\n\n\n김보람 \n\n\n\n\nSep 17, 2023\n\n\n[STBDA2023] 02wk-007: 타이타닉, Autogluon(Fsize,Dropout)\n\n\n김보람 \n\n\n\n\nSep 17, 2023\n\n\n[STBDA2023] 02wk-006: 타이타닉, Autogluon(Fsize)\n\n\n김보람 \n\n\n\n\nSep 17, 2023\n\n\n[STBDA2023] 02wk-005: 타이타닉, Autogluon\n\n\n김보람 \n\n\n\n\nSep 17, 2023\n\n\n[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n김보람 \n\n\n\n\nSep 17, 2023\n\n\n[STBDA2023] 02wk-003: 타이타닉, 첫 제출\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/13wk-51.out.html",
    "href": "posts/13wk-51.out.html",
    "title": "[STBDA2023] 13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-51.out.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-51.out.html#a.-y의-분포-xy의-관계-시각화",
    "title": "[STBDA2023] 13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='sales',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nsales\n280\n51.101886\n21.167573\n10.335207\n33.053077\n47.844021\n70.451589\n88.994376\nfloat64\n280\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for sales &gt;= 0.5\n\n\n\n\n\nFeature interaction between temp/sales in train_data"
  },
  {
    "objectID": "posts/13wk-51.out.html#target-variable-analysis",
    "href": "posts/13wk-51.out.html#target-variable-analysis",
    "title": "[STBDA2023] 13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-51.out.html#b.-중요한-설명변수",
    "href": "posts/13wk-51.out.html#b.-중요한-설명변수",
    "title": "[STBDA2023] 13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data=df_train,\n    label='sales',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231203_074133/\"\n\n\nModel Prediction for sales\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-3.549018\n-4.206044\n0.002413\n0.00081\n0.149133\n0.002413\n0.00081\n0.149133\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\ntemp\n25.164809\n1.617020\n0.000002\n5\n28.494276\n21.835342\n\n\ntype\n-0.048470\n0.059119\n0.929654\n5\n0.073258\n-0.170197\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\nsales_pred\nerror\n\n\n\n\n73\n-3.7\nvanilla\n12.432354\n24.724379\n12.292025\n\n\n191\n-0.3\nvanilla\n16.436525\n24.724379\n8.287854\n\n\n218\n14.7\nchoco\n60.178468\n52.961044\n7.217424\n\n\n166\n16.1\nchoco\n66.821367\n59.932861\n6.888506\n\n\n5\n23.2\nvanilla\n75.697957\n82.155197\n6.457240\n\n\n118\n8.3\nchoco\n45.364110\n38.923119\n6.440991\n\n\n198\n4.4\nvanilla\n24.924572\n31.039103\n6.114530\n\n\n7\n11.2\nchoco\n45.593168\n51.416027\n5.822859\n\n\n89\n25.7\nvanilla\n87.788320\n82.155197\n5.633123\n\n\n109\n2.0\nvanilla\n19.398204\n24.724379\n5.326174"
  },
  {
    "objectID": "posts/13wk-51.out.html#c.-관측치별-해석",
    "href": "posts/13wk-51.out.html#c.-관측치별-해석",
    "title": "[STBDA2023] 13wk-51: 아이스크림(type무의미) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[0]],\n    display_rows=True,\n    plot='waterfall'\n)   \n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n0\n19.4\nchoco\n64.807407\n\n\n\n\n\n\n\n\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[1]],\n    display_rows=True,\n    plot='waterfall'\n)   \n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n1\n0.9\nvanilla\n25.656697"
  },
  {
    "objectID": "posts/12wk-044.out.html",
    "href": "posts/12wk-044.out.html",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "",
    "text": "최규빈\n2023-11-21"
  },
  {
    "objectID": "posts/12wk-044.out.html#a.-baggin으로-적합",
    "href": "posts/12wk-044.out.html#a.-baggin으로-적합",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "A. Baggin으로 적합",
    "text": "A. Baggin으로 적합\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.ensemble.BaggingRegressor()\n# step3\npredictr.fit(X,y)\n# step4 -- pass \n\nBaggingRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BaggingRegressorBaggingRegressor()"
  },
  {
    "objectID": "posts/12wk-044.out.html#b.-결과-시각화",
    "href": "posts/12wk-044.out.html#b.-결과-시각화",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "B. 결과 시각화",
    "text": "B. 결과 시각화\n- 관찰: 트리들의 다양하지 않다.\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[4],\n    feature_names=X.columns,\n    max_depth=1\n);\nfig = plt.gcf()\nfig.set_dpi(200)"
  },
  {
    "objectID": "posts/12wk-044.out.html#c.-우수성-vs-다양성",
    "href": "posts/12wk-044.out.html#c.-우수성-vs-다양성",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "C. 우수성 vs 다양성",
    "text": "C. 우수성 vs 다양성\n- 다양성이 없는게 왜 문제인가?\n\n기존의 의사결정나무: 데이터에 최적화된 똑똑한 하나의 트리를 발견\nBagging: “데이터에 최적화”를 희생하고 “다양성”을 확보. 즉 똑똑한 하나의 트리대신에 모자란 여러개의 트리를 생성하고 힘을 합침.\n\n- 우수성 vs 다양성 – 우수하지만 비슷한 생각을 가진 10명의 인재 vs 평범하지만 다양한 의견을 가진 10명의 인재\n\n뭐가 좋을까요?\n김용대 교수님의 통찰! https://biz.heraldcorp.com/view.php?ud=20170802000434\n\n\nNote\n기계학습 방법론 중 앙상블이라는 방법이 있다. 앙상블이란 음악에서 여러 명의 연주자에 의한 합주 또는 합창을 의미하는데, 다양한 의견들을 조화롭게 결합하는 방법을 의미하기도 한다. 기계학습에서 앙상블이란 같은 데이터를 여러 개의 기계학습 알고리즘들이 분석하여 각자 지식을 습득한 후 이를 결합하여 새롭고 유용한 지식을 창출하는 방법이다.\n앙상블 방법론에 숨어 있는 매우 흥미롭고 이해하기 어려운 과학적 현상으로는, 앙상블의 성능을 높이기 위해서는 개별 알고리즘들의 성능보다는 알고리즘들의 다양성이 훨씬 중요하다는 것이다. 즉, 주어진 문제에 대해서 모두 비슷한 답을 주는 성능이 우수한 10개의 알고리즘보다는 성능은 좀 떨어지지만 다양한 답을 제공하는 10개의 알고리즘이 앙상블에는 더 효율적이라는 것이다. 이를 인간 사회에 적용하면, 비슷한 생각을 가진 우수한 10명의 인재보다는 다양한 의견을 가진 평범한 10명의 의견이 훨씬 유용할 수 있다는 것이다.\n\n- 요약\n\n통찰: Bagging은 의사결정나무보다 다양성을 추구하는 알고리즘이다.\n문제점: 하지만 \\({\\bf X}\\)가 고차원인 상황에서 배깅만으로는 그렇게 다양한 트리가 나오지 않는다. (모든 트리가 천편일륜적으로 흡연여부가 보험료에 미치는 영향을 우선적으로 연구한다)\n소망: 혹시 어떤 괴짜는 흡연여부를 연구하지 않고 다른 변수들을 최우선으로 연구하는 연구자가 있다면 좋겠는데.. (그러면 트리가 다양해질텐데)"
  },
  {
    "objectID": "posts/12wk-044.out.html#a.-개념",
    "href": "posts/12wk-044.out.html#a.-개념",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "A. 개념",
    "text": "A. 개념\n- RandomForeset = Bagging + max_feature=??\n\n엄밀한 수식은 아닙니다.\n\n- 슈도-알고리즘의 비교\n## Bagging \ntrees = []\nfor i in range(100):\n    tree = sklearn.tree.DecisionTreeRegressor() \n    X_sample,y_sample = boostrap(X,y) # 매순간 샘플바뀜\n    tree.fit(X_sample,y_sample) # 일부 샘플만으로 적합\n    trees.append(tree) # 학습한 나무를 저장\nyhat = ensemble(trees) # 여러개의 나무를 종합하여 하나의 예측값을 만들어냄 (합주)\n\n## RandomForeset \nforest = [] \nfor i in range(100):\n    tree = sklearn.tree.DecisionTreeRegressor(max_feature=1) # 매순간 설명변수 바뀜\n    X_sample,y_sample = boostrap(X,y) # 매순간 샘플바뀜    \n    tree.fit(X_sample,y_sample) # 일부설명변수, 일부샘플만으로 적합 \n    forest.append(tree) # 학습한 나무를 숲에 저장 \nyhat = ensemble(forest) # 여러개의 나무를 종합하여 하나의 예측값을 만들어냄 (합주)"
  },
  {
    "objectID": "posts/12wk-044.out.html#b.-일단-적합",
    "href": "posts/12wk-044.out.html#b.-일단-적합",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "B. 일단 적합",
    "text": "B. 일단 적합\n\n# step1 -- pass \n# step2 \npredictr = sklearn.ensemble.RandomForestRegressor(\n    max_depth=1,\n    max_features=1/3\n)\n# step3\npredictr.fit(X,y)\n# step4 \n\nRandomForestRegressor(max_depth=1, max_features=0.3333333333333333)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=1, max_features=0.3333333333333333)\n\n\n\n주의1: max_feature=1은 1개의 feature를 고려한다는 의미이고, max_feature=1.0은 100%의 feature를 고려한다는 의미이다.\n\n\n주의2: max_feature=1.0 이 default값이며 이 값을 사용한다면 “RandomForest = Bagging” 이다. (아래 ref 참고)\n\n\nref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
  },
  {
    "objectID": "posts/12wk-044.out.html#c.-시각화",
    "href": "posts/12wk-044.out.html#c.-시각화",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "C. 시각화",
    "text": "C. 시각화\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[5],\n    feature_names=X.columns,\n    max_depth=1\n);\n\n\n\n\n\n일반적인 배깅보다 더 다양하게 표현된다."
  },
  {
    "objectID": "posts/12wk-044.out.html#a.-random_state-추출",
    "href": "posts/12wk-044.out.html#a.-random_state-추출",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "A. random_state 추출",
    "text": "A. random_state 추출\n- 첫번째 트리 - random_state 확인\n\npredictr.estimators_[0]\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=1691233933)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=1691233933)\n\n\n- 첫번째 트리의 random_state가 저장된 곳\n\npredictr.estimators_[0].random_state\n\n1691233933\n\n\n- 각 나무들의 random_state 추출\n\nrs = [tree.random_state for tree in predictr.estimators_]"
  },
  {
    "objectID": "posts/12wk-044.out.html#b.-forest-생성",
    "href": "posts/12wk-044.out.html#b.-forest-생성",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "B. forest 생성",
    "text": "B. forest 생성\n\nmy_forest = [sklearn.tree.DecisionTreeRegressor(max_depth=1,max_features=1/3,random_state=r) for r in rs]\n\n\nmy_forest[-1]\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=2017293411)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=2017293411)\n\n\n\npredictr.estimators_[-1]\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=2017293411)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=2017293411)"
  },
  {
    "objectID": "posts/12wk-044.out.html#c.-부스트랩-샘플생성",
    "href": "posts/12wk-044.out.html#c.-부스트랩-샘플생성",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "C. 부스트랩 샘플생성",
    "text": "C. 부스트랩 샘플생성\n- 저장된 부스트랩 샘플을 확보하자. – 실패\n\npredictr.estimators_samples_\n\nAttributeError: 'RandomForestRegressor' object has no attribute 'estimators_samples_'\n\n\n\n샘플들을 재현하기 귀찮게 되어있다\n\n- 그냥 새로 만들어보자!\n\nsklearn.ensemble._forest._generate_sample_indices?\n\n\nSignature:\nsklearn.ensemble._forest._generate_sample_indices(\n    random_state,\n    n_samples,\n    n_samples_bootstrap,\n)\nDocstring: Private function used to _parallel_build_trees function.\nFile:      ~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\nType:      function\n\n\n\n\nsample = sklearn.ensemble._forest._generate_sample_indices\n\n- ex\n\nsample(random_state=0, n_samples=10, n_samples_bootstrap=5)\n\narray([5, 0, 3, 3, 7])\n\n\n\n10개 중에 중복을 포함해서 5개 랜덤으로 뽑아줌\n\n\nmy_index = [sample(random_state=r,n_samples=1338,n_samples_bootstrap=1338) for r in rs]"
  },
  {
    "objectID": "posts/12wk-044.out.html#d.-적합",
    "href": "posts/12wk-044.out.html#d.-적합",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "D. 적합",
    "text": "D. 적합\n\nfor idx,tree in zip(my_index,my_forest):\n    X_sampled, y_sampled = np.array(X)[idx], np.array(y)[idx]\n    tree.fit(X_sampled,y_sampled)"
  },
  {
    "objectID": "posts/12wk-044.out.html#e.-앙상블",
    "href": "posts/12wk-044.out.html#e.-앙상블",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "E. 앙상블",
    "text": "E. 앙상블\n(100,1388) —-&gt; ( , 1388) 만 남아야되니까 axis=0해주자\n\ndef ensemble(forest):\n    return np.stack([tree.predict(X) for tree in forest]).mean(axis=0)\n\n\nensemble(my_forest)\n\narray([16026.44823846, 12334.12631177, 12551.94966227, ...,\n       12299.65255643, 10838.15108764, 17321.74126152])\n\n\n\npredictr.predict(X)\n\narray([16026.44823846, 12334.12631177, 12551.94966227, ...,\n       12299.65255643, 10838.15108764, 17321.74126152])"
  },
  {
    "objectID": "posts/12wk-044.out.html#f.-주의",
    "href": "posts/12wk-044.out.html#f.-주의",
    "title": "[STBDA2023] 12wk-044: Medical Cost / 랜덤포레스트",
    "section": "F. 주의",
    "text": "F. 주의\n- max_depth가 깊을 경우 ensemble(my_forest)와 predictr.predict(X)의 결과가 일치하지 않을 수 있다. 이유는 트리의 성장을 멈추는 조건에서 각 leaf의 최소 샘플숫자가 기여하는데, 샘플의 가중치를 고려하느냐 하지 않느냐에 따라서 샘플숫자의 차이가 있기 때문\n- 시각화 비교 (samples가 서로다름을 파악!!)\n\nsklearn.tree.plot_tree(my_forest[0])\n\n[Text(0.5, 0.75, 'x[1] &lt;= 30.735\\nsquared_error = 136553089.754\\nsamples = 1338\\nvalue = 12821.558'),\n Text(0.25, 0.25, 'squared_error = 66788429.896\\nsamples = 697\\nvalue = 10468.675'),\n Text(0.75, 0.25, 'squared_error = 199847306.104\\nsamples = 641\\nvalue = 15379.998')]\n\n\n\n\n\n\nsklearn.tree.plot_tree(predictr.estimators_[0])\n\n[Text(0.5, 0.75, 'x[1] &lt;= 30.735\\nsquared_error = 136553089.754\\nsamples = 840\\nvalue = 12821.558'),\n Text(0.25, 0.25, 'squared_error = 66788429.896\\nsamples = 443\\nvalue = 10468.675'),\n Text(0.75, 0.25, 'squared_error = 199847306.104\\nsamples = 397\\nvalue = 15379.998')]"
  },
  {
    "objectID": "posts/11wk-040.out.html",
    "href": "posts/11wk-040.out.html",
    "title": "[STBDA2023] 11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "",
    "text": "최규빈\n2023-11-16"
  },
  {
    "objectID": "posts/11wk-040.out.html#a.-기본시각화",
    "href": "posts/11wk-040.out.html#a.-기본시각화",
    "title": "[STBDA2023] 11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "A. 기본시각화",
    "text": "A. 기본시각화\n\nsklearn.tree.plot_tree(predictr);\n\n\n\n\n\n잘 안보임"
  },
  {
    "objectID": "posts/11wk-040.out.html#b.-max_depth-조정",
    "href": "posts/11wk-040.out.html#b.-max_depth-조정",
    "title": "[STBDA2023] 11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "B. max_depth 조정",
    "text": "B. max_depth 조정\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth=0\n);"
  },
  {
    "objectID": "posts/11wk-040.out.html#c.-변수이름-추가",
    "href": "posts/11wk-040.out.html#c.-변수이름-추가",
    "title": "[STBDA2023] 11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "C. 변수이름 추가",
    "text": "C. 변수이름 추가\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth=0,\n    feature_names=X.columns\n);"
  },
  {
    "objectID": "posts/11wk-040.out.html#d.-fig-오브젝트",
    "href": "posts/11wk-040.out.html#d.-fig-오브젝트",
    "title": "[STBDA2023] 11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "D. fig 오브젝트",
    "text": "D. fig 오브젝트\n- plt.gcf()를 이용하여 fig 오브젝트 추출\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth=1,\n    feature_names=X.columns\n);\nfig = plt.gcf()\n\n\n\n\n- fig.suptitle 을 이용하여 제목을 붙일 수도 있지 않을까?\n\nfig.suptitle(\"title??\")\n\nText(0.5, 0.98, 'title??')\n\n\n\nfig\n\n\n\n\n- dpi(해상도) 조정\n\nfig.set_dpi(250)\nfig"
  },
  {
    "objectID": "posts/11wk-040.out.html#e.-matplotlib의-ax에-그리기",
    "href": "posts/11wk-040.out.html#e.-matplotlib의-ax에-그리기",
    "title": "[STBDA2023] 11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "E. matplotlib의 ax에 그리기",
    "text": "E. matplotlib의 ax에 그리기\n\nfig = plt.figure()\nax = fig.subplots(2,1)\nax[0].plot(y,y,'--')\nax[0].plot(y,predictr.predict(X),'o',alpha=0.1)\nsklearn.tree.plot_tree(predictr,feature_names=X.columns,ax=ax[1],max_depth=0);"
  },
  {
    "objectID": "posts/04wk-016.out.html",
    "href": "posts/04wk-016.out.html",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/04wk-016.out.html#a.-결측치-체크",
    "href": "posts/04wk-016.out.html#a.-결측치-체크",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "A. 결측치 체크",
    "text": "A. 결측치 체크\n- 결측치확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB"
  },
  {
    "objectID": "posts/04wk-016.out.html#b.-시각화",
    "href": "posts/04wk-016.out.html#b.-시각화",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "B. 시각화",
    "text": "B. 시각화\n\nmsno.matrix(df_train) \n# msno.bar(df_train) # 큰 의미 X \n# msno.dendrogram(df_train) # 큰 의미 X \n# msno.heatmap(df_train) # 큰 의미 X \n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/04wk-016.out.html#c.-결측치-처리",
    "href": "posts/04wk-016.out.html#c.-결측치-처리",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "C. 결측치 처리",
    "text": "C. 결측치 처리\n- 처리 전\n\ndf_train.select_dtypes(include=\"number\")\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\n27.0\n0\n0\n13.0000\n\n\n887\n888\n1\n1\n19.0\n0\n0\n30.0000\n\n\n888\n889\n0\n3\nNaN\n1\n2\n23.4500\n\n\n889\n890\n1\n1\n26.0\n0\n0\n30.0000\n\n\n890\n891\n0\n3\n32.0\n0\n0\n7.7500\n\n\n\n\n891 rows × 7 columns\n\n\n\n\ndef impute_missing(df):\n    df_imputed = df.copy()\n    df_num = df.select_dtypes(include=\"number\")\n    df_cat = df.select_dtypes(exclude=\"number\")\n    df_imputed[df_num.columns] = sklearn.impute.SimpleImputer().fit_transform(df_num) \n    df_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat) \n    return df_imputed\n\n- 처리 후\n\nimpute_missing(df_test)\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892.0\n3.0\nKelly, Mr. James\nmale\n34.50000\n0.0\n0.0\n330911\n7.8292\nB57 B59 B63 B66\nQ\n\n\n1\n893.0\n3.0\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.00000\n1.0\n0.0\n363272\n7.0000\nB57 B59 B63 B66\nS\n\n\n2\n894.0\n2.0\nMyles, Mr. Thomas Francis\nmale\n62.00000\n0.0\n0.0\n240276\n9.6875\nB57 B59 B63 B66\nQ\n\n\n3\n895.0\n3.0\nWirz, Mr. Albert\nmale\n27.00000\n0.0\n0.0\n315154\n8.6625\nB57 B59 B63 B66\nS\n\n\n4\n896.0\n3.0\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.00000\n1.0\n1.0\n3101298\n12.2875\nB57 B59 B63 B66\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305.0\n3.0\nSpector, Mr. Woolf\nmale\n30.27259\n0.0\n0.0\nA.5. 3236\n8.0500\nB57 B59 B63 B66\nS\n\n\n414\n1306.0\n1.0\nOliva y Ocana, Dona. Fermina\nfemale\n39.00000\n0.0\n0.0\nPC 17758\n108.9000\nC105\nC\n\n\n415\n1307.0\n3.0\nSaether, Mr. Simon Sivertsen\nmale\n38.50000\n0.0\n0.0\nSOTON/O.Q. 3101262\n7.2500\nB57 B59 B63 B66\nS\n\n\n416\n1308.0\n3.0\nWare, Mr. Frederick\nmale\n30.27259\n0.0\n0.0\n359309\n8.0500\nB57 B59 B63 B66\nS\n\n\n417\n1309.0\n3.0\nPeter, Master. Michael J\nmale\n30.27259\n1.0\n1.0\n2668\n22.3583\nB57 B59 B63 B66\nC\n\n\n\n\n418 rows × 11 columns"
  },
  {
    "objectID": "posts/04wk-016.out.html#a.-자료의-정리",
    "href": "posts/04wk-016.out.html#a.-자료의-정리",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "A. 자료의 정리",
    "text": "A. 자료의 정리\n\n{c:len(set(df_train[c])) for c in df_train.select_dtypes(include=\"object\").columns}\n\n{'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 148, 'Embarked': 4}\n\n\n\ncategory 형의 len 조사 똑같은게 몇개 나오는지\n\n\nX = pd.get_dummies(impute_missing(df_train).drop(['Survived','Name','Ticket','Cabin'],axis=1))\ny = impute_missing(df_train)[['Survived']]"
  },
  {
    "objectID": "posts/04wk-016.out.html#b.-predictor-생성",
    "href": "posts/04wk-016.out.html#b.-predictor-생성",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\nprdtr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/04wk-016.out.html#c.-학습",
    "href": "posts/04wk-016.out.html#c.-학습",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\nprdtr.fit(X,y)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/04wk-016.out.html#d.-예측",
    "href": "posts/04wk-016.out.html#d.-예측",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "D. 예측",
    "text": "D. 예측\n\nprdtr.predict(X)\n\narray([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n       0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n       1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n       1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n       0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n       1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n       1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n       1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n       0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 1., 0., 1., 1., 0., 0.])"
  },
  {
    "objectID": "posts/04wk-016.out.html#e.-평가",
    "href": "posts/04wk-016.out.html#e.-평가",
    "title": "[STBDA2023] 04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\nprdtr.score(X,y)\n\n0.7991021324354658"
  },
  {
    "objectID": "posts/13wk-50.out.html",
    "href": "posts/13wk-50.out.html",
    "title": "[STBDA2023] 13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-50.out.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-50.out.html#a.-y의-분포-xy의-관계-시각화",
    "title": "[STBDA2023] 13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='ice_sales',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nice_sales\n100\n13.178805\n4.337878\n8.273155\n11.296645\n12.856589\n14.294614\n50.0\nfloat64\n100\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for ice_sales &gt;= 0.5\n\n\n\n\n\nFeature interaction between temp/ice_sales in train_data"
  },
  {
    "objectID": "posts/13wk-50.out.html#target-variable-analysis",
    "href": "posts/13wk-50.out.html#target-variable-analysis",
    "title": "[STBDA2023] 13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-50.out.html#b.-중요한-설명변수",
    "href": "posts/13wk-50.out.html#b.-중요한-설명변수",
    "title": "[STBDA2023] 13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수?",
    "text": "B. 중요한 설명변수?\n\npass # 설명변수가 하나라서.."
  },
  {
    "objectID": "posts/13wk-50.out.html#c.-관측치별-해석",
    "href": "posts/13wk-50.out.html#c.-관측치별-해석",
    "title": "[STBDA2023] 13wk-50: 아이스크림(이상치) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\ndf_train.iloc[[0]]\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.0\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[0]])\n\n0    18.292442\nName: ice_sales, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[0]],\n    display_rows= True,\n    plot='waterfall'\n)\n    \n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.0"
  },
  {
    "objectID": "posts/02wk-003-타이타닉, 첫 제출.out.html",
    "href": "posts/02wk-003-타이타닉, 첫 제출.out.html",
    "title": "[STBDA2023] 02wk-003: 타이타닉, 첫 제출",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/02wk-003-타이타닉, 첫 제출.out.html#a.-test-답을-모름-제출해야-알-수-있음",
    "href": "posts/02wk-003-타이타닉, 첫 제출.out.html#a.-test-답을-모름-제출해야-알-수-있음",
    "title": "[STBDA2023] 02wk-003: 타이타닉, 첫 제출",
    "section": "A. test – 답을 모름, 제출해야 알 수 있음",
    "text": "A. test – 답을 모름, 제출해야 알 수 있음\n- 제출결과는 리더보드에서 확인할 수 있음."
  },
  {
    "objectID": "posts/02wk-003-타이타닉, 첫 제출.out.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "href": "posts/02wk-003-타이타닉, 첫 제출.out.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "title": "[STBDA2023] 02wk-003: 타이타닉, 첫 제출",
    "section": "B. train – 스스로 풀어보고 채점할 수 있음",
    "text": "B. train – 스스로 풀어보고 채점할 수 있음\n- 캐글에서 code \\(\\to\\) New Notebook 클릭\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\ntr=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntst=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n# 예비학습 – accuracy의 계산\n\nimport pandas as pd \n\n\ndf = pd.DataFrame({'Surv':[1,0,1,1,0],'Sex':['f','m','f','m','m']})\n\n- Surv 열의 선택\n\ndf.Surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: Surv, dtype: int64\n\n\n- Sex 열의 선택\n\ndf.Sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: Sex, dtype: object\n\n\n- Sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\n(df.Sex == 'f')*1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: Sex, dtype: int64\n\n\n- 결과를 정리하면 아래와 같음\n\npd.DataFrame({'real': df.Surv, 'estimate': (df.Sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n- accuracy를 손으로 계산하면 \\(\\frac{4}{5}=0.8\\).\n- 컴퓨터로 accuracy를 계산한다면\n\n(df.Surv == (df.Sex == 'f')*1).sum()/5 # 방법1\n\n0.8\n\n\n\n(df.Surv == (df.Sex == 'f')*1).mean() # 방법2\n\n0.8\n\n\n#\n- 실제자료의 accuracy를 구해보자.\n\ntr = pd.read_csv('~/Desktop/titanic/train.csv')\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()\n\n0.7867564534231201\n\n\n\ntr[tr.Sex == 'female'].Survived.mean()\n\n0.7420382165605095"
  },
  {
    "objectID": "posts/11wk-042.out.html",
    "href": "posts/11wk-042.out.html",
    "title": "[STBDA2023] 11wk-042: Weighted_Data / 의사결정나무 weights",
    "section": "",
    "text": "11wk-042: Weighted_Data / 의사결정나무 weights\n최규빈\n2023-11-16\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-yI1U6as_CdfTqS-6hPhXdr&si=W5npg8aaXMcs0WVz\n\n\n2. Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.tree\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. weights\n# 예제1 – 아래의 예제를 생각하자.\n\nX = np.array([1,2,3,4,5,6,7]).reshape(-1,1)\ny = np.array([10,11,12,20,21,22,23])\n\n\nplt.plot(X,y,'o')\n\n\n\n\n- 어디에서 분기점을 나누는게 좋을까? 당연히 3.5 정도..\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npredictr.fit(X,y)\nsklearn.tree.plot_tree(predictr);\n\n\n\n\n#\n# 예제2 – 아래의 예제를 생각하자.\n\nX = np.array([1]*5000+[2]*5000+[3,4,5,6,7]).reshape(-1,1)\ny = np.array([10]*5000+[11]*5000+[12,20,21,22,23])\n\n\nplt.plot(X,y,'o',alpha=0.1)\n\n\n\n\n- 분기점은 어디에 나누는게 좋을까? 이 경우는 1.5근처에서 나누는게 합리적으로 보임\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n\npredictr.fit(X,y)\n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n\nsklearn.tree.plot_tree(\n    predictr\n)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 1.5\\nsquared_error = 0.299\\nsamples = 10005\\nvalue = 10.505'),\n Text(0.25, 0.25, 'squared_error = 0.0\\nsamples = 5000\\nvalue = 10.0'),\n Text(0.75, 0.25, 'squared_error = 0.089\\nsamples = 5005\\nvalue = 11.009')]\n\n\n\n\n\n#\n# 예제3 – 다시 아래의 예제를 생각하자.\n\nX = np.array([1,2,3,4,5,6,7]).reshape(-1,1)\ny = np.array([10,11,12,20,21,22,23])\n\n\nplt.plot(X,y,'o')\nplt.plot(X[:2],y[:2],'o')\n\n\n\n\n\n주황색점들을 잘 맞추는 것이 파란색점들을 잘 맞추는 것보다 5000배정도 중요하다고 상상하자.\n\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npredictr.fit(X,y,sample_weight=[5000,5000,1,1,1,1,1])\nsklearn.tree.plot_tree(predictr);\n\n\n\n\n#"
  },
  {
    "objectID": "posts/13wk-49.out.html",
    "href": "posts/13wk-49.out.html",
    "title": "[STBDA2023] 13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-49.out.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-49.out.html#a.-y의-분포-xy의-관계-시각화",
    "title": "[STBDA2023] 13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='height',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nheight\n280\n174.605431\n9.430102\n148.975298\n167.572671\n175.186487\n181.132612\n195.797169\nfloat64\n280\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for height &gt;= 0.5\n\n\n\n\n\nFeature interaction between weight/height in train_data\n\n\n\n\n\nFeature interaction between sex/height in train_data"
  },
  {
    "objectID": "posts/13wk-49.out.html#target-variable-analysis",
    "href": "posts/13wk-49.out.html#target-variable-analysis",
    "title": "[STBDA2023] 13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-49.out.html#b.-중요한-설명변수",
    "href": "posts/13wk-49.out.html#b.-중요한-설명변수",
    "title": "[STBDA2023] 13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수?",
    "text": "B. 중요한 설명변수?\n\n중요하다고 생각하는 설명변수를 랭킹을 통해 알려줌\n\n\nauto.quick_fit(\n    train_data=df_train,\n    label='height',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231203_071354/\"\n\n\nModel Prediction for height\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-3.441217\n-3.881789\n0.006712\n0.001602\n0.306659\n0.006712\n0.001602\n0.306659\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nweight\n9.433162\n0.468715\n7.290537e-07\n5\n10.398253\n8.468071\n\n\nsex\n1.710680\n0.464422\n5.924364e-04\n5\n2.666932\n0.754427\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nweight\nsex\nheight\nheight_pred\nerror\n\n\n\n\n208\nNaN\nfemale\n159.027430\n168.600342\n9.572911\n\n\n263\n54.145913\nfemale\n165.791300\n173.811340\n8.020041\n\n\n146\n76.642564\nmale\n175.011295\n182.954391\n7.943097\n\n\n228\n56.473758\nfemale\n165.962051\n173.811340\n7.849289\n\n\n92\n51.018586\nfemale\n160.851952\n167.398270\n6.546318\n\n\n198\nNaN\nmale\n173.915293\n180.355164\n6.439870\n\n\n157\n46.214566\nfemale\n154.289882\n160.576065\n6.286183\n\n\n106\n69.667856\nmale\n179.665916\n173.611328\n6.054588\n\n\n118\n48.711791\nfemale\n168.305739\n162.763138\n5.542602\n\n\n166\n77.068343\nmale\n177.439194\n182.954391\n5.515197"
  },
  {
    "objectID": "posts/13wk-49.out.html#c.-관측치별-해석",
    "href": "posts/13wk-49.out.html#c.-관측치별-해석",
    "title": "[STBDA2023] 13wk-49: 키와 몸무게 (결측치, 성별교호작용) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\n0번 obs\n- 0번 observation\n\ndf_train.iloc[[0]]\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n0\n71.169041\nmale\n180.906857\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[0]])\n\n0    178.642868\nName: height, dtype: float32\n\n\n\n왜 178.642868로 예측했을까?\n\n- 해석\n\nauto.explain_rows(\n    train_data= df_train,\n    model = predictr,\n    rows = df_train.iloc[[0]],\n    display_rows= True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n0\n71.169041\nmale\n180.906857\n\n\n\n\n\n\n\n\n\n\n\n왜 178.642868로 예측했을까?\n일단은 평균값인 173.115에 적합\nsex을 고려하여 +2.1\nweight을 고려하여 +3.43\n최종적으로는 178.643\n\n\n\n208번 obs\n- 208번 observation\n\ndf_train.iloc[[208]]\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n208\nNaN\nfemale\n159.02743\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[208]])\n\n208    168.788971\nName: height, dtype: float32\n\n\n\n왜 168.788971로 예측했을까?\n\n- 해석\n\nauto.explain_rows(\n    train_data= df_train,\n    model = predictr,\n    rows=df_train.iloc[[208]],\n    display_rows= True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n208\nNaN\nfemale\n159.02743\n\n\n\n\n\n\n\n\n\n\n\n왜 178.642868로 예측했을까?\n일단은 평균값인 173.115에 적합\nsex을 고려하여 -4.57\nweight=nan을 고려하여 +0.25\n최종적으로는 168.789\n\n\n결측치를 그냥 하나의 관측치로 해석함 (nan이라는 값을 가지고 있다고 해석하는 느낌)\n\n\n이게 왜 가능한가? (이런걸 가능하게 하는 테크닉이 많음, nan을 -9999로 처리하고 트리를 돌린다고 상상)\n\n\n\n211번 obs\n- 211번 observation\n\ndf_train.iloc[[211]]\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n211\nNaN\nfemale\n165.076235\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[211]])\n\n211    168.788971\nName: height, dtype: float32\n\n\n- 해석\n\nauto.explain_rows(\n    train_data= df_train,\n    model = predictr,\n    rows=df_train.iloc[[211]],\n    display_rows= True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n211\nNaN\nfemale\n165.076235\n\n\n\n\n\n\n\n\n\n\n- 우리가 생각한 현실적인 적합은 사실 이러함\n\ndf_train[df_train.sex =='female'].weight.mean()\n\n49.567060917121516\n\n\n\nonerow = df_train.iloc[[211]].copy()\nonerow.weight = 49.567060917121516\nonerow\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n211\n49.567061\nfemale\n165.076235\n\n\n\n\n\n\n\n\npredictr.predict(onerow)\n\n211    164.488647\nName: height, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data= df_train,\n    model = predictr,\n    rows=onerow,\n    display_rows= True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n211\n49.567061\nfemale\n165.076235\n\n\n\n\n\n\n\n\n\n\n\n\n198번 obs\n- 198번 observation\n\ndf_train.iloc[[198]]\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n198\nNaN\nmale\n173.915293\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[198]])\n\n198    178.869781\nName: height, dtype: float32\n\n\n- 해석\n\nauto.explain_rows(\n    train_data= df_train,\n    model = predictr,\n    rows=df_train.iloc[[198]],\n    display_rows= True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n198\nNaN\nmale\n173.915293"
  },
  {
    "objectID": "posts/14wk-59.out.html",
    "href": "posts/14wk-59.out.html",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/14wk-59.out.html#a.-적합",
    "href": "posts/14wk-59.out.html#a.-적합",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "A. 적합",
    "text": "A. 적합\n\nset(df_train.columns) - set(df_test.columns)\n\n{'casual', 'count', 'registered'}\n\n\n\nsampleSubmission.head()\n\n\n\n\n\n\n\n\ndatetime\ncount\n\n\n\n\n0\n2011-01-20 00:00:00\n0\n\n\n1\n2011-01-20 01:00:00\n0\n\n\n2\n2011-01-20 02:00:00\n0\n\n\n3\n2011-01-20 03:00:00\n0\n\n\n4\n2011-01-20 04:00:00\n0\n\n\n\n\n\n\n\n\ncount를 맞추자!\n\n- 데이터 전처리(step1)\n\ndf_train_featured = df_train.copy()\ndf_test_featured = df_test.copy()\n#---# \ndf_train_featured = df_train_featured.drop(['casual','registered'],axis=1)\n\n- step2~4\n\n# step1 -- pass \n# step2\npredictr = TabularPredictor(label='count')\n# step3\npredictr.fit(df_train_featured)\n# step4 \nyhat = predictr.predict(df_train_featured)\nyyhat = predictr.predict(df_test_featured)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231210_063818/\"\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231210_063818/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2\nDisk Space Avail:   651.50 GB / 982.82 GB (66.3%)\nTrain Data Rows:    10886\nTrain Data Columns: 9\nLabel Column: count\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n    Label info (max, min, mean, stddev): (977, 1, 191.57413, 181.14445)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    128823.96 MB\n    Train Data (Original)  Memory Usage: 1.52 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 2 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting DatetimeFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])                      : 3 | ['temp', 'atemp', 'windspeed']\n        ('int', [])                        : 5 | ['season', 'holiday', 'workingday', 'weather', 'humidity']\n        ('object', ['datetime_as_object']) : 1 | ['datetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n        ('int', [])                  : 3 | ['season', 'weather', 'humidity']\n        ('int', ['bool'])            : 2 | ['holiday', 'workingday']\n        ('int', ['datetime_as_int']) : 5 | ['datetime', 'datetime.year', 'datetime.month', 'datetime.day', 'datetime.dayofweek']\n    0.0s = Fit runtime\n    9 features in original data used to generate 13 features in processed data.\n    Train Data (Processed) Memory Usage: 0.98 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.05s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.1, Train Rows: 9797, Val Rows: 1089\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models ...\nFitting model: KNeighborsUnif ...\n    -109.7394    = Validation score   (-root_mean_squared_error)\n    0.08s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    -92.4421     = Validation score   (-root_mean_squared_error)\n    0.04s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMXT ...\n    -135.958     = Validation score   (-root_mean_squared_error)\n    0.83s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    -134.0804    = Validation score   (-root_mean_squared_error)\n    0.59s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE ...\n    -122.0128    = Validation score   (-root_mean_squared_error)\n    1.43s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: CatBoost ...\n    -134.2362    = Validation score   (-root_mean_squared_error)\n    2.65s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -128.4294    = Validation score   (-root_mean_squared_error)\n    0.96s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    -136.6546    = Validation score   (-root_mean_squared_error)\n    7.3s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    -135.0751    = Validation score   (-root_mean_squared_error)\n    0.73s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    -138.9909    = Validation score   (-root_mean_squared_error)\n    23.89s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    -132.1736    = Validation score   (-root_mean_squared_error)\n    0.84s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    -92.4421     = Validation score   (-root_mean_squared_error)\n    0.17s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 40.62s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231210_063818/\")\n\n\n[1000]  valid_set's rmse: 136.065\n\n\n- 적합한것을 관찰해보자.\n\nplt.plot(df_train['count'][:300],'--',label='y')\nplt.plot(yhat[:300],alpha=0.5,lw=4,label='yhat')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2d0dede160&gt;\n\n\n\n\n\n\n잘 맞추는데?.. (수상할 정도로)"
  },
  {
    "objectID": "posts/14wk-59.out.html#b.-제출",
    "href": "posts/14wk-59.out.html#b.-제출",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "B. 제출",
    "text": "B. 제출\n- 제출\n\nsampleSubmission['count'] = yyhat \nsampleSubmission.to_csv(\"submission.csv\",index=False)\n!kaggle competitions submit -c bike-sharing-demand -f submission.csv -m \"Message\"\n!rm submission.csv\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|████████████████████████████████████████| 188k/188k [00:02&lt;00:00, 83.1kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n- 순위확인\n\n\n3171/3242 # 냈다면\n\n0.9780999383096853\n\n\n- yyhat을 살펴봄\n\nplt.plot(yyhat[:300])\n\n\n\n\n\nyhat이랑 모양자체가 너무 다름"
  },
  {
    "objectID": "posts/14wk-59.out.html#a.-피처엔지니어링",
    "href": "posts/14wk-59.out.html#a.-피처엔지니어링",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "A. 피처엔지니어링",
    "text": "A. 피처엔지니어링\n- 이미 시계열로 적합할 의지가 없으므로 datetime열은 삭제하는게 좋겠음. (인덱스의 역할만 하는 쓸모없는 변수)\n\ndf_train_featured = df_train.copy()\ndf_test_featured = df_test.copy()\n#----# \ndf_train_featured = df_train_featured.drop(['casual','registered'],axis=1)\n#--#\ndf_train_featured = df_train_featured.drop(['datetime'],axis=1)\ndf_test_featured = df_test_featured.drop(['datetime'],axis=1)"
  },
  {
    "objectID": "posts/14wk-59.out.html#b.-적합",
    "href": "posts/14wk-59.out.html#b.-적합",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "B. 적합",
    "text": "B. 적합\n- 조용히 적합 (verbosity=False)\n\n# step1 -- pass \n# step2\npredictr = TabularPredictor(label='count',verbosity=False)\n# step3\npredictr.fit(df_train_featured)\n# step4 \nyhat = predictr.predict(df_train_featured)\nyyhat = predictr.predict(df_test_featured)\n\n- 적합결과 시각화\n\nplt.plot(df_train['count'][:300],'--',label='y')\nplt.plot(yhat[:300],alpha=0.5,lw=4,label='yhat')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2d0c215e20&gt;\n\n\n\n\n\n\n오히려 좋아\n\n\nplt.plot(yyhat[:300],alpha=0.5,lw=4,color='C1')\n\n\n\n\n- 더 예쁜 시각화\n\ndf = pd.concat([\n    df_train.assign(count_hat = yhat, dataset_type = 'train'),\n    df_test.assign(count_hat = yyhat, dataset_type = 'test')\n])\ndf['datetime'] = pd.to_datetime(df['datetime'])\nsns.lineplot(\n    df.sort_values('datetime')[:(24*28)],\n    x='datetime',y='count',\n    hue='dataset_type',\n    linestyle='--',\n    lw=0.8\n)\nsns.lineplot(\n    df.sort_values('datetime')[:(24*28)],\n    x='datetime',y='count_hat',\n    hue='dataset_type',\n    alpha=0.5,\n    lw=3\n)\nfig = plt.gcf()\nfig.set_size_inches(8,2)  # x축 y축 너비조정\nplt.xticks(rotation=15);  # 글자 기울기 조정\n\n\n\n\n시각화코드를 함수로 구현\n\ndef plot(yhat,yyhat):\n    df = pd.concat([\n        df_train.assign(count_hat = yhat, dataset_type = 'train'),\n        df_test.assign(count_hat = yyhat, dataset_type = 'test')\n    ])\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    sns.lineplot(\n        df.sort_values('datetime')[:(24*28)],\n        x='datetime',y='count',\n        hue='dataset_type',\n        linestyle='--',\n        lw=0.8\n    )\n    sns.lineplot(\n        df.sort_values('datetime')[:(24*28)],\n        x='datetime',y='count_hat',\n        hue='dataset_type',\n        alpha=0.5,\n        lw=3\n    )\n    fig = plt.gcf()\n    fig.set_size_inches(8,2)\n    plt.xticks(rotation=15); \n    fig.show()\n\n\nplot(yhat,yyhat)"
  },
  {
    "objectID": "posts/14wk-59.out.html#c.-제출",
    "href": "posts/14wk-59.out.html#c.-제출",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "C. 제출",
    "text": "C. 제출\n- 제출\n\nsampleSubmission['count'] = yyhat \nsampleSubmission.to_csv(\"submission.csv\",index=False)\n!kaggle competitions submit -c bike-sharing-demand -f submission.csv -m \"Message\"\n!rm submission.csv\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 188k/188k [00:01&lt;00:00, 102kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n- 순위확인\n\n\n2951/3242 # 냈다면\n\n0.9102405922270204"
  },
  {
    "objectID": "posts/14wk-59.out.html#d.-pipeline-automation-싹다-함수로-구현",
    "href": "posts/14wk-59.out.html#d.-pipeline-automation-싹다-함수로-구현",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "D. Pipeline Automation – 싹다 함수로 구현",
    "text": "D. Pipeline Automation – 싹다 함수로 구현\n\ndef fit_predict(df_train_featured, df_test_featured):\n    # step1 -- pass \n    # step2\n    predictr = TabularPredictor(label='count',verbosity=False)\n    # step3\n    predictr.fit(df_train_featured)\n    # step4 \n    yhat = predictr.predict(df_train_featured)\n    yyhat = predictr.predict(df_test_featured)\n    # display\n    display(predictr.leaderboard())\n    return yhat, yyhat \n\n\ndef submit(yyhat):\n    sampleSubmission['count'] = yyhat \n    sampleSubmission['count'] = sampleSubmission['count'].apply(lambda x: x if x&gt;0 else 0)\n    sampleSubmission.to_csv(\"submission.csv\",index=False)\n    !kaggle competitions submit -c bike-sharing-demand -f submission.csv -m \"Message\"\n    !rm submission.csv\n\n\ndef auto(df_train_featured, df_test_featured):\n    yhat,yyhat = fit_predict(df_train_featured, df_test_featured)\n    plot(yhat,yyhat)\n    submit(yyhat)\n\n\nauto(df_train_featured,df_test_featured)\n\n                  model   score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2 -146.942817       0.093126  19.834418                0.000256           0.173803            2       True         12\n1              CatBoost -148.454154       0.001523   0.862499                0.001523           0.862499            1       True          6\n2       NeuralNetFastAI -148.887325       0.009749   5.793004                0.009749           5.793004            1       True          8\n3         LightGBMLarge -149.213280       0.002825   0.750371                0.002825           0.750371            1       True         11\n4            LightGBMXT -149.261116       0.011158   0.808311                0.011158           0.808311            1       True          3\n5               XGBoost -149.642096       0.006088   0.241403                0.006088           0.241403            1       True          9\n6              LightGBM -149.739171       0.001782   0.447107                0.001782           0.447107            1       True          4\n7        NeuralNetTorch -151.552860       0.005795   9.768490                0.005795           9.768490            1       True         10\n8         ExtraTreesMSE -156.627917       0.030079   0.714512                0.030079           0.714512            1       True          7\n9       RandomForestMSE -157.475877       0.031739   0.963428                0.031739           0.963428            1       True          5\n10       KNeighborsUnif -165.533975       0.007050   0.052896                0.007050           0.052896            1       True          1\n11       KNeighborsDist -176.146340       0.006670   0.050870                0.006670           0.050870            1       True          2\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|████████████████████████████████████████| 243k/243k [00:02&lt;00:00, 98.7kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n-146.942817\n0.093126\n19.834418\n0.000256\n0.173803\n2\nTrue\n12\n\n\n1\nCatBoost\n-148.454154\n0.001523\n0.862499\n0.001523\n0.862499\n1\nTrue\n6\n\n\n2\nNeuralNetFastAI\n-148.887325\n0.009749\n5.793004\n0.009749\n5.793004\n1\nTrue\n8\n\n\n3\nLightGBMLarge\n-149.213280\n0.002825\n0.750371\n0.002825\n0.750371\n1\nTrue\n11\n\n\n4\nLightGBMXT\n-149.261116\n0.011158\n0.808311\n0.011158\n0.808311\n1\nTrue\n3\n\n\n5\nXGBoost\n-149.642096\n0.006088\n0.241403\n0.006088\n0.241403\n1\nTrue\n9\n\n\n6\nLightGBM\n-149.739171\n0.001782\n0.447107\n0.001782\n0.447107\n1\nTrue\n4\n\n\n7\nNeuralNetTorch\n-151.552860\n0.005795\n9.768490\n0.005795\n9.768490\n1\nTrue\n10\n\n\n8\nExtraTreesMSE\n-156.627917\n0.030079\n0.714512\n0.030079\n0.714512\n1\nTrue\n7\n\n\n9\nRandomForestMSE\n-157.475877\n0.031739\n0.963428\n0.031739\n0.963428\n1\nTrue\n5\n\n\n10\nKNeighborsUnif\n-165.533975\n0.007050\n0.052896\n0.007050\n0.052896\n1\nTrue\n1\n\n\n11\nKNeighborsDist\n-176.146340\n0.006670\n0.050870\n0.006670\n0.050870\n1\nTrue\n2"
  },
  {
    "objectID": "posts/14wk-59.out.html#a.-시간정보-피처엔지니어링",
    "href": "posts/14wk-59.out.html#a.-시간정보-피처엔지니어링",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "A. 시간정보 피처엔지니어링",
    "text": "A. 시간정보 피처엔지니어링\n\ndf_train_featured = df_train.copy()\ndf_test_featured = df_test.copy()\n#----# \ndf_train_featured = df_train_featured.drop(['casual','registered'],axis=1)\n#--#\ndf_train_featured['hour'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_test_featured['hour'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_train_featured['weekday'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.weekday\ndf_test_featured['weekday'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.weekday\n#--#\ndf_train_featured = df_train_featured.drop(['datetime'],axis=1)\ndf_test_featured = df_test_featured.drop(['datetime'],axis=1)"
  },
  {
    "objectID": "posts/14wk-59.out.html#b.-적합---시각화---제출",
    "href": "posts/14wk-59.out.html#b.-적합---시각화---제출",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "B. 적합 -> 시각화 -> 제출",
    "text": "B. 적합 -&gt; 시각화 -&gt; 제출\n\nauto(df_train_featured,df_test_featured)\n\n                  model   score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2  -59.456941       0.102968  64.794636                0.000255           0.167149            2       True         12\n1         LightGBMLarge  -60.899261       0.007749   1.221596                0.007749           1.221596            1       True         11\n2              CatBoost  -61.268467       0.003084  24.933573                0.003084          24.933573            1       True          6\n3              LightGBM  -61.447456       0.021623   1.687860                0.021623           1.687860            1       True          4\n4               XGBoost  -61.749260       0.007222   0.476288                0.007222           0.476288            1       True          9\n5            LightGBMXT  -62.400538       0.057122   3.474121                0.057122           3.474121            1       True          3\n6       RandomForestMSE  -67.993149       0.038693   1.092855                0.038693           1.092855            1       True          5\n7         ExtraTreesMSE  -68.246627       0.034320   0.640087                0.034320           0.640087            1       True          7\n8        NeuralNetTorch  -68.927865       0.005912  32.834049                0.005912          32.834049            1       True         10\n9       NeuralNetFastAI  -70.979733       0.010306   7.108909                0.010306           7.108909            1       True          8\n10       KNeighborsDist -115.023130       0.008851   0.055676                0.008851           0.055676            1       True          2\n11       KNeighborsUnif -117.802477       0.008756   0.056952                0.008756           0.056952            1       True          1\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 240k/240k [00:01&lt;00:00, 124kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n-59.456941\n0.102968\n64.794636\n0.000255\n0.167149\n2\nTrue\n12\n\n\n1\nLightGBMLarge\n-60.899261\n0.007749\n1.221596\n0.007749\n1.221596\n1\nTrue\n11\n\n\n2\nCatBoost\n-61.268467\n0.003084\n24.933573\n0.003084\n24.933573\n1\nTrue\n6\n\n\n3\nLightGBM\n-61.447456\n0.021623\n1.687860\n0.021623\n1.687860\n1\nTrue\n4\n\n\n4\nXGBoost\n-61.749260\n0.007222\n0.476288\n0.007222\n0.476288\n1\nTrue\n9\n\n\n5\nLightGBMXT\n-62.400538\n0.057122\n3.474121\n0.057122\n3.474121\n1\nTrue\n3\n\n\n6\nRandomForestMSE\n-67.993149\n0.038693\n1.092855\n0.038693\n1.092855\n1\nTrue\n5\n\n\n7\nExtraTreesMSE\n-68.246627\n0.034320\n0.640087\n0.034320\n0.640087\n1\nTrue\n7\n\n\n8\nNeuralNetTorch\n-68.927865\n0.005912\n32.834049\n0.005912\n32.834049\n1\nTrue\n10\n\n\n9\nNeuralNetFastAI\n-70.979733\n0.010306\n7.108909\n0.010306\n7.108909\n1\nTrue\n8\n\n\n10\nKNeighborsDist\n-115.023130\n0.008851\n0.055676\n0.008851\n0.055676\n1\nTrue\n2\n\n\n11\nKNeighborsUnif\n-117.802477\n0.008756\n0.056952\n0.008756\n0.056952\n1\nTrue\n1"
  },
  {
    "objectID": "posts/14wk-59.out.html#a.-step1-관련없는-변수-삭제",
    "href": "posts/14wk-59.out.html#a.-step1-관련없는-변수-삭제",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "A. Step1 – 관련없는 변수 삭제",
    "text": "A. Step1 – 관련없는 변수 삭제\n- 지금까지 수행한 피처엔지니어링\n\ndf_train_featured = df_train.copy()\ndf_test_featured = df_test.copy()\n#----# \ndf_train_featured = df_train_featured.drop(['casual','registered'],axis=1)\n#--#\ndf_train_featured['hour'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_test_featured['hour'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_train_featured['weekday'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.weekday\ndf_test_featured['weekday'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.weekday\n#--#\ndf_train_featured = df_train_featured.drop(['datetime'],axis=1)\ndf_test_featured = df_test_featured.drop(['datetime'],axis=1)\n\n\nsns.heatmap(df_train_featured.set_index('count').reset_index().corr(),vmin=-1,cmap='bwr')\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\nholiday, workingday, weekday는 count와 관련이 없어보인다. –&gt; 제외하고 분석\n\n\nauto(\n    df_train_featured.drop(['holiday', 'workingday', 'weekday'],axis=1),\n    df_test_featured.drop(['holiday', 'workingday', 'weekday'],axis=1)\n)\n\n                  model   score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2 -100.669244       0.052277  41.755835                0.000254           0.174775            2       True         12\n1              CatBoost -101.264453       0.001435   1.276704                0.001435           1.276704            1       True          6\n2            LightGBMXT -102.499627       0.011276   0.884664                0.011276           0.884664            1       True          3\n3         LightGBMLarge -102.767101       0.002414   0.788208                0.002414           0.788208            1       True         11\n4               XGBoost -103.481823       0.002760   0.260296                0.002760           0.260296            1       True          9\n5              LightGBM -103.565687       0.004105   0.565680                0.004105           0.565680            1       True          4\n6        NeuralNetTorch -104.992742       0.005868  30.550567                0.005868          30.550567            1       True         10\n7       NeuralNetFastAI -105.342264       0.009668   7.785035                0.009668           7.785035            1       True          8\n8       RandomForestMSE -106.378229       0.032638   1.180547                0.032638           1.180547            1       True          5\n9         ExtraTreesMSE -106.730503       0.039648   0.762568                0.039648           0.762568            1       True          7\n10       KNeighborsUnif -128.806002       0.007811   0.067189                0.007811           0.067189            1       True          1\n11       KNeighborsDist -128.946333       0.007767   0.064981                0.007767           0.064981            1       True          2\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 242k/242k [00:02&lt;00:00, 113kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n-100.669244\n0.052277\n41.755835\n0.000254\n0.174775\n2\nTrue\n12\n\n\n1\nCatBoost\n-101.264453\n0.001435\n1.276704\n0.001435\n1.276704\n1\nTrue\n6\n\n\n2\nLightGBMXT\n-102.499627\n0.011276\n0.884664\n0.011276\n0.884664\n1\nTrue\n3\n\n\n3\nLightGBMLarge\n-102.767101\n0.002414\n0.788208\n0.002414\n0.788208\n1\nTrue\n11\n\n\n4\nXGBoost\n-103.481823\n0.002760\n0.260296\n0.002760\n0.260296\n1\nTrue\n9\n\n\n5\nLightGBM\n-103.565687\n0.004105\n0.565680\n0.004105\n0.565680\n1\nTrue\n4\n\n\n6\nNeuralNetTorch\n-104.992742\n0.005868\n30.550567\n0.005868\n30.550567\n1\nTrue\n10\n\n\n7\nNeuralNetFastAI\n-105.342264\n0.009668\n7.785035\n0.009668\n7.785035\n1\nTrue\n8\n\n\n8\nRandomForestMSE\n-106.378229\n0.032638\n1.180547\n0.032638\n1.180547\n1\nTrue\n5\n\n\n9\nExtraTreesMSE\n-106.730503\n0.039648\n0.762568\n0.039648\n0.762568\n1\nTrue\n7\n\n\n10\nKNeighborsUnif\n-128.806002\n0.007811\n0.067189\n0.007811\n0.067189\n1\nTrue\n1\n\n\n11\nKNeighborsDist\n-128.946333\n0.007767\n0.064981\n0.007767\n0.064981\n1\nTrue\n2\n\n\n\n\n\n\n\n\n\n\n\n\n안좋아졌음..\n\n- 왜 이런 결과가 나오는가?\n- 참고: https://guebin.github.io/DV2023/posts/02wk-1.html\n\nsex =  np.array([0,0,0,0]*100+[0] + [1]+[1,1,1,1]*100 + [2]*401)\nsurv = np.array([0,0,0,0]*100+[1] + [0]+[1,1,1,1]*100 + [0]*401) \nsurv_conti = surv + np.random.randn(len(surv))*0.1\n_df = pd.DataFrame({'sex':sex, 'surv':surv, 'surv_conti':surv_conti})\n_df.corr()\n\n\n\n\n\n\n\n\nsex\nsurv\nsurv_conti\n\n\n\n\nsex\n1.00000\n-0.002160\n0.006710\n\n\nsurv\n-0.00216\n1.000000\n0.978614\n\n\nsurv_conti\n0.00671\n0.978614\n1.000000\n\n\n\n\n\n\n\n\nsns.scatterplot(_df, x='sex',y='surv_conti',alpha=0.5)\n\n&lt;AxesSubplot: xlabel='sex', ylabel='surv_conti'&gt;\n\n\n\n\n\n\nsurv_conti.mean()\n\n0.33106272309886425\n\n\n\nsns.heatmap(_df.corr(),cmap='bwr',vmin=-1)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n- 우리의 예제\n\nsns.scatterplot(\n    df_train_featured,\n    x='holiday',\n    y='count',\n    alpha=0.1\n)\n\n&lt;AxesSubplot: xlabel='holiday', ylabel='count'&gt;\n\n\n\n\n\n\nsns.scatterplot(\n    df_train_featured,\n    x='weekday',\n    y='count',\n    alpha=0.1\n)\n\n&lt;AxesSubplot: xlabel='weekday', ylabel='count'&gt;\n\n\n\n\n\n\nsns.scatterplot(\n    df_train_featured,\n    x='workingday',\n    y='count',\n    alpha=0.1\n)\n\n&lt;AxesSubplot: xlabel='workingday', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/14wk-59.out.html#b.-step2-atemp-혹은-temp-삭제",
    "href": "posts/14wk-59.out.html#b.-step2-atemp-혹은-temp-삭제",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "B. Step2 – atemp 혹은 temp 삭제",
    "text": "B. Step2 – atemp 혹은 temp 삭제\n- 지금까지 한 피처엔지니어링\n\ndf_train_featured = df_train.copy()\ndf_test_featured = df_test.copy()\n#----# \ndf_train_featured = df_train_featured.drop(['casual','registered'],axis=1)\n#--#\ndf_train_featured['hour'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_test_featured['hour'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_train_featured['weekday'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.weekday\ndf_test_featured['weekday'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.weekday\n#--#\ndf_train_featured = df_train_featured.drop(['datetime'],axis=1)\ndf_test_featured = df_test_featured.drop(['datetime'],axis=1)\n\n\nsns.heatmap(df_train_featured.set_index('count').reset_index().corr(),vmin=-1,cmap='bwr')\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ntemp와 atemp가 동시에 있어서 공선성 문제를 만들 수 있어보임.\n둘중 하나를 제거하는게 좋을것 같음.\n\n\nauto(\n    df_train_featured.drop(['temp'],axis=1),\n    df_test_featured.drop(['temp'],axis=1)\n)\n\n                  model   score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2  -59.511984       0.097477  58.259779                0.000259           0.167847            2       True         12\n1         LightGBMLarge  -61.029920       0.008677   1.338407                0.008677           1.338407            1       True         11\n2              LightGBM  -61.580307       0.019255   1.537220                0.019255           1.537220            1       True          4\n3              CatBoost  -61.849961       0.001914  17.761223                0.001914          17.761223            1       True          6\n4               XGBoost  -62.741724       0.007695   0.495664                0.007695           0.495664            1       True          9\n5            LightGBMXT  -63.351618       0.053134   3.360234                0.053134           3.360234            1       True          3\n6         ExtraTreesMSE  -67.852239       0.036566   0.597020                0.036566           0.597020            1       True          7\n7        NeuralNetTorch  -68.046825       0.006544  33.599183                0.006544          33.599183            1       True         10\n8       RandomForestMSE  -68.525817       0.037002   0.909598                0.037002           0.909598            1       True          5\n9       NeuralNetFastAI  -72.517457       0.009779   6.932450                0.009779           6.932450            1       True          8\n10       KNeighborsDist -114.334789       0.008465   0.068125                0.008465           0.068125            1       True          2\n11       KNeighborsUnif -116.835940       0.008546   0.069387                0.008546           0.069387            1       True          1\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 241k/241k [00:01&lt;00:00, 173kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n-59.511984\n0.097477\n58.259779\n0.000259\n0.167847\n2\nTrue\n12\n\n\n1\nLightGBMLarge\n-61.029920\n0.008677\n1.338407\n0.008677\n1.338407\n1\nTrue\n11\n\n\n2\nLightGBM\n-61.580307\n0.019255\n1.537220\n0.019255\n1.537220\n1\nTrue\n4\n\n\n3\nCatBoost\n-61.849961\n0.001914\n17.761223\n0.001914\n17.761223\n1\nTrue\n6\n\n\n4\nXGBoost\n-62.741724\n0.007695\n0.495664\n0.007695\n0.495664\n1\nTrue\n9\n\n\n5\nLightGBMXT\n-63.351618\n0.053134\n3.360234\n0.053134\n3.360234\n1\nTrue\n3\n\n\n6\nExtraTreesMSE\n-67.852239\n0.036566\n0.597020\n0.036566\n0.597020\n1\nTrue\n7\n\n\n7\nNeuralNetTorch\n-68.046825\n0.006544\n33.599183\n0.006544\n33.599183\n1\nTrue\n10\n\n\n8\nRandomForestMSE\n-68.525817\n0.037002\n0.909598\n0.037002\n0.909598\n1\nTrue\n5\n\n\n9\nNeuralNetFastAI\n-72.517457\n0.009779\n6.932450\n0.009779\n6.932450\n1\nTrue\n8\n\n\n10\nKNeighborsDist\n-114.334789\n0.008465\n0.068125\n0.008465\n0.068125\n1\nTrue\n2\n\n\n11\nKNeighborsUnif\n-116.835940\n0.008546\n0.069387\n0.008546\n0.069387\n1\nTrue\n1\n\n\n\n\n\n\n\n\n\n\n\n\nauto(\n    df_train_featured.drop(['atemp'],axis=1),\n    df_test_featured.drop(['atemp'],axis=1)\n)\n\n                  model   score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2  -59.317972       0.070543  52.287304                0.000294           0.195872            2       True         12\n1         LightGBMLarge  -60.667457       0.008713   1.374132                0.008713           1.374132            1       True         11\n2              CatBoost  -60.919338       0.001944  11.113472                0.001944          11.113472            1       True          6\n3            LightGBMXT  -61.740606       0.037555   2.373389                0.037555           2.373389            1       True          3\n4              LightGBM  -62.028032       0.016164   1.334948                0.016164           1.334948            1       True          4\n5               XGBoost  -62.503591       0.006645   0.457786                0.006645           0.457786            1       True          9\n6       RandomForestMSE  -67.814371       0.032844   0.992604                0.032844           0.992604            1       True          5\n7         ExtraTreesMSE  -67.843089       0.032126   0.775694                0.032126           0.775694            1       True          7\n8        NeuralNetTorch  -68.163726       0.005872  35.895491                0.005872          35.895491            1       True         10\n9       NeuralNetFastAI  -71.526328       0.009695   6.962153                0.009695           6.962153            1       True          8\n10       KNeighborsDist -112.678494       0.008421   0.068928                0.008421           0.068928            1       True          2\n11       KNeighborsUnif -115.103505       0.008441   0.070454                0.008441           0.070454            1       True          1\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 241k/241k [00:02&lt;00:00, 108kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n-59.317972\n0.070543\n52.287304\n0.000294\n0.195872\n2\nTrue\n12\n\n\n1\nLightGBMLarge\n-60.667457\n0.008713\n1.374132\n0.008713\n1.374132\n1\nTrue\n11\n\n\n2\nCatBoost\n-60.919338\n0.001944\n11.113472\n0.001944\n11.113472\n1\nTrue\n6\n\n\n3\nLightGBMXT\n-61.740606\n0.037555\n2.373389\n0.037555\n2.373389\n1\nTrue\n3\n\n\n4\nLightGBM\n-62.028032\n0.016164\n1.334948\n0.016164\n1.334948\n1\nTrue\n4\n\n\n5\nXGBoost\n-62.503591\n0.006645\n0.457786\n0.006645\n0.457786\n1\nTrue\n9\n\n\n6\nRandomForestMSE\n-67.814371\n0.032844\n0.992604\n0.032844\n0.992604\n1\nTrue\n5\n\n\n7\nExtraTreesMSE\n-67.843089\n0.032126\n0.775694\n0.032126\n0.775694\n1\nTrue\n7\n\n\n8\nNeuralNetTorch\n-68.163726\n0.005872\n35.895491\n0.005872\n35.895491\n1\nTrue\n10\n\n\n9\nNeuralNetFastAI\n-71.526328\n0.009695\n6.962153\n0.009695\n6.962153\n1\nTrue\n8\n\n\n10\nKNeighborsDist\n-112.678494\n0.008421\n0.068928\n0.008421\n0.068928\n1\nTrue\n2\n\n\n11\nKNeighborsUnif\n-115.103505\n0.008441\n0.070454\n0.008441\n0.070454\n1\nTrue\n1"
  },
  {
    "objectID": "posts/14wk-59.out.html#c.-step3-season을-범주로",
    "href": "posts/14wk-59.out.html#c.-step3-season을-범주로",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "C. Step3 – season을 범주로?",
    "text": "C. Step3 – season을 범주로?\n- 지금까지한 피처엔지니어링\n\ndf_train_featured = df_train.copy()\ndf_test_featured = df_test.copy()\n#----# \ndf_train_featured = df_train_featured.drop(['casual','registered'],axis=1)\n#--#\ndf_train_featured['hour'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_test_featured['hour'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_train_featured['weekday'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.weekday\ndf_test_featured['weekday'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.weekday\n#--#\ndf_train_featured = df_train_featured.drop(['datetime'],axis=1)\ndf_test_featured = df_test_featured.drop(['datetime'],axis=1)\n#--#\ndf_train_featured = df_train_featured.drop(['atemp'],axis=1)\ndf_test_featured = df_test_featured.drop(['atemp'],axis=1)\n\n- 사실 season의 의미는 season -  1 = spring, 2 = summer, 3 = fall, 4 = winter 임\n\n지금은 season이 1,2,3,4로 코딩되어 있는데, 이것을 문자열로 바꾸면 더 좋지 않을까?\n\n\nauto(\n    df_train_featured.assign(season = df_train_featured.season.map({1:'spring',2:'summer',3:'fall',4:'winter'})),\n    df_test_featured.assign(season = df_train_featured.season.map({1:'spring',2:'summer',3:'fall',4:'winter'}))\n)\n\n                  model   score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2  -58.809100       0.124894  88.477947                0.000289           0.195235            2       True         12\n1            LightGBMXT  -61.072683       0.087661   4.569525                0.087661           4.569525            1       True          3\n2         LightGBMLarge  -61.188052       0.012944   1.727106                0.012944           1.727106            1       True         11\n3              CatBoost  -62.040378       0.006600  47.570805                0.006600          47.570805            1       True          6\n4              LightGBM  -62.161719       0.018669   1.587194                0.018669           1.587194            1       True          4\n5               XGBoost  -62.183089       0.010632   0.598557                0.010632           0.598557            1       True          9\n6        NeuralNetTorch  -67.358419       0.006769  33.816719                0.006769          33.816719            1       True         10\n7       RandomForestMSE  -68.346914       0.032818   1.102551                0.032818           1.102551            1       True          5\n8         ExtraTreesMSE  -68.569688       0.035215   0.590428                0.035215           0.590428            1       True          7\n9       NeuralNetFastAI  -72.355712       0.012358   7.412937                0.012358           7.412937            1       True          8\n10       KNeighborsDist -113.647125       0.008502   0.069228                0.008502           0.069228            1       True          2\n11       KNeighborsUnif -116.110797       0.008447   0.071381                0.008447           0.071381            1       True          1\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 240k/240k [00:01&lt;00:00, 124kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n-58.809100\n0.124894\n88.477947\n0.000289\n0.195235\n2\nTrue\n12\n\n\n1\nLightGBMXT\n-61.072683\n0.087661\n4.569525\n0.087661\n4.569525\n1\nTrue\n3\n\n\n2\nLightGBMLarge\n-61.188052\n0.012944\n1.727106\n0.012944\n1.727106\n1\nTrue\n11\n\n\n3\nCatBoost\n-62.040378\n0.006600\n47.570805\n0.006600\n47.570805\n1\nTrue\n6\n\n\n4\nLightGBM\n-62.161719\n0.018669\n1.587194\n0.018669\n1.587194\n1\nTrue\n4\n\n\n5\nXGBoost\n-62.183089\n0.010632\n0.598557\n0.010632\n0.598557\n1\nTrue\n9\n\n\n6\nNeuralNetTorch\n-67.358419\n0.006769\n33.816719\n0.006769\n33.816719\n1\nTrue\n10\n\n\n7\nRandomForestMSE\n-68.346914\n0.032818\n1.102551\n0.032818\n1.102551\n1\nTrue\n5\n\n\n8\nExtraTreesMSE\n-68.569688\n0.035215\n0.590428\n0.035215\n0.590428\n1\nTrue\n7\n\n\n9\nNeuralNetFastAI\n-72.355712\n0.012358\n7.412937\n0.012358\n7.412937\n1\nTrue\n8\n\n\n10\nKNeighborsDist\n-113.647125\n0.008502\n0.069228\n0.008502\n0.069228\n1\nTrue\n2\n\n\n11\nKNeighborsUnif\n-116.110797\n0.008447\n0.071381\n0.008447\n0.071381\n1\nTrue\n1\n\n\n\n\n\n\n\n\n\n\n\n\n이건 적용하지 말자.\n어차피 트리계열은 명목형변수를 순서형변수로 잘못 적용해도 크게 상관없음."
  },
  {
    "objectID": "posts/14wk-59.out.html#d.-step4-y의-분포",
    "href": "posts/14wk-59.out.html#d.-step4-y의-분포",
    "title": "[STBDA2023] 14wk-59: 자전거대여 / 시계열자료분석?(Autogluon)",
    "section": "D. Step4 – \\(y\\)의 분포",
    "text": "D. Step4 – \\(y\\)의 분포\n- 지금까지한 피처엔지니어링\n\ndf_train_featured = df_train.copy()\ndf_test_featured = df_test.copy()\n#----# \ndf_train_featured = df_train_featured.drop(['casual','registered'],axis=1)\n#--#\ndf_train_featured['hour'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_test_featured['hour'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.hour\ndf_train_featured['weekday'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.weekday\ndf_test_featured['weekday'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.weekday\n#--#\ndf_train_featured = df_train_featured.drop(['datetime'],axis=1)\ndf_test_featured = df_test_featured.drop(['datetime'],axis=1)\n#--#\ndf_train_featured = df_train_featured.drop(['atemp'],axis=1)\ndf_test_featured = df_test_featured.drop(['atemp'],axis=1)\n\n\ndf_train_featured['count'].hist() # 정규분포가 아니네\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\ntransfomr = sklearn.preprocessing.PowerTransformer(method='box-cox')\n\n\ncount2 = transfomr.fit_transform(df_train_featured[['count']]).reshape(-1)\nplt.hist(count2);\n\n\n\n\n\ndf_train_featured.assign(count = count2)\n\n\n\n\n\n\n\n\nseason\nholiday\nworkingday\nweather\ntemp\nhumidity\nwindspeed\ncount\nhour\nweekday\n\n\n\n\n0\n1\n0\n0\n1\n9.84\n81\n0.0000\n-1.255010\n0\n5\n\n\n1\n1\n0\n0\n1\n9.02\n80\n0.0000\n-0.801417\n1\n5\n\n\n2\n1\n0\n0\n1\n9.02\n80\n0.0000\n-0.924248\n2\n5\n\n\n3\n1\n0\n0\n1\n9.84\n75\n0.0000\n-1.340805\n3\n5\n\n\n4\n1\n0\n0\n1\n9.84\n75\n0.0000\n-2.043720\n4\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10881\n4\n0\n1\n1\n15.58\n50\n26.0027\n0.928271\n19\n2\n\n\n10882\n4\n0\n1\n1\n14.76\n57\n15.0013\n0.576172\n20\n2\n\n\n10883\n4\n0\n1\n1\n13.94\n61\n15.0013\n0.233448\n21\n2\n\n\n10884\n4\n0\n1\n1\n13.94\n61\n6.0032\n0.006178\n22\n2\n\n\n10885\n4\n0\n1\n1\n13.12\n66\n8.9981\n-0.291061\n23\n2\n\n\n\n\n10886 rows × 10 columns\n\n\n\n- 적합\n\n# step1 -- pass \n# step2 \npredictr = TabularPredictor(label='count',verbosity=False)\n# step3\npredictr.fit(df_train_featured.assign(count = count2))\n# step4\nyhat = predictr.predict(df_train_featured) \nyyhat = predictr.predict(df_test_featured)\n\n\nyhat = transfomr.inverse_transform(yhat.to_frame()).reshape(-1)\nyyhat = transfomr.inverse_transform(yyhat.to_frame()).reshape(-1)\n\n\nplot(yhat,yyhat)\n\n\n\n\n\nsubmit(yyhat)\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|████████████████████████████████████████| 243k/243k [00:02&lt;00:00, 96.3kB/s]\nSuccessfully submitted to Bike Sharing Demand"
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html",
    "title": "[STBDA2023] 02wk-008: 타이타닉, Autogluon(best_quality)",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html#a.-데이터",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html#a.-데이터",
    "title": "[STBDA2023] 02wk-008: 타이타닉, Autogluon(best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"~/Desktop/titanic/train.csv\")\ntst = TabularDataset(\"~/Desktop/titanic/test.csv\")\n\nLoaded data from: ~/Desktop/titanic/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ~/Desktop/titanic/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n\ntst\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n\n\n\n\n418 rows × 11 columns"
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html#b.-predictor-생성",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html#b.-predictor-생성",
    "title": "[STBDA2023] 02wk-008: 타이타닉, Autogluon(best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230917_141828/\""
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html#c.-적합fit",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html#c.-적합fit",
    "title": "[STBDA2023] 02wk-008: 타이타닉, Autogluon(best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230917_141828/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   775.46 GB / 982.82 GB (78.9%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    37750.16 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.16s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f20705933a0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    0.6308   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f20705e85e0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.69s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8608   = Validation score   (accuracy)\n    1.7s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.853    = Validation score   (accuracy)\n    2.19s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8462   = Validation score   (accuracy)\n    3.98s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.04s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.5s     = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 19.49s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230917_141828/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f212cb60f40&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.023981  1.700775                0.023981           1.700775            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.025379  2.204065                0.001398           0.503290            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.852974       0.067810  2.190649                0.067810           2.190649            1       True         10\n3     NeuralNetTorch_BAG_L1   0.846240       0.086196  3.984127                0.086196           3.984127            1       True         12\n4           LightGBM_BAG_L1   0.840629       0.023935  0.687209                0.023935           0.687209            1       True          4\n5      LightGBMLarge_BAG_L1   0.840629       0.025161  1.040782                0.025161           1.040782            1       True         13\n6            XGBoost_BAG_L1   0.840629       0.039286  0.545162                0.039286           0.545162            1       True         11\n7   RandomForestGini_BAG_L1   0.837262       0.058148  0.252888                0.058148           0.252888            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.058385  0.259539                0.058385           0.259539            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.022836  0.638991                0.022836           0.638991            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.056459  0.251367                0.056459           0.251367            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.058829  0.257241                0.058829           0.257241            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.012997  0.003672                0.012997           0.003672            1       True          2\n13    KNeighborsUnif_BAG_L1   0.630752       0.011647  0.003759                0.011647           0.003759            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.023981\n1.700775\n0.023981\n1.700775\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.025379\n2.204065\n0.001398\n0.503290\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.852974\n0.067810\n2.190649\n0.067810\n2.190649\n1\nTrue\n10\n\n\n3\nNeuralNetTorch_BAG_L1\n0.846240\n0.086196\n3.984127\n0.086196\n3.984127\n1\nTrue\n12\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.023935\n0.687209\n0.023935\n0.687209\n1\nTrue\n4\n\n\n5\nLightGBMLarge_BAG_L1\n0.840629\n0.025161\n1.040782\n0.025161\n1.040782\n1\nTrue\n13\n\n\n6\nXGBoost_BAG_L1\n0.840629\n0.039286\n0.545162\n0.039286\n0.545162\n1\nTrue\n11\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.058148\n0.252888\n0.058148\n0.252888\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.058385\n0.259539\n0.058385\n0.259539\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.022836\n0.638991\n0.022836\n0.638991\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.056459\n0.251367\n0.056459\n0.251367\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.058829\n0.257241\n0.058829\n0.257241\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.012997\n0.003672\n0.012997\n0.003672\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.630752\n0.011647\n0.003759\n0.011647\n0.003759\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html#d.-예측-predict",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).out.html#d.-예측-predict",
    "title": "[STBDA2023] 02wk-008: 타이타닉, Autogluon(best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.898989898989899\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/11wk-041.out.html",
    "href": "posts/11wk-041.out.html",
    "title": "[STBDA2023] 11wk-041: Medical Cost / 의사결정나무 max_feature,random_state",
    "section": "",
    "text": "11wk-041: Medical Cost / 의사결정나무 max_feature,random_state\n최규빈\n2023-11-16\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-yQ9Pe1jX-NSI2FqYZn30co&si=ura_7Fm-18S4vxx1\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.tree\nimport graphviz\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. 데이터준비\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/insurance.csv')\ndf_train\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\n\n\n\n\n\n4. max_features\n- max_features에 대한 제한을 주지 않음 \\(\\to\\) 항상 같은 결과가 나옴\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns);\n\n\n\n\n- max_features=4로 제한\n\nlen(X.columns) \n\n8\n\n\n\nmax_features=4로 제한한다는 의미는 8개의 설명변수중에서 4개만 임의로 뽑아서 그중에서 “최적의 변수”와 “최적의 \\(c\\)”를 찾겠다는 의미\n\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor(max_features=4)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor(max_features=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features=4)\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns);\n\n\n\n\n\n절반정도는 smoking 유무가 가장 위에 위치한다.\n\n\n\n5. random_state\n- max_features=4로 제한\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor(max_features=4,random_state=43)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor(max_features=4, random_state=43)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features=4, random_state=43)\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns);"
  },
  {
    "objectID": "posts/13wk-57.out.html",
    "href": "posts/13wk-57.out.html",
    "title": "[STBDA2023] 13wk-57: House Prices / 자료분석(Autogluon)",
    "section": "",
    "text": "13wk-57: House Prices / 자료분석(Autogluon)\n최규빈\n2023-12-01\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-x-PYcds3K7ck8ELQyVlVoN&si=ZCdvUB2r4dQ7cnQx\n\n\n2. Imports\n\n#!pip install autogluon.eda\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n#---#\nfrom autogluon.tabular import TabularPredictor\nimport autogluon.eda.auto as auto\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. Data\nref: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview\n\n!kaggle competitions download -c house-prices-advanced-regression-techniques\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\nDownloading house-prices-advanced-regression-techniques.zip to /home/coco/Dropbox/Class/STBDA23/posts\n100%|█████████████████████████████████████████| 199k/199k [00:00&lt;00:00, 436kB/s]\n100%|█████████████████████████████████████████| 199k/199k [00:00&lt;00:00, 435kB/s]\n\n\n\n!unzip house-prices-advanced-regression-techniques.zip\n\nArchive:  house-prices-advanced-regression-techniques.zip\n  inflating: data_description.txt    \n  inflating: sample_submission.csv   \n  inflating: test.csv                \n  inflating: train.csv               \n\n\n\ndf_submission = pd.read_csv(\"sample_submission.csv\")\ndf_train = pd.read_csv(\"train.csv\")\ndf_test = pd.read_csv(\"test.csv\")\n\n\n!rm sample_submission.csv\n!rm train.csv\n!rm test.csv\n!rm data_description.txt\n!rm house-prices-advanced-regression-techniques.zip\n\n\n\n4. 적합\n\nset(df_train.columns) - set(df_test.columns)\n\n{'SalePrice'}\n\n\n\n# step1 -- pass\n# step2\npredictr = TabularPredictor(label='SalePrice')\n# step3\npredictr.fit(df_train)\n# step4\nyhat = predictr.predict(df_train)\nyyhat = predictr.predict(df_test)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231210_084023/\"\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231210_084023/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2\nDisk Space Avail:   643.98 GB / 982.82 GB (65.5%)\nTrain Data Rows:    1460\nTrain Data Columns: 80\nLabel Column: SalePrice\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n    Label info (max, min, mean, stddev): (755000, 34900, 180921.19589, 79442.50288)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    128242.01 MB\n    Train Data (Original)  Memory Usage: 4.06 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 3 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])  :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n        ('int', [])    : 34 | ['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', ...]\n        ('object', []) : 43 | ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', ...]\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])  : 40 | ['MSZoning', 'Alley', 'LotShape', 'LandContour', 'LotConfig', ...]\n        ('float', [])     :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n        ('int', [])       : 34 | ['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', ...]\n        ('int', ['bool']) :  3 | ['Street', 'Utilities', 'CentralAir']\n    0.2s = Fit runtime\n    80 features in original data used to generate 80 features in processed data.\n    Train Data (Processed) Memory Usage: 0.52 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.18s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 1168, Val Rows: 292\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models ...\nFitting model: KNeighborsUnif ...\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f2e74356550&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    -52278.8213  = Validation score   (-root_mean_squared_error)\n    0.2s     = Training   runtime\n    0.04s    = Validation runtime\nFitting model: KNeighborsDist ...\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f2e4f9dd670&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    -51314.2734  = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    -27196.7065  = Validation score   (-root_mean_squared_error)\n    2.18s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM ...\n    -28692.2871  = Validation score   (-root_mean_squared_error)\n    5.13s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: RandomForestMSE ...\n    -32785.3519  = Validation score   (-root_mean_squared_error)\n    0.37s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    -28465.6966  = Validation score   (-root_mean_squared_error)\n    43.66s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -32045.9062  = Validation score   (-root_mean_squared_error)\n    0.3s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    -33846.1211  = Validation score   (-root_mean_squared_error)\n    1.29s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: XGBoost ...\n    -27778.2437  = Validation score   (-root_mean_squared_error)\n    0.87s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    -36076.0341  = Validation score   (-root_mean_squared_error)\n    2.24s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMLarge ...\n    -32084.1712  = Validation score   (-root_mean_squared_error)\n    7.97s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    -26322.571   = Validation score   (-root_mean_squared_error)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 65.86s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231210_084023/\")\nWARNING: Int features without null values at train time contain null values at inference time! Imputing nulls to 0. To avoid this, pass the features as floats during fit!\nWARNING: Int features with nulls: ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'GarageArea']\n\n\n[1000]  valid_set's rmse: 27505.1\n[2000]  valid_set's rmse: 27240.4\n[3000]  valid_set's rmse: 27201.5\n[4000]  valid_set's rmse: 27197.3\n[5000]  valid_set's rmse: 27197.2\n[1000]  valid_set's rmse: 29499.8\n[2000]  valid_set's rmse: 28896.4\n[3000]  valid_set's rmse: 28752.1\n[4000]  valid_set's rmse: 28705.7\n[5000]  valid_set's rmse: 28695.2\n[6000]  valid_set's rmse: 28693\n[7000]  valid_set's rmse: 28692.5\n[8000]  valid_set's rmse: 28692.3\n[9000]  valid_set's rmse: 28692.3\n[10000] valid_set's rmse: 28692.3\n[1000]  valid_set's rmse: 32134.9\n[2000]  valid_set's rmse: 32087.8\n[3000]  valid_set's rmse: 32084.2\n[4000]  valid_set's rmse: 32084.2\n[5000]  valid_set's rmse: 32084.2\n\n\n\n\n5. 제출\n\ndf_submission['SalePrice'] = yyhat \ndf_submission.to_csv(\"submission.csv\",index=False)\n\n\n!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"오토글루온을 이용하여 첫제출\"\n!rm submission.csv\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|██████████████████████████████████████| 21.1k/21.1k [00:01&lt;00:00, 11.4kB/s]\nSuccessfully submitted to House Prices - Advanced Regression Techniques\n\n\n\n\n958/4955\n\n0.19334006054490413\n\n\n나쁘지 않은 순위..\n\n\n6. 해석 및 시각화 (HW)\n- 변수들중에서 SalePrice를 예측하기에 적절한 변수들을 조사해볼것.\n\ndf_train\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1455\n1456\n60\nRL\n62.0\n7917\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n8\n2007\nWD\nNormal\n175000\n\n\n1456\n1457\n20\nRL\n85.0\n13175\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nMnPrv\nNaN\n0\n2\n2010\nWD\nNormal\n210000\n\n\n1457\n1458\n70\nRL\n66.0\n9042\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nGdPrv\nShed\n2500\n5\n2010\nWD\nNormal\n266500\n\n\n1458\n1459\n20\nRL\n68.0\n9717\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n4\n2010\nWD\nNormal\n142125\n\n\n1459\n1460\n20\nRL\n75.0\n9937\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n6\n2008\nWD\nNormal\n147500\n\n\n\n\n1460 rows × 81 columns\n\n\n\n\nauto.quick_fit(\n    train_data=df_train,\n    label='SalePrice',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231210_084134/\"\n\n\nModel Prediction for SalePrice\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-30529.412291\n-32535.182194\n0.008621\n0.006892\n0.269097\n0.008621\n0.006892\n0.269097\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nOverallQual\n11716.915326\n705.955990\n1.573763e-06\n5\n13170.488468\n10263.342185\n\n\nGrLivArea\n6071.089919\n394.407430\n2.125470e-06\n5\n6883.180269\n5258.999569\n\n\nGarageCars\n3209.379857\n372.418575\n2.137296e-05\n5\n3976.194851\n2442.564863\n\n\nBsmtFinSF1\n2719.389615\n125.895615\n5.496647e-07\n5\n2978.610426\n2460.168805\n\n\nTotalBsmtSF\n2187.752068\n328.342663\n5.909712e-05\n5\n2863.814149\n1511.689987\n\n\n1stFlrSF\n1688.228494\n248.951746\n5.513520e-05\n5\n2200.823579\n1175.633408\n\n\nNeighborhood\n1534.658374\n377.814804\n4.073284e-04\n5\n2312.584278\n756.732471\n\n\nHalfBath\n881.236830\n353.675558\n2.542584e-03\n5\n1609.459692\n153.013968\n\n\n2ndFlrSF\n872.631138\n123.229525\n4.647906e-05\n5\n1126.362433\n618.899844\n\n\nFireplaces\n868.173697\n364.469844\n2.989959e-03\n5\n1618.622144\n117.725249\n\n\nExterQual\n829.910645\n401.498301\n4.933027e-03\n5\n1656.601195\n3.220095\n\n\nYearRemodAdd\n675.119616\n166.871475\n4.136303e-04\n5\n1018.710290\n331.528942\n\n\nLotArea\n645.731150\n240.401151\n1.933927e-03\n5\n1140.720443\n150.741856\n\n\nFullBath\n587.941341\n102.476487\n1.064026e-04\n5\n798.941844\n376.940838\n\n\nTotRmsAbvGrd\n455.362588\n146.798615\n1.134317e-03\n5\n757.622965\n153.102211\n\n\nBsmtExposure\n452.574670\n100.266016\n2.711034e-04\n5\n659.023783\n246.125556\n\n\nOverallCond\n415.878748\n101.116481\n3.882599e-04\n5\n624.078980\n207.678516\n\n\nYearBuilt\n328.365122\n49.414644\n5.972756e-05\n5\n430.110557\n226.619687\n\n\nMasVnrArea\n291.987049\n102.297731\n1.546173e-03\n5\n502.619491\n81.354608\n\n\nGarageArea\n266.571657\n194.368541\n1.870635e-02\n5\n666.779168\n-133.635855\n\n\nCentralAir\n252.005310\n43.263826\n1.002687e-04\n5\n341.086127\n162.924494\n\n\nLotFrontage\n234.476025\n232.700363\n4.367093e-02\n5\n713.609289\n-244.657239\n\n\nGarageYrBlt\n179.569936\n41.348994\n3.147824e-04\n5\n264.708087\n94.431785\n\n\nBsmtFullBath\n130.488441\n27.317412\n2.176156e-04\n5\n186.735370\n74.241513\n\n\nMSSubClass\n115.302650\n77.955090\n1.486407e-02\n5\n275.813256\n-45.207957\n\n\nBsmtFinType1\n89.671801\n23.805159\n5.438720e-04\n5\n138.686952\n40.656649\n\n\nKitchenAbvGr\n88.796843\n13.144665\n5.597648e-05\n5\n115.861890\n61.731796\n\n\nOpenPorchSF\n76.599171\n32.768044\n3.198232e-03\n5\n144.069027\n9.129315\n\n\nYrSold\n55.041099\n17.204734\n1.010331e-03\n5\n90.465884\n19.616315\n\n\nMSZoning\n52.239352\n30.345403\n9.156204e-03\n5\n114.720955\n-10.242251\n\n\nBsmtQual\n45.470810\n50.360612\n5.681646e-02\n5\n149.164006\n-58.222385\n\n\nPavedDrive\n27.952965\n11.965434\n3.205450e-03\n5\n52.589960\n3.315970\n\n\nFoundation\n15.764567\n21.136187\n8.534210e-02\n5\n59.284268\n-27.755134\n\n\nWoodDeckSF\n14.490268\n49.028633\n2.724108e-01\n5\n115.440900\n-86.460364\n\n\nBsmtUnfSF\n14.251367\n14.062238\n4.304693e-02\n5\n43.205709\n-14.702976\n\n\nScreenPorch\n10.514393\n6.927148\n1.371401e-02\n5\n24.777486\n-3.748700\n\n\nHeatingQC\n10.365567\n6.499702\n1.172951e-02\n5\n23.748544\n-3.017409\n\n\nId\n9.875743\n25.538459\n2.179917e-01\n5\n62.459783\n-42.708298\n\n\nRoofStyle\n8.089551\n53.663840\n3.765020e-01\n5\n118.584139\n-102.405037\n\n\nLandSlope\n6.869875\n7.000689\n4.662255e-02\n5\n21.284391\n-7.544641\n\n\nBsmtFinSF2\n6.856908\n2.939085\n3.220858e-03\n5\n12.908525\n0.805292\n\n\nExterior1st\n4.492434\n8.697021\n1.561884e-01\n5\n22.399720\n-13.414852\n\n\nHouseStyle\n3.571663\n3.437669\n4.042508e-02\n5\n10.649870\n-3.506545\n\n\nBsmtHalfBath\n2.760108\n1.020890\n1.888221e-03\n5\n4.862135\n0.658082\n\n\nMoSold\n1.776995\n18.530416\n4.203498e-01\n5\n39.931378\n-36.377389\n\n\nFireplaceQu\n0.246895\n8.880015\n4.767049e-01\n5\n18.530969\n-18.037180\n\n\nGarageQual\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nGarageCond\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nUtilities\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nLandContour\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\n3SsnPorch\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nPoolArea\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nAlley\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nPoolQC\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nFence\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nMiscFeature\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nMiscVal\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nStreet\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nSaleType\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nLotConfig\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nCondition1\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nSaleCondition\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nMasVnrType\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nGarageType\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nCondition2\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nFunctional\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nBldgType\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nBsmtFinType2\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nBsmtCond\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nKitchenQual\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nHeating\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nRoofMatl\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nLowQualFinSF\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nElectrical\n-0.249231\n0.232446\n9.627220e-01\n5\n0.229380\n-0.727841\n\n\nBedroomAbvGr\n-0.281759\n5.209289\n5.452164e-01\n5\n10.444239\n-11.007758\n\n\nEnclosedPorch\n-0.726761\n1.153234\n8.842100e-01\n5\n1.647763\n-3.101286\n\n\nExterior2nd\n-1.300222\n2.040234\n8.863621e-01\n5\n2.900648\n-5.501092\n\n\nGarageFinish\n-4.409206\n10.141071\n8.070016e-01\n5\n16.471400\n-25.289812\n\n\nExterCond\n-4.973063\n0.837647\n9.999070e-01\n5\n-3.248336\n-6.697791\n\n\nLotShape\n-56.043807\n64.453373\n9.381189e-01\n5\n76.666578\n-188.754193\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\nSalePrice_pred\nerror\n\n\n\n\n1182\n1183\n60\nRL\n160.0\n15623\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n7\n2007\nWD\nAbnorml\n745000\n458492.125000\n286507.875000\n\n\n1298\n1299\n60\nRL\n313.0\n63887\nPave\nNaN\nIR3\nBnk\nAllPub\n...\nNaN\nNaN\n0\n1\n2008\nNew\nPartial\n160000\n394141.625000\n234141.625000\n\n\n688\n689\n20\nRL\n60.0\n8089\nPave\nNaN\nReg\nHLS\nAllPub\n...\nNaN\nNaN\n0\n10\n2007\nNew\nPartial\n392000\n236151.000000\n155849.000000\n\n\n898\n899\n20\nRL\n100.0\n12919\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n3\n2010\nNew\nPartial\n611657\n463175.906250\n148481.093750\n\n\n440\n441\n20\nRL\n105.0\n15431\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2009\nWD\nNormal\n555000\n419354.593750\n135645.406250\n\n\n581\n582\n20\nRL\n98.0\n12704\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n8\n2009\nNew\nPartial\n253293\n382104.968750\n128811.968750\n\n\n769\n770\n60\nRL\n47.0\n53504\nPave\nNaN\nIR2\nHLS\nAllPub\n...\nNaN\nNaN\n0\n6\n2010\nWD\nNormal\n538000\n429698.281250\n108301.718750\n\n\n632\n633\n20\nRL\n85.0\n11900\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2009\nWD\nFamily\n82500\n182936.734375\n100436.734375\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n333203.062500\n83203.062500\n\n\n666\n667\n60\nRL\nNaN\n18450\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n8\n2007\nWD\nAbnorml\n129000\n210298.609375\n81298.609375\n\n\n\n\n10 rows × 83 columns\n\n\n\n\ndf_train.iloc[[1]]\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n\n\n1 rows × 81 columns\n\n\n\n\npredictr.predict(df_train.iloc[[1]])\n\n1    170013.546875\nName: SalePrice, dtype: float32\n\n\n\n# auto.explain_rows(\n#     train_data=df_train,\n#     model=predictr,\n#     rows=df_train.iloc[[1]],\n#     display_rows=True,\n#     plot='waterfall'\n# )\n\n\n위 코드 돌아가는데 너무 오래걸리네\n\n\nchatGPT의 변수에 대한 설명\n이 데이터는 Kaggle의 “House Prices - Advanced Regression Techniques” 대회에서 사용되는 데이터로, 주택 판매 가격을 예측하는 문제입니다. 아래는 데이터에 있는 변수들에 대한 설명입니다.\n\nSalePrice (판매 가격): 주택의 판매 가격 (예측하려는 대상 변수)\nMSSubClass (건물 종류): 건물의 종류를 나타내는 코드\nMSZoning (일반 분류): 일반적인 구역 분류\nLotFrontage (부동산에 연결된 도로의 선형 길이): 주택과 연결된 도로의 길이\nLotArea (부지 면적): 부지의 면적 (제곱 피트)\nStreet (도로 접근 유형): 도로 접근 유형 (포장 도로 등)\nAlley (골목 접근 유형): 골목 접근 유형\nLotShape (부지의 일반적인 모양): 부지의 모양\nLandContour (부지의 평평함): 부지의 평평함 정도\nUtilities (사용 가능한 유틸리티 유형): 사용 가능한 유틸리티의 유형\nLotConfig (부지 구성): 부지의 구성\nLandSlope (부지 경사): 부지의 경사\nNeighborhood (Ames 시 한계 내의 물리적 위치): 물리적 위치\nCondition1 (주요 도로 또는 철도와의 근접성): 주요 도로 또는 철도와의 근접성\nCondition2 (주요 도로 또는 철도와의 근접성(두 번째)): 두 번째로 주어진 경우의 주요 도로 또는 철도와의 근접성\nBldgType (주거 형태): 주거 형태\nHouseStyle (주택 스타일): 주택 스타일\nOverallQual (전체 자재 및 마감 품질): 전반적인 자재 및 마감 품질\nOverallCond (전반적인 상태 등급): 전반적인 상태 등급\nYearBuilt (원래 건설 날짜): 원래 건설된 연도\n\n… 그 외에도 다양한 주택 특징 및 설명이 포함되어 있습니다.\n이 변수들은 각각 주택에 대한 다양한 특징을 설명하고 있으며, 이러한 특징을 사용하여 각 주택의 판매 가격을 예측하는 것이 목표입니다.\n\n\n위 데이터에서 SalePrice을 예측하는데 중요한 변수로는 OverallQual이 가장 중요하다.\n\n\nset(df_train['OverallQual'])\n\n{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\n\n등급이 높을 수록 saleprice가 더 높다.\n- 변수 overallqual만 해보자\n\ndf_tr_ = df_train[['OverallQual','SalePrice']]\ndf_ts_ = df_test[['OverallQual']]\n\n\ndf_tr_\n\n\n\n\n\n\n\n\nOverallQual\nSalePrice\n\n\n\n\n0\n7\n208500\n\n\n1\n6\n181500\n\n\n2\n7\n223500\n\n\n3\n7\n140000\n\n\n4\n8\n250000\n\n\n...\n...\n...\n\n\n1455\n6\n175000\n\n\n1456\n6\n210000\n\n\n1457\n7\n266500\n\n\n1458\n5\n142125\n\n\n1459\n5\n147500\n\n\n\n\n1460 rows × 2 columns\n\n\n\n\n# step1 -- pass\n# step2\npredictr = TabularPredictor(label='SalePrice')\n# step3\npredictr.fit(df_tr_)\n# step4\nyhat = predictr.predict(df_tr_)\n#yyhat = predictr.predict(df_ts_)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231210_090017/\"\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231210_090017/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2\nDisk Space Avail:   643.82 GB / 982.82 GB (65.5%)\nTrain Data Rows:    1460\nTrain Data Columns: 1\nLabel Column: SalePrice\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n    Label info (max, min, mean, stddev): (755000, 34900, 180921.19589, 79442.50288)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    128082.22 MB\n    Train Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('int', []) : 1 | ['OverallQual']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('int', []) : 1 | ['OverallQual']\n    0.0s = Fit runtime\n    1 features in original data used to generate 1 features in processed data.\n    Train Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.03s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 1168, Val Rows: 292\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models ...\nFitting model: KNeighborsUnif ...\n    -51302.9262  = Validation score   (-root_mean_squared_error)\n    0.04s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    -51302.9262  = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    -61288.6739  = Validation score   (-root_mean_squared_error)\n    0.28s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    -48771.8547  = Validation score   (-root_mean_squared_error)\n    0.21s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestMSE ...\n    -47823.2599  = Validation score   (-root_mean_squared_error)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    -47813.9877  = Validation score   (-root_mean_squared_error)\n    0.13s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -47823.2599  = Validation score   (-root_mean_squared_error)\n    0.24s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    -47262.4708  = Validation score   (-root_mean_squared_error)\n    0.67s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: XGBoost ...\n    -47576.4125  = Validation score   (-root_mean_squared_error)\n    0.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    -47489.2325  = Validation score   (-root_mean_squared_error)\n    1.25s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMLarge ...\n    -47629.3214  = Validation score   (-root_mean_squared_error)\n    0.22s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    -47251.0188  = Validation score   (-root_mean_squared_error)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 3.77s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231210_090017/\")\n\n\n[1000]  valid_set's rmse: 61288.7\n[2000]  valid_set's rmse: 61288.7\n\n\n\nyyhat = predictr.predict(df_ts_)\n\n\nauto.quick_fit(\n    train_data=df_tr_,\n    label='SalePrice',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231210_090127/\"\n\n\nModel Prediction for SalePrice\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-52272.947289\n-50360.652261\n0.000823\n0.000755\n0.215841\n0.000823\n0.000755\n0.215841\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nOverallQual\n52258.930244\n2494.060513\n6.206537e-07\n5\n57394.235311\n47123.625176\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nOverallQual\nSalePrice\nSalePrice_pred\nerror\n\n\n\n\n1182\n10\n745000\n302755.9375\n442244.0625\n\n\n898\n9\n611657\n302755.9375\n308901.0625\n\n\n440\n10\n555000\n302755.9375\n252244.0625\n\n\n769\n8\n538000\n302755.9375\n235244.0625\n\n\n1243\n10\n465000\n302755.9375\n162244.0625\n\n\n1298\n10\n160000\n302755.9375\n142755.9375\n\n\n458\n8\n161000\n302755.9375\n141755.9375\n\n\n1211\n8\n164000\n302755.9375\n138755.9375\n\n\n58\n10\n438780\n302755.9375\n136024.0625\n\n\n991\n8\n168000\n302755.9375\n134755.9375\n\n\n\n\n\n\n\n\ndf_submission['SalePrice'] = yyhat \ndf_submission.to_csv(\"submission.csv\",index=False)\n\n\n!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"overallqual\"\n!rm submission.csv\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|██████████████████████████████████████| 21.4k/21.4k [00:01&lt;00:00, 11.6kB/s]\nSuccessfully submitted to House Prices - Advanced Regression Techniques\n\n\n\n\n\nimage.png\n\n\n\n1110/5012\n\n0.2214684756584198\n\n\n높아졌다잉"
  },
  {
    "objectID": "posts/13wk-54.out.html",
    "href": "posts/13wk-54.out.html",
    "title": "[STBDA2023] 13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-54.out.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-54.out.html#a.-y의-분포-xy의-관계-시각화",
    "title": "[STBDA2023] 13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='Weight_Loss',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nWeight_Loss\n10000\n5.11908\n6.092669\n-3.484888\n0.277797\n2.683447\n12.075458\n18.725299\nfloat64\n10000\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for Weight_Loss &gt;= 0.5\n\n\n\n\n\nFeature interaction between Exercise/Weight_Loss in train_data"
  },
  {
    "objectID": "posts/13wk-54.out.html#target-variable-analysis",
    "href": "posts/13wk-54.out.html#target-variable-analysis",
    "title": "[STBDA2023] 13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-54.out.html#b.-중요한-설명변수",
    "href": "posts/13wk-54.out.html#b.-중요한-설명변수",
    "title": "[STBDA2023] 13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data=df_train,\n    label='Weight_Loss',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231203_082842/\"\n\n\nModel Prediction for Weight_Loss\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-1.013205\n-0.973093\n0.005561\n0.003141\n0.168503\n0.005561\n0.003141\n0.168503\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nExercise\n6.735479\n0.105037\n7.094759e-09\n5\n6.951752\n6.519206\n\n\nSupplement\n4.018616\n0.073537\n1.344948e-08\n5\n4.170030\n3.867202\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nWeight_Loss_pred\nerror\n\n\n\n\n4639\nTrue\nFalse\n4.358143\n0.484748\n3.873395\n\n\n1683\nTrue\nTrue\n18.432093\n14.966088\n3.466005\n\n\n1275\nFalse\nTrue\n1.693150\n5.012056\n3.318906\n\n\n2631\nFalse\nTrue\n8.328789\n5.012056\n3.316733\n\n\n5334\nTrue\nFalse\n3.769029\n0.484748\n3.284281\n\n\n3437\nFalse\nFalse\n-3.205225\n0.039978\n3.245204\n\n\n2761\nFalse\nTrue\n1.853100\n5.012056\n3.158956\n\n\n9675\nTrue\nTrue\n18.070419\n14.966088\n3.104331\n\n\n3161\nFalse\nFalse\n-3.020254\n0.039978\n3.060232\n\n\n4637\nFalse\nTrue\n2.077349\n5.012056\n2.934706"
  },
  {
    "objectID": "posts/13wk-54.out.html#c.-관측치별-해석",
    "href": "posts/13wk-54.out.html#c.-관측치별-해석",
    "title": "[STBDA2023] 13wk-54: 체중감량(교호작용) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n- 2번관측치\n\ndf_train.iloc[[2]]\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n2\nTrue\nTrue\n13.824148\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[2]])\n\n2    14.999411\nName: Weight_Loss, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[2]]*1,\n    display_rows=True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n2\n1.0\n1.0\n13.824148\n\n\n\n\n\n\n\n\n\n\n- 5번관측치\n\ndf_train.iloc[[5]]\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n5\nTrue\nFalse\n-0.379401\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[5]])\n\n5    0.584295\nName: Weight_Loss, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[5]]*1,\n    display_rows=True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n5\n1.0\n0.0\n-0.379401\n\n\n\n\n\n\n\n\n\n\n- 10번관측치\n\ndf_train.iloc[[10]]\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n10\nFalse\nTrue\n4.356397\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[10]])\n\n10    4.990985\nName: Weight_Loss, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[10]]*1,\n    display_rows=True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n10\n0.0\n1.0\n4.356397"
  },
  {
    "objectID": "posts/04wk-014.out.html",
    "href": "posts/04wk-014.out.html",
    "title": "[STBDA2023] 04wk-014: 결측치 시각화, msno",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/04wk-014.out.html#a.-df.info",
    "href": "posts/04wk-014.out.html#a.-df.info",
    "title": "[STBDA2023] 04wk-014: 결측치 시각화, msno",
    "section": "A. df.info()",
    "text": "A. df.info()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   A       668 non-null    float64\n 1   B       656 non-null    float64\n 2   C       608 non-null    float64\n 3   D       668 non-null    float64\n 4   E       660 non-null    float64\ndtypes: float64(5)\nmemory usage: 39.2 KB"
  },
  {
    "objectID": "posts/04wk-014.out.html#b.-msno.bar---결측치-시각화-패키지",
    "href": "posts/04wk-014.out.html#b.-msno.bar---결측치-시각화-패키지",
    "title": "[STBDA2023] 04wk-014: 결측치 시각화, msno",
    "section": "B. msno.bar() <- 결측치 시각화 패키지",
    "text": "B. msno.bar() &lt;- 결측치 시각화 패키지\n\nmsno.bar(df)\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/04wk-014.out.html#a.-msno.matrix",
    "href": "posts/04wk-014.out.html#a.-msno.matrix",
    "title": "[STBDA2023] 04wk-014: 결측치 시각화, msno",
    "section": "A. msno.matrix()",
    "text": "A. msno.matrix()\n\nmsno.matrix(df)\n\n\n\n\n- 결측치의 패턴이란?\n\n하얀 부분이 결측치가 존재하는 부분\n오른쪽에 지ㅣㅈ직… 되어있는 부분이 0~5 에 있는 데이터의 분포를 확인 가능"
  },
  {
    "objectID": "posts/04wk-014.out.html#b.-msno.heatmap",
    "href": "posts/04wk-014.out.html#b.-msno.heatmap",
    "title": "[STBDA2023] 04wk-014: 결측치 시각화, msno",
    "section": "B. msno.heatmap()",
    "text": "B. msno.heatmap()\n\nmsno.heatmap(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n숫자가 클수록 비슷한 유형의 결측치라는 것 말한다.\n(A,B), (D,E)가 0.3 비슷한 정도를 볼 수 있음"
  },
  {
    "objectID": "posts/04wk-014.out.html#c.-msno.dendrogram",
    "href": "posts/04wk-014.out.html#c.-msno.dendrogram",
    "title": "[STBDA2023] 04wk-014: 결측치 시각화, msno",
    "section": "C. msno.dendrogram()",
    "text": "C. msno.dendrogram()\n\nmsno.dendrogram(df)\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/04wk-017.out.html",
    "href": "posts/04wk-017.out.html",
    "title": "[STBDA2023] 04wk-017: 취업, 로지스틱을 더 깊게",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임\n\n\n04wk-017: 취업, 로지스틱을 더 깊게\n최규빈\n2023-09-26\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-yZKLoD4xCQYvimA2q_8lCl&si=vBBY-dA7arD2SCSy\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\n\n\n\n3. 데이터 불러오기 \\(\\to\\) 학습\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\n\nX = df[['toeic','gpa']]\ny = df[['employment']]\npredictr = sklearn.linear_model.LogisticRegression()\npredictr.fit(X,y)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n\n4. yhat이 나오는 방식?\n- 확인: 무슨 수식에 의하여 나오긴함\n\npredictr.coef_, predictr.intercept_\n\n(array([[0.00571598, 2.46520018]]), array([-8.45433334]))\n\n\n\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n0      0.000523\n1      0.096780\n2      0.453003\n3      0.005627\n4      0.979312\n         ...   \n495    0.976295\n496    0.432939\n497    0.000855\n498    0.016991\n499    0.932777\nLength: 500, dtype: float64\n\n\n\n((v &gt; 0.5) == predictr.predict(X)).mean()\n\n1.0\n\n\n- 하여튼 아래와 같은 구조임\n(구조1)\n\n(구조2) – 단순화\n\n- v 값을 알고 싶다면 어쩌지?\n\nv[:5].round(3)\n\n0    0.001\n1    0.097\n2    0.453\n3    0.006\n4    0.979\ndtype: float64\n\n\n\npredictr.predict_proba(X)[:5].round(3) \n\narray([[0.999, 0.001],\n       [0.903, 0.097],\n       [0.547, 0.453],\n       [0.994, 0.006],\n       [0.021, 0.979]])\n\n\npredictr.predict_proba(X)에서 오른쪽이 v값(취업이 될 확률)\npredictr.predict_proba(X)에서 왼쪽 열은 취업이 안 될 확률"
  },
  {
    "objectID": "posts/06wk-025.out.html",
    "href": "posts/06wk-025.out.html",
    "title": "[STBDA2023] 06wk-025: 취업+각종영어점수, Lasso",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/06wk-025.out.html#a.-정확한-설명",
    "href": "posts/06wk-025.out.html#a.-정확한-설명",
    "title": "[STBDA2023] 06wk-025: 취업+각종영어점수, Lasso",
    "section": "A. 정확한 설명",
    "text": "A. 정확한 설명\n- 어려워요.."
  },
  {
    "objectID": "posts/06wk-025.out.html#b.-직관적-설명-엄밀하지-않은-설명",
    "href": "posts/06wk-025.out.html#b.-직관적-설명-엄밀하지-않은-설명",
    "title": "[STBDA2023] 06wk-025: 취업+각종영어점수, Lasso",
    "section": "B. 직관적 설명 (엄밀하지 않은 설명)",
    "text": "B. 직관적 설명 (엄밀하지 않은 설명)\n- 느낌: 몇 개의 toeic coef들로 쉽게 0.01을 만들게 해서는 안된다.\n\n아이디어1: 0.01을 동일한 값으로 균등하게 배분한다. – Ridge, L2-penalty\n아이디어2: 아주 적은숫자의 coef만을 살려두고 나머지 coef값은 0으로 강제한다. – Lasso, L1-penalty\n\n- 계수값이 0이라는 의미: 그 변수를 제거한것과 같은 효과\n- 아이디어2의 기원: y ~ toeic + gpa 가 트루이지만, y ~ toeic0 + gpa 으로 적합해도 괜찮잖아?\n- 진짜 학습된 계수값이 대부분 0인지 확인해보자.\n\nplt.plot(predictr.coef_[1:])"
  },
  {
    "objectID": "posts/06wk-025.out.html#c.-alpha-에-따른-변화-관찰",
    "href": "posts/06wk-025.out.html#c.-alpha-에-따른-변화-관찰",
    "title": "[STBDA2023] 06wk-025: 취업+각종영어점수, Lasso",
    "section": "C. \\(\\alpha\\) 에 따른 변화 관찰",
    "text": "C. \\(\\alpha\\) 에 따른 변화 관찰\n- 여러개의 predictor 학습\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train[['employment_score']]\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test[['employment_score']]\n## step2 \nalphas = np.linspace(0,2,100)\npredictrs = [sklearn.linear_model.Lasso(alpha=alpha) for alpha in alphas]\n## step3\nfor predictr in predictrs: \n    predictr.fit(X,y)\n## step4 : pass \n\n/tmp/ipykernel_488427/2200579212.py:12: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n  predictr.fit(X,y)\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e+01, tolerance: 3.337e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.730e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.734e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.620e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.391e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.141e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.634e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.121e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.523e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.895e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.027e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.049e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.655e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.628e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.093e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.556e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.539e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.488e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.407e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.367e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.261e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.332e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.488e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.628e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.387e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.166e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.010e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.831e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.670e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.446e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.999e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.776e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.467e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.330e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.288e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.197e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.040e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.853e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.622e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.327e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.026e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.745e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.505e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.301e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.121e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.950e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.692e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.502e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.333e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.099e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.543e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.289e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.832e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.662e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.520e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.475e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.420e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.361e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.737e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.617e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.472e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.327e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.186e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.167e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.743e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.946e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.406e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.662e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.827e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.072e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.953e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.231e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.107e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.934e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.787e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.613e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.524e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.098e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.087e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.080e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.073e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n\nplt.plot(predictrs[0].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[0].alpha))\nplt.plot(predictrs[50].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[50].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[-1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fdff090e970&gt;\n\n\n\n\n\nlasso는 계수들이 sparse하게 존재한다고 말함..듬성듬성!\n- predictor 들의 toeic 계수합은 여전히 0.01 근처\n\nprint(f'alpha={predictrs[0].alpha:.4f}\\tsum(toeic_coef)={predictrs[0].coef_[1:].sum()}')\nprint(f'alpha={predictrs[50].alpha:.4f}\\tsum(toeic_coef)={predictrs[50].coef_[1:].sum()}')\nprint(f'alpha={predictrs[-1].alpha:.4f}\\tsum(toeic_coef)={predictrs[-1].coef_[1:].sum()}')\n\nalpha=0.0000    sum(toeic_coef)=0.010291301406468494\nalpha=1.0101    sum(toeic_coef)=0.009986115762478664\nalpha=2.0000    sum(toeic_coef)=0.009864586871194559\n\n\n- number of non-zero coefs 를 시각화\n\nnon_zero_coefs = [(abs(predictr.coef_[1:])&gt;0).sum() for predictr in predictrs]\n\n\nplt.plot(alphas,non_zero_coefs)\n\n\n\n\nalpha가 커질수록 coef값들이 0이 되는게 많아지넹"
  },
  {
    "objectID": "posts/06wk-025.out.html#d.-coef를-0으로-만드는-수학적-장치",
    "href": "posts/06wk-025.out.html#d.-coef를-0으로-만드는-수학적-장치",
    "title": "[STBDA2023] 06wk-025: 취업+각종영어점수, Lasso",
    "section": "D. coef를 0으로 만드는 수학적 장치",
    "text": "D. coef를 0으로 만드는 수학적 장치\n- Ridge(복습): coef의 값들을 엔빵하는 수학적 장치\n\n패널티: 유사토익들의 계수값을 제곱한뒤 합치고(=L2-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n- Lasso: coef의 값들을 대부분 0으로 만드는 수학적 장치\n\n패널티: 유사토익들의 계수값의 절대값을 구한뒤에 합치고(=L1-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n- 사실 L1, L2 패널티에 따라서 이러한 결과가 나오는 것은 이해하기 어렵다. (그래서 취업/대학원 진학시 단골질문중 하나)"
  },
  {
    "objectID": "posts/10wk-036.out.html",
    "href": "posts/10wk-036.out.html",
    "title": "[STBDA2023] 10wk-036: 애니메이션",
    "section": "",
    "text": "10wk-036: 애니메이션\n최규빈\n2023-11-10\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-y9yXxaaoC_E512J-sCkurO&si=orvxib5Xb_wjcJAd\n\n\n2. Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython\n\n\n\n3. FuncAnimation\n- 모티브\n\nk = 6\nx = np.linspace(0,10,100)\ny = np.sin(0.1*k*x) \nplt.plot(x,y)\n\n\n\n\n\\(k=1,2,3,\\dots\\)로 바꾸면서 변화하는 그림을 연속으로 출력되게 하여 애니메이션으로 보고 싶다. 따라서\n\n하나의 고정된 그림을 정의하고\n그림안의 내용물을 frame에 따라 바꾸는 동작을 정의하여\n\n이들을 결합하는 전략을 생각해보자.\n- 위의코드는 아래와 같다.\n\nk = 4\nx = np.linspace(0,10,100)\ny = np.sin(0.1*k*x) \nfig = plt.figure() # 하나의 고정된 그림을 정의하는 코드 \nax = fig.gca() \nax.plot(x,y) # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 정의하는 코드 \n\n\n\n\n- 애니메이션\n\nfig = plt.figure() # 하나의 고정된 그림을 정의하는 코드 \n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca() \n    ax.clear()\n    x = np.linspace(0,10,100)\n    y = np.sin(0.1*frame*x) \n    ax.plot(x,y) # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 정의하는 코드 \n\n\nani = matplotlib.animation.FuncAnimation(\n    fig, # 하나의 고정된 그림\n    func, # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 함수로 정의하고, 그 함수를 넣음\n    frames=50 \n) \n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n4. HW\n\nx = np.linspace(0,10,100)\ny = np.sin(5*x) \nplt.plot(x,y)\n\n\n\n\n위의 그림을 이용하여 애니메이션을 만들어라. 이때 frame이 짝수일경우는 color=’C0’로 frame이 홀수일 경우는 color=’C1’으로 그린 그림이 나오도록 애니메이션을 구성하라.\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca() \n    ax.clear()\n    x = np.linspace(0,10,100)\n    y = np.sin(5*frame*x)\n    color = 'C0' if frame % 2 == 0 else 'C1'\n    ax.plot(x,y,color=color) # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 정의하는 코드 \n\n\nani = matplotlib.animation.FuncAnimation(\n    fig, \n    func, \n    frames=50 \n) \n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/13wk-48.out.html",
    "href": "posts/13wk-48.out.html",
    "title": "[STBDA2023] 13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-48.out.html#a.-맨날-쓰는-코드",
    "href": "posts/13wk-48.out.html#a.-맨날-쓰는-코드",
    "title": "[STBDA2023] 13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "A. 맨날 쓰는 코드",
    "text": "A. 맨날 쓰는 코드\n\n# step1 -- pass \n# step2\npredictr = TabularPredictor(label='sales')\n# step3 \npredictr.fit(df_train)\n# step4 \nyhat = predictr.predict(df_train)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231203_071110/\"\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231203_071110/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   673.03 GB / 982.82 GB (68.5%)\nTrain Data Rows:    100\nTrain Data Columns: 1\nLabel Column: sales\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n    Label info (max, min, mean, stddev): (61.561043278721556, 10.90026146402572, 33.97342, 10.63375)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    107392.5 MB\n    Train Data (Original)  Memory Usage: 0.0 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['temp']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['temp']\n    0.0s = Fit runtime\n    1 features in original data used to generate 1 features in processed data.\n    Train Data (Processed) Memory Usage: 0.0 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.03s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 80, Val Rows: 20\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models ...\nFitting model: KNeighborsUnif ...\n    -4.2111  = Validation score   (-root_mean_squared_error)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    -4.6714  = Validation score   (-root_mean_squared_error)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    -35.2477     = Validation score   (-root_mean_squared_error)\n    0.17s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    -5.3708  = Validation score   (-root_mean_squared_error)\n    0.1s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestMSE ...\n    -4.4041  = Validation score   (-root_mean_squared_error)\n    0.2s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: CatBoost ...\n    -3.8364  = Validation score   (-root_mean_squared_error)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -4.2375  = Validation score   (-root_mean_squared_error)\n    0.21s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    -3.7128  = Validation score   (-root_mean_squared_error)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: XGBoost ...\n    -4.0555  = Validation score   (-root_mean_squared_error)\n    0.06s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    -3.4399  = Validation score   (-root_mean_squared_error)\n    0.49s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMLarge ...\n    -3.979   = Validation score   (-root_mean_squared_error)\n    0.12s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    -3.4399  = Validation score   (-root_mean_squared_error)\n    0.15s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 2.22s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231203_071110/\")\n\n\n[1000]  valid_set's rmse: 5.45375\n[2000]  valid_set's rmse: 5.40245"
  },
  {
    "objectID": "posts/13wk-48.out.html#b.-적합결과-시각화",
    "href": "posts/13wk-48.out.html#b.-적합결과-시각화",
    "title": "[STBDA2023] 13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "B. 적합결과 시각화",
    "text": "B. 적합결과 시각화\n\nsns.scatterplot(df_train, x='temp',y='sales',label='y')\nsns.lineplot(df_train, x='temp',y=yhat,color='C1',linestyle='--',label='yhat')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fb3278fa880&gt;\n\n\n\n\n\n\n잘 맞춘다?"
  },
  {
    "objectID": "posts/13wk-48.out.html#c.-모형들-확인",
    "href": "posts/13wk-48.out.html#c.-모형들-확인",
    "title": "[STBDA2023] 13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "C. 모형들 확인",
    "text": "C. 모형들 확인\n\npredictr.leaderboard(silent=True)\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nNeuralNetTorch\n-3.439941\n0.002146\n0.485392\n0.002146\n0.485392\n1\nTrue\n10\n\n\n1\nWeightedEnsemble_L2\n-3.439941\n0.002357\n0.635780\n0.000212\n0.150388\n2\nTrue\n12\n\n\n2\nNeuralNetFastAI\n-3.712791\n0.003752\n0.395201\n0.003752\n0.395201\n1\nTrue\n8\n\n\n3\nCatBoost\n-3.836449\n0.000815\n0.176307\n0.000815\n0.176307\n1\nTrue\n6\n\n\n4\nLightGBMLarge\n-3.978956\n0.000636\n0.123188\n0.000636\n0.123188\n1\nTrue\n11\n\n\n5\nXGBoost\n-4.055491\n0.001284\n0.062169\n0.001284\n0.062169\n1\nTrue\n9\n\n\n6\nKNeighborsUnif\n-4.211090\n0.005753\n0.010933\n0.005753\n0.010933\n1\nTrue\n1\n\n\n7\nExtraTreesMSE\n-4.237516\n0.014664\n0.206832\n0.014664\n0.206832\n1\nTrue\n7\n\n\n8\nRandomForestMSE\n-4.404096\n0.013223\n0.202250\n0.013223\n0.202250\n1\nTrue\n5\n\n\n9\nKNeighborsDist\n-4.671405\n0.002980\n0.002525\n0.002980\n0.002525\n1\nTrue\n2\n\n\n10\nLightGBM\n-5.370826\n0.000522\n0.100535\n0.000522\n0.100535\n1\nTrue\n4\n\n\n11\nLightGBMXT\n-35.247682\n0.000819\n0.170966\n0.000819\n0.170966\n1\nTrue\n3"
  },
  {
    "objectID": "posts/13wk-48.out.html#d.-최강모형의-r2_score-계산",
    "href": "posts/13wk-48.out.html#d.-최강모형의-r2_score-계산",
    "title": "[STBDA2023] 13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "D. 최강모형의 r2_score 계산",
    "text": "D. 최강모형의 r2_score 계산\n- r2_score 계산 – 방법1\n\n_y = df_train.sales\n_yhat = predictr.predict(df_train)\nsklearn.metrics.r2_score(_y,_yhat)\n\n0.929782932976498\n\n\n- r2_score 계산 – 방법2\n\n_y = df_train.sales\n_yhat = predictr.predict(df_train,model='NeuralNetTorch')\nsklearn.metrics.r2_score(_y,_yhat)\n\n0.929782932976498"
  },
  {
    "objectID": "posts/13wk-48.out.html#e.-특정모형의-적합값-구경",
    "href": "posts/13wk-48.out.html#e.-특정모형의-적합값-구경",
    "title": "[STBDA2023] 13wk-48: 아이스크림 / 자료분석(Autogluon)",
    "section": "E. 특정모형의 적합값 구경",
    "text": "E. 특정모형의 적합값 구경\n- XGBoost 궁금해..\n\n_y = df_train.sales\n_yhat = predictr.predict(df_train,model='XGBoost')\nsklearn.metrics.r2_score(_y,_yhat)\n\n0.9516437954914487\n\n\n\nsns.scatterplot(df_train, x='temp', y='sales', label='y')\nsns.lineplot(df_train, x='temp',y=_yhat,color='C1',linestyle='--',label='yhat')\nax = plt.gca()\nax.set_title(\"XGBoost\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fb3278f8490&gt;"
  },
  {
    "objectID": "posts/06wk-024.out.html",
    "href": "posts/06wk-024.out.html",
    "title": "[STBDA2023] 06wk-024: 취업+각종영어점수, RidgeCV",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임\n\n\n06wk-024: 취업+각종영어점수, RidgeCV\n최규빈\n2023-10-05\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-wCfFLHO2uCcH6izfJPTKro&si=YYn_bwPwcuwTk0Ld\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport sklearn.linear_model\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. RidgeCV\n- RidgeCV 클래스에서 모형을 선택해보자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV()\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV()\n\n\n\npredictr.score(X,y)\n\n0.9999996840224943\n\n\n\npredictr.score(XX,yy)\n\n0.11914945948787758\n\n\n- alpha들의 후보를 우리가 직접 선정하자.\n\nsklearn.linear_model.RidgeCV?\n\n\nInit signature:\nsklearn.linear_model.RidgeCV(\n    alphas=(0.1, 1.0, 10.0),\n    *,\n    fit_intercept=True,\n    scoring=None,\n    cv=None,\n    gcv_mode=None,\n    store_cv_values=False,\n    alpha_per_target=False,\n)\nDocstring:     \nRidge regression with built-in cross-validation.\nSee glossary entry for :term:`cross-validation estimator`.\nBy default, it performs efficient Leave-One-Out Cross-Validation.\nRead more in the :ref:`User Guide &lt;ridge_regression&gt;`.\nParameters\n----------\nalphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n    Array of alpha values to try.\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n    :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`.\n    If using Leave-One-Out cross-validation, alphas must be positive.\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\nscoring : str, callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n    If None, the negative mean squared error if cv is 'auto' or None\n    (i.e. when using leave-one-out cross-validation), and r2 score\n    otherwise.\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n    - None, to use the efficient Leave-One-Out cross-validation\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n    :class:`~sklearn.model_selection.KFold` is used.\n    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various\n    cross-validation strategies that can be used here.\ngcv_mode : {'auto', 'svd', 'eigen'}, default='auto'\n    Flag indicating which strategy to use when performing\n    Leave-One-Out Cross-Validation. Options are::\n        'auto' : use 'svd' if n_samples &gt; n_features, otherwise use 'eigen'\n        'svd' : force use of singular value decomposition of X when X is\n            dense, eigenvalue decomposition of X^T.X when X is sparse.\n        'eigen' : force computation via eigendecomposition of X.X^T\n    The 'auto' mode is the default and is intended to pick the cheaper\n    option of the two depending on the shape of the training data.\nstore_cv_values : bool, default=False\n    Flag indicating if the cross-validation values corresponding to\n    each alpha should be stored in the ``cv_values_`` attribute (see\n    below). This flag is only compatible with ``cv=None`` (i.e. using\n    Leave-One-Out Cross-Validation).\nalpha_per_target : bool, default=False\n    Flag indicating whether to optimize the alpha value (picked from the\n    `alphas` parameter list) for each target separately (for multi-output\n    settings: multiple prediction targets). When set to `True`, after\n    fitting, the `alpha_` attribute will contain a value for each target.\n    When set to `False`, a single alpha is used for all targets.\n    .. versionadded:: 0.24\nAttributes\n----------\ncv_values_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional\n    Cross-validation values for each alpha (only available if\n    ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been\n    called, this attribute will contain the mean squared errors if\n    `scoring is None` otherwise it will contain standardized per point\n    prediction values.\ncoef_ : ndarray of shape (n_features) or (n_targets, n_features)\n    Weight vector(s).\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\nalpha_ : float or ndarray of shape (n_targets,)\n    Estimated regularization parameter, or, if ``alpha_per_target=True``,\n    the estimated regularization parameter for each target.\nbest_score_ : float or ndarray of shape (n_targets,)\n    Score of base estimator with best alpha, or, if\n    ``alpha_per_target=True``, a score for each target.\n    .. versionadded:: 0.23\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n    .. versionadded:: 0.24\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n    .. versionadded:: 1.0\nSee Also\n--------\nRidge : Ridge regression.\nRidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\nRidgeClassifierCV : Ridge classifier with built-in cross validation.\nExamples\n--------\n&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.linear_model import RidgeCV\n&gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)\n&gt;&gt;&gt; clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n&gt;&gt;&gt; clf.score(X, y)\n0.5166...\nFile:           ~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py\nType:           ABCMeta\nSubclasses:     \n\n\n\nalphas=(0.1, 1.0, 10.0) 중 값이 가장 좋은 걸 선택해 주는 듯.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV(alphas=[5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8])\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])\n\n\n\npredictr.score(X,y)\n\n0.752126856015936\n\n\n\npredictr.score(XX,yy)\n\n0.7450309251010896\n\n\n\npredictr.alpha_\n\n50000000.0\n\n\n참고로 이 적합결과는 아래의 코드를 실행한것과 같다\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Ridge(alpha=50000000.0)\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidge(alpha=50000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=50000000.0)\n\n\n\npredictr.score(X,y)\n\n0.752126856015936\n\n\n\npredictr.score(XX,yy)\n\n0.7450309251010894"
  },
  {
    "objectID": "posts/13wk-52.out.html",
    "href": "posts/13wk-52.out.html",
    "title": "[STBDA2023] 13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-52.out.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-52.out.html#a.-y의-분포-xy의-관계-시각화",
    "title": "[STBDA2023] 13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='employment',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nunique\ntop\nfreq\ndtypes\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nemployment\n1500\n2\nNo\n755\nobject\n\n\nobject\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for employment &gt;= 0.5\n\n\n\n\n\nFeature interaction between toiec/employment in train_data"
  },
  {
    "objectID": "posts/13wk-52.out.html#target-variable-analysis",
    "href": "posts/13wk-52.out.html#target-variable-analysis",
    "title": "[STBDA2023] 13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-52.out.html#b.-중요한-설명변수",
    "href": "posts/13wk-52.out.html#b.-중요한-설명변수",
    "title": "[STBDA2023] 13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data= df_train,\n    label = 'employment',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231203_075536/\"\n\n\nModel Prediction for employment\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n0.886667\n0.838095\n0.000841\n0.00106\n0.152619\n0.000841\n0.00106\n0.152619\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\ntoiec\n0.285778\n0.021175\n0.000004\n5\n0.329378\n0.242177\n\n\ngpa\n0.164444\n0.018592\n0.000019\n5\n0.202727\n0.126162\n\n\nbalance5\n0.012889\n0.002434\n0.000146\n5\n0.017901\n0.007877\n\n\nbalance1\n0.005333\n0.012727\n0.200894\n5\n0.031538\n-0.020872\n\n\nbalance9\n0.005333\n0.002981\n0.008065\n5\n0.011472\n-0.000805\n\n\nbalance6\n0.003556\n0.004608\n0.079776\n5\n0.013044\n-0.005933\n\n\nbalance0\n0.001778\n0.003296\n0.147128\n5\n0.008564\n-0.005009\n\n\nbalance2\n0.001778\n0.003975\n0.186950\n5\n0.009963\n-0.006407\n\n\nbalance8\n0.000889\n0.004608\n0.344229\n5\n0.010377\n-0.008599\n\n\nbalance3\n0.000889\n0.002534\n0.238310\n5\n0.006106\n-0.004328\n\n\nbalance7\n0.000444\n0.003975\n0.407451\n5\n0.008630\n-0.007741\n\n\nbalance4\n-0.000889\n0.004608\n0.655771\n5\n0.008599\n-0.010377\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\ntoiec\ngpa\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\nemployment\nNo\nYes\nerror\n\n\n\n\n4344\n505\n3.79\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\nTrue\nNo\n0.160372\n0.839628\n0.679257\n\n\n1365\n415\n2.08\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nYes\n0.822812\n0.177188\n0.645625\n\n\n2371\n515\n3.85\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nNo\n0.185117\n0.814883\n0.629766\n\n\n2523\n425\n4.05\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nFalse\nNo\n0.193315\n0.806685\n0.613371\n\n\n4617\n565\n3.63\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nNo\n0.238860\n0.761140\n0.522280\n\n\n3486\n90\n3.57\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nYes\n0.738586\n0.261414\n0.477172\n\n\n1982\n720\n3.10\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nNo\n0.267186\n0.732814\n0.465628\n\n\n2987\n960\n2.05\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nNo\n0.282117\n0.717883\n0.435766\n\n\n1396\n205\n3.11\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nYes\n0.701129\n0.298871\n0.402259\n\n\n4828\n295\n3.46\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nYes\n0.683596\n0.316404\n0.367191\n\n\n\n\n\n\n\nRows with the least distance vs other class\n\n\nRows in this category are the closest to the decision boundary vs the other class and are good candidates for additional labeling\n\n\n\n\n\n\n\n\n\ntoiec\ngpa\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\nemployment\nNo\nYes\nerror\n\n\n\n\n2983\n470\n2.67\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\nNo\n0.496940\n0.503060\n0.006120\n\n\n1603\n360\n3.42\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nYes\n0.505437\n0.494563\n0.010875\n\n\n3951\n110\n3.87\nTrue\nFalse\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nNo\n0.490789\n0.509211\n0.018421\n\n\n2695\n165\n4.15\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\nYes\n0.510669\n0.489331\n0.021338\n\n\n1297\n785\n1.70\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nYes\n0.511867\n0.488133\n0.023734\n\n\n2595\n560\n2.75\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\nYes\n0.516889\n0.483111\n0.033778\n\n\n4027\n865\n1.10\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nYes\n0.518515\n0.481485\n0.037030\n\n\n1388\n935\n1.10\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nYes\n0.520431\n0.479569\n0.040863\n\n\n3056\n495\n2.75\nTrue\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nYes\n0.520479\n0.479521\n0.040959\n\n\n2122\n820\n1.88\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nNo\n0.478370\n0.521630\n0.043259"
  },
  {
    "objectID": "posts/13wk-52.out.html#c.-관측치별-해석",
    "href": "posts/13wk-52.out.html#c.-관측치별-해석",
    "title": "[STBDA2023] 13wk-52: 취업(오버피팅) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n- 0번관측치\n\ndf_train.iloc[[0]]\n\n\n\n\n\n\n\n\ntoiec\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n4431\n865\n3.77\nYes\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[0]])\n\n4431    Yes\nName: employment, dtype: object\n\n\n\npredictr.predict_proba(df_train.iloc[[0]])\n\n\n\n\n\n\n\n\nNo\nYes\n\n\n\n\n4431\n0.03926\n0.96074\n\n\n\n\n\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[0]]*1,\n    display_rows=True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\ntoiec\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n4431\n865\n3.77\nYes\n1\n0\n0\n1\n1\n0\n1\n0\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n# 떨어진 이유\n\n- 1번관측치\n\ndf_train.iloc[[1]]\n\n\n\n\n\n\n\n\ntoiec\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n2162\n605\n2.7\nYes\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[1]])\n\n2162    Yes\nName: employment, dtype: object\n\n\n\npredictr.predict_proba(df_train.iloc[[1]])\n\n\n\n\n\n\n\n\nNo\nYes\n\n\n\n\n2162\n0.297759\n0.702241\n\n\n\n\n\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[1]]*1,\n    display_rows=True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\ntoiec\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n2162\n605\n2.7\nYes\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n# 합격한이유"
  },
  {
    "objectID": "posts/06wk-023.out.html",
    "href": "posts/06wk-023.out.html",
    "title": "[STBDA2023] 06wk-023: 취업+각종영어점수, Ridge",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/06wk-023.out.html#a.-정확한-설명",
    "href": "posts/06wk-023.out.html#a.-정확한-설명",
    "title": "[STBDA2023] 06wk-023: 취업+각종영어점수, Ridge",
    "section": "A. 정확한 설명",
    "text": "A. 정확한 설명\n- SVD를 이용하여 이론적인 계산하면 sklearn.linear_model.LinearRegression()로 적합한 결과보다 sklearn.linear_model.Ridge()로 적합한 결과를 더 좋게 만드는 \\(\\alpha\\)가 항상 존재함을 증명할 수 있음."
  },
  {
    "objectID": "posts/06wk-023.out.html#b.-직관적-설명-엄밀하지-않은-설명",
    "href": "posts/06wk-023.out.html#b.-직관적-설명-엄밀하지-않은-설명",
    "title": "[STBDA2023] 06wk-023: 취업+각종영어점수, Ridge",
    "section": "B. 직관적 설명 (엄밀하지 않은 설명)",
    "text": "B. 직관적 설명 (엄밀하지 않은 설명)\n\nStep1: LinearRegression은 왜 망했는가?\n- 토익의 계수는 실제로 \\(\\frac{1}{100}\\)이다. 적딩히\n\ntoeic_coef + … + toeic499_coef \\(\\approx\\) 0.01 이라면\n\n대충 맞는 답이다.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.LinearRegression()\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1171\n\n\n\ns= pd.Series(predictr.coef_)\ns.index = X.columns\ns[1:].sum()\n\n0.009490999015123288\n\n\n- 그런데 사실 저 0.01이라는 값은 몇개의 계수만 있어도 만들 수 있다. (toeic2와 toeic3에 해당하는 계수)\n\ns['toeic2']+s['toeic3'] \n\n3.7802381107347816e-05\n\n\n\n0.01&gt; s['toeic2']+s['toeic3'] \n\nTrue\n\n\n- 이런논리로 치면 toeic2, toeic3에 해당하는 계수만 있다면 사실 \\(y\\)를 설명하는데 충분했고, 나머지는 불필요한 특징이 된다. (그리고 불필요한 특징은 오버피팅을 유발한다)\n\n\nStep2: Ridge의 아이디어\n- Ridge의 아이디어: toeic2, toeic3 와 같이 몇개의 변수로만 0.01이라는 수를 설명할 수 없도록 “강제”하자. 즉 몇개의 변수로만 0.01이라는 수를 설명할 수 없도록 “패널티”를 주자.\n- 패널티: 유사토익들의 계수값을 제곱한뒤 합치고(=L2-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n이러한 패널티를 줄 경우 결과적으로 0.01의 값이 “동일하게 나누어져서” 나오는 값(=\\(\\frac{1}{100}\\frac{1}{501}\\))이 계수값으로 추정된다. (왜? – 요건 정확하게 이해하는게 사실 지금은 힘듬)\n\n- 잘 적용된 Ridge의 결과를 보면 아래와 같이 계수값이 저장되어 있음.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=5e8)\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.7507\ntest_score: 0.7438\n\n\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa         0.000001\ntoeic       0.000019\ntoeic0      0.000018\ntoeic1      0.000018\ntoeic2      0.000019\n              ...   \ntoeic495    0.000018\ntoeic496    0.000019\ntoeic497    0.000019\ntoeic498    0.000019\ntoeic499    0.000019\nLength: 502, dtype: float64\n\n\n계수들의 값이 대충 아래와 같이 비슷하게 설정되어 있음\n\n0.01/501\n\n1.9960079840319362e-05\n\n\n- 결국 Ridge를 사용하면 계수들의 값이 “동일하게 나누어지는” 효과가 나타남\n- 패널티를 주는 정도? \\(\\alpha\\)로 조절함.. \\(\\alpha\\)를 크게 할수록 패널티를 많이줌"
  },
  {
    "objectID": "posts/06wk-023.out.html#c.-alpha에-따른-계수값-변화",
    "href": "posts/06wk-023.out.html#c.-alpha에-따른-계수값-변화",
    "title": "[STBDA2023] 06wk-023: 취업+각종영어점수, Ridge",
    "section": "C. \\(\\alpha\\)에 따른 계수값 변화",
    "text": "C. \\(\\alpha\\)에 따른 계수값 변화\n- 여러개의 predictor 학습\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\nalphas = [5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8]  # 5x10^2=500, 5x10^3=5000, ...\npredictrs = [sklearn.linear_model.Ridge(alpha=alpha) for alpha in alphas]\n## step3 \nfor predictr in predictrs:\n    predictr.fit(X,y)\n## step4 -- pass \n\n- 계수값 시각화\n\npredictrs[0].alpha\n\n500.0\n\n\n\nplt.plot(predictrs[0].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[0].alpha)) #toeic\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f9225ff2eb0&gt;\n\n\n\n\n\n\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[3].alpha))\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.legend()\n\n\n\n\n\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[-1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f9224583610&gt;\n\n\n\n\n\n- 직관: 마지막 predictor의 계수값을 살펴보자.\n\npredictrs[-1].coef_\n\narray([1.10421248e-06, 1.89938091e-05, 1.77768343e-05, 1.82118332e-05,\n       1.90895673e-05, 1.87128138e-05, 1.90343037e-05, 1.82483251e-05,\n       1.90405022e-05, 1.85802628e-05, 1.90021086e-05, 1.88952130e-05,\n       1.96003229e-05, 1.89154663e-05, 1.86638217e-05, 1.92666606e-05,\n       1.97107043e-05, 1.92214868e-05, 1.92961317e-05, 1.93321368e-05,\n       1.92194541e-05, 1.85663279e-05, 1.86805137e-05, 1.81649873e-05,\n       1.78656367e-05, 1.83171419e-05, 1.94428947e-05, 1.89710925e-05,\n       2.00598946e-05, 1.88384883e-05, 1.98903125e-05, 1.81113551e-05,\n       1.85043847e-05, 1.84424971e-05, 1.91508275e-05, 1.97427867e-05,\n       1.93598061e-05, 1.98264264e-05, 1.89934042e-05, 1.84770850e-05,\n       1.83617634e-05, 1.79346774e-05, 1.84943159e-05, 1.89803006e-05,\n       1.78633749e-05, 1.80073666e-05, 1.85664525e-05, 1.97390143e-05,\n       1.86574281e-05, 1.92233226e-05, 1.91281904e-05, 1.85617627e-05,\n       1.83939489e-05, 1.84309427e-05, 1.88142167e-05, 1.84159665e-05,\n       1.94078579e-05, 1.84515402e-05, 1.88107980e-05, 1.85889903e-05,\n       1.89357356e-05, 1.88750847e-05, 1.92107444e-05, 1.81799279e-05,\n       1.92122152e-05, 1.97863670e-05, 1.89851436e-05, 1.88974919e-05,\n       1.88566578e-05, 1.95841935e-05, 1.86398380e-05, 1.95801159e-05,\n       1.87550098e-05, 1.87392625e-05, 1.87462595e-05, 1.96056001e-05,\n       1.80626630e-05, 1.88237701e-05, 1.83108446e-05, 1.88087164e-05,\n       1.84723703e-05, 1.84767748e-05, 1.89267252e-05, 1.87604297e-05,\n       1.86945591e-05, 1.92924236e-05, 1.77843453e-05, 1.85415541e-05,\n       1.91448999e-05, 1.98281375e-05, 1.97994651e-05, 1.86653004e-05,\n       1.87298830e-05, 1.87474975e-05, 1.90018315e-05, 1.92043808e-05,\n       1.88941675e-05, 1.81646176e-05, 1.91508494e-05, 2.04322537e-05,\n       1.92111546e-05, 1.93061022e-05, 1.92088349e-05, 1.80206353e-05,\n       1.89399818e-05, 1.96895533e-05, 1.94410839e-05, 1.92051217e-05,\n       1.84961416e-05, 1.89785667e-05, 1.92235780e-05, 1.86729143e-05,\n       1.88439733e-05, 1.76776615e-05, 1.87493841e-05, 1.86986837e-05,\n       1.81917859e-05, 1.94657238e-05, 1.82063420e-05, 1.78143049e-05,\n       1.88432683e-05, 1.90674860e-05, 1.86411824e-05, 1.93286721e-05,\n       1.75163829e-05, 1.86852659e-05, 2.02343956e-05, 1.82025623e-05,\n       1.89153395e-05, 1.98862774e-05, 1.94775038e-05, 1.90665531e-05,\n       1.94170642e-05, 1.88227118e-05, 1.88792179e-05, 1.89712787e-05,\n       1.87855482e-05, 1.87895464e-05, 2.00798925e-05, 1.97167119e-05,\n       1.91644137e-05, 1.90990710e-05, 1.85836048e-05, 1.82346595e-05,\n       1.85731253e-05, 1.84871242e-05, 1.90728256e-05, 1.90277156e-05,\n       1.93085319e-05, 1.91719254e-05, 1.80097271e-05, 1.82517485e-05,\n       1.90904218e-05, 1.85232604e-05, 1.88184612e-05, 1.84002976e-05,\n       2.00337440e-05, 1.86478638e-05, 1.93507546e-05, 1.85547358e-05,\n       1.97154574e-05, 1.91189346e-05, 1.93320777e-05, 1.85313268e-05,\n       1.91085306e-05, 1.88406812e-05, 1.87444892e-05, 1.96637559e-05,\n       1.83552699e-05, 1.80759243e-05, 1.94662845e-05, 1.93761303e-05,\n       1.98339288e-05, 1.87139235e-05, 1.91131387e-05, 1.85801855e-05,\n       1.91544816e-05, 1.98413649e-05, 1.84027849e-05, 1.81842651e-05,\n       1.95888229e-05, 1.80738476e-05, 1.92457286e-05, 1.91474170e-05,\n       1.88737956e-05, 1.78029998e-05, 1.97734483e-05, 1.92409710e-05,\n       1.97346045e-05, 1.99425451e-05, 1.89157923e-05, 1.82538525e-05,\n       1.87475300e-05, 1.79663692e-05, 1.94360535e-05, 1.93333725e-05,\n       1.81368431e-05, 1.91860664e-05, 2.03648683e-05, 1.92870391e-05,\n       1.92561212e-05, 1.92408929e-05, 1.77556464e-05, 1.89317813e-05,\n       1.95230859e-05, 1.91845519e-05, 1.88923023e-05, 1.88368476e-05,\n       1.89013580e-05, 1.82113056e-05, 1.86295402e-05, 1.92236940e-05,\n       1.80025543e-05, 1.92322271e-05, 1.80917953e-05, 1.87188051e-05,\n       1.93772655e-05, 1.87894009e-05, 1.86773984e-05, 1.96830961e-05,\n       1.94593808e-05, 1.99377297e-05, 1.85707832e-05, 1.88667594e-05,\n       1.85589760e-05, 1.98498326e-05, 1.88878514e-05, 1.90686529e-05,\n       1.86868639e-05, 1.90576790e-05, 1.95494214e-05, 1.86567117e-05,\n       1.85992014e-05, 1.77199587e-05, 1.82193592e-05, 1.90965903e-05,\n       1.96016869e-05, 1.88116657e-05, 1.81131528e-05, 1.85436209e-05,\n       1.92951259e-05, 1.92495993e-05, 1.84570073e-05, 1.94529446e-05,\n       1.92760629e-05, 1.92236816e-05, 1.85750512e-05, 1.95451343e-05,\n       1.82912208e-05, 1.88851896e-05, 1.86295173e-05, 1.84150640e-05,\n       1.95101106e-05, 1.98423439e-05, 1.88687440e-05, 1.91657943e-05,\n       1.89387389e-05, 1.89907539e-05, 1.90653825e-05, 1.80854343e-05,\n       1.86906336e-05, 1.85793308e-05, 1.84992786e-05, 1.93964742e-05,\n       1.83344151e-05, 1.89611068e-05, 1.91457644e-05, 1.88755070e-05,\n       1.98511526e-05, 1.93068196e-05, 1.93316489e-05, 1.89507435e-05,\n       1.89083004e-05, 1.91358509e-05, 1.87803906e-05, 1.78160168e-05,\n       1.94603877e-05, 2.02569965e-05, 1.87423291e-05, 1.94609617e-05,\n       1.91292677e-05, 1.85958571e-05, 1.88629266e-05, 1.90600256e-05,\n       1.82221314e-05, 1.95093258e-05, 1.89176339e-05, 2.00028045e-05,\n       1.94052035e-05, 1.86744967e-05, 1.89125601e-05, 2.02089363e-05,\n       1.80569192e-05, 2.02141130e-05, 1.93147541e-05, 1.89011113e-05,\n       1.93335891e-05, 1.96767360e-05, 1.90364715e-05, 1.94635849e-05,\n       1.90397143e-05, 1.91973258e-05, 1.85857694e-05, 1.91487106e-05,\n       1.92897509e-05, 1.99589223e-05, 1.89690091e-05, 1.90089893e-05,\n       1.80391078e-05, 1.89867708e-05, 1.91430968e-05, 1.92719424e-05,\n       1.95648244e-05, 1.85975115e-05, 1.92077870e-05, 1.84415844e-05,\n       1.88715614e-05, 1.85970322e-05, 1.93261490e-05, 1.86726361e-05,\n       1.97716032e-05, 1.92749150e-05, 2.00954709e-05, 1.90876286e-05,\n       1.89190693e-05, 1.98831620e-05, 1.91612367e-05, 1.86269524e-05,\n       1.89155394e-05, 1.89824518e-05, 1.98347756e-05, 1.86788886e-05,\n       1.83508292e-05, 1.85069060e-05, 1.86909372e-05, 1.85978543e-05,\n       1.88150510e-05, 1.89755849e-05, 1.90099289e-05, 1.90515657e-05,\n       1.93189513e-05, 1.82151178e-05, 1.78471089e-05, 1.91763316e-05,\n       1.84903926e-05, 1.92863572e-05, 1.90497739e-05, 1.87657428e-05,\n       1.87801680e-05, 1.85137448e-05, 1.91226761e-05, 1.94084785e-05,\n       1.81950620e-05, 1.81823646e-05, 1.87513814e-05, 1.97922951e-05,\n       1.87200102e-05, 1.98409879e-05, 1.85874173e-05, 1.90513332e-05,\n       1.85234477e-05, 1.81902197e-05, 1.76367508e-05, 1.90389194e-05,\n       1.85299355e-05, 1.95358518e-05, 1.81772601e-05, 1.93671350e-05,\n       1.91528856e-05, 1.91322975e-05, 1.85830738e-05, 1.85626882e-05,\n       1.86250726e-05, 1.84514809e-05, 1.86800234e-05, 1.89256964e-05,\n       1.90280385e-05, 1.88870537e-05, 1.86929332e-05, 1.95167742e-05,\n       1.86377119e-05, 1.93693632e-05, 1.94429807e-05, 1.90730542e-05,\n       1.86276638e-05, 1.86225787e-05, 1.87333026e-05, 1.94293224e-05,\n       1.87174307e-05, 1.93106731e-05, 1.91898445e-05, 1.91446507e-05,\n       1.83627209e-05, 1.85185991e-05, 1.90680366e-05, 1.88180597e-05,\n       1.86586581e-05, 1.80051184e-05, 1.83329730e-05, 1.82088945e-05,\n       1.87516598e-05, 1.82744310e-05, 1.90219092e-05, 1.89098591e-05,\n       1.89001214e-05, 1.90959896e-05, 1.77157866e-05, 1.91760361e-05,\n       1.80496598e-05, 1.85629242e-05, 1.93527162e-05, 1.85046434e-05,\n       1.97977476e-05, 1.82757747e-05, 1.92849021e-05, 1.86829990e-05,\n       1.86752898e-05, 1.95540241e-05, 1.92250030e-05, 1.84817730e-05,\n       1.94636774e-05, 1.86057300e-05, 1.90096458e-05, 1.91037821e-05,\n       1.98095086e-05, 1.92558748e-05, 1.94175627e-05, 1.86155519e-05,\n       1.91386204e-05, 1.89659072e-05, 1.89507918e-05, 1.88868989e-05,\n       1.91223138e-05, 1.81488441e-05, 1.95885497e-05, 1.87850789e-05,\n       1.90457546e-05, 1.96549561e-05, 1.86983597e-05, 1.89788151e-05,\n       1.98384237e-05, 1.99479277e-05, 1.91275095e-05, 1.89970341e-05,\n       1.85749782e-05, 1.91683345e-05, 1.91850806e-05, 1.97386011e-05,\n       1.93320833e-05, 1.92560345e-05, 1.85426153e-05, 1.85185853e-05,\n       1.85764448e-05, 1.94279426e-05, 1.97685699e-05, 1.91733090e-05,\n       1.84972022e-05, 1.89924907e-05, 1.83467563e-05, 1.95149016e-05,\n       1.84410610e-05, 1.86536281e-05, 1.88181888e-05, 1.85487807e-05,\n       1.88565643e-05, 1.89056942e-05, 1.95082352e-05, 1.91711709e-05,\n       1.91422027e-05, 1.91363321e-05, 1.89114818e-05, 1.85390554e-05,\n       1.92949067e-05, 1.88019353e-05, 1.85332879e-05, 1.86699430e-05,\n       1.96934870e-05, 2.01293426e-05, 1.81411289e-05, 1.86806981e-05,\n       1.90987154e-05, 1.85866377e-05, 1.96875267e-05, 1.88785203e-05,\n       1.94435510e-05, 1.85812461e-05, 1.97178935e-05, 1.90067232e-05,\n       2.02306858e-05, 1.86213361e-05, 1.94255182e-05, 1.86417320e-05,\n       1.95689564e-05, 1.97728792e-05, 1.94352125e-05, 1.93768903e-05,\n       1.90643113e-05, 1.79709383e-05, 1.90573271e-05, 1.85638225e-05,\n       1.91337229e-05, 1.86437625e-05])\n\n\n\n불필요한 변수가 나올 수 없는 구조가 되어버렸음 (한 두개로 0.01을 만들 수 없음)\n모든 변수는 대략 2e-5(\\(\\approx \\frac{1}{100}\\frac{1}{501}\\))정도 만큼 똑같이 중요하다고 생각된다.\n고급: 살짝 1/(100*501)보다 전체적으로 값이 작아보이는데, 이는 기분탓이 아님 (Ridge 특징)\n\n\n1/100*1/501\n\n1.9960079840319362e-05"
  },
  {
    "objectID": "posts/06wk-023.out.html#d.-alpha에-따른-실험내용-정리",
    "href": "posts/06wk-023.out.html#d.-alpha에-따른-실험내용-정리",
    "title": "[STBDA2023] 06wk-023: 취업+각종영어점수, Ridge",
    "section": "D. \\(\\alpha\\)에 따른 실험내용 정리",
    "text": "D. \\(\\alpha\\)에 따른 실험내용 정리\n- 예비개념: L2-penalty는 그냥 대충 분산같은것..\n\nx = np.random.randn(5)\nl2_penalty = (x**2).sum()\nl2_penalty, 5*(x.var()+(x.mean()**2))\n\n(4.573810681546266, 4.573810681546266)\n\n\n- \\(\\alpha\\)가 커질수록 생기는 일\n\n크게 느낀것: 계수들의 값이 점점 비슷해짐 –&gt; 계수들의 값들을 모아서 분산을 구하면 작아진다는 의미 –&gt; L2-penalty 가 작아진다는 의미\n미묘하게 느껴지는 점: toeic, 그리고 toeic0~toeic499 까지의 계수총합은 0.01이 되어야 하는데, 그 총합이 미묘하게 작어지는 느낌.\n\n\nfor predictr in predictrs: \n    print(\n        f'alpha={predictr.alpha:.2e}\\t'\n        f'l2_penalty={((predictr.coef_)**2).sum():.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}'\n    )\n\nalpha=5.00e+02  l2_penalty=0.046715 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5.00e+03  l2_penalty=0.021683 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5.00e+04  l2_penalty=0.003263 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5.00e+05  l2_penalty=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5.00e+06  l2_penalty=0.000002 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5.00e+07  l2_penalty=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5.00e+08  l2_penalty=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438\n\n\n- L2-penalty의 느낌은 대충 아래와 같이 분산으로 이해해도 무방\n\nfor predictr in predictrs: \n    print(\n        f'alpha={predictr.alpha:.2e}\\t'\n        f'var(coefs)={(predictr.coef_).var()*501:.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}'\n    )\n\nalpha=5.00e+02  var(coefs)=0.046618 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5.00e+03  var(coefs)=0.021638 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5.00e+04  var(coefs)=0.003256 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5.00e+05  var(coefs)=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5.00e+06  var(coefs)=0.000001 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5.00e+07  var(coefs)=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5.00e+08  var(coefs)=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438"
  },
  {
    "objectID": "posts/06wk-023.out.html#e.-alpha가-크다고-무조건-좋은건-아니다.",
    "href": "posts/06wk-023.out.html#e.-alpha가-크다고-무조건-좋은건-아니다.",
    "title": "[STBDA2023] 06wk-023: 취업+각종영어점수, Ridge",
    "section": "E. \\(\\alpha\\)가 크다고 무조건 좋은건 아니다.",
    "text": "E. \\(\\alpha\\)가 크다고 무조건 좋은건 아니다.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=1e12)\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n\nRidge(alpha=1000000000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=1000000000000.0)\n\n\n\nprint(f'train_score={predictr.score(X,y):.4f}')\nprint(f'test_score={predictr.score(XX,yy):.4f}')\n\ntrain_score=0.0191\ntest_score=0.0140\n\n\n\npredictr.coef_[1:].sum() # 이 값이 0.01이어야 하는데, 많이 작아짐\n\n0.0001258531920489157"
  },
  {
    "objectID": "posts/03wk-013.out.html",
    "href": "posts/03wk-013.out.html",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/03wk-013.out.html#a.-데이터-정리",
    "href": "posts/03wk-013.out.html#a.-데이터-정리",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df_train.drop(['PassengerId','Survived'],axis=1))\ny = df_train[['Survived']]"
  },
  {
    "objectID": "posts/03wk-013.out.html#b.-predictor-생성",
    "href": "posts/03wk-013.out.html#b.-predictor-생성",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()\npredictr \n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.out.html#c.-학습-fit-learn",
    "href": "posts/03wk-013.out.html#c.-학습-fit-learn",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\n\n\nX가 NaN값이 있어서 오류남"
  },
  {
    "objectID": "posts/03wk-013.out.html#a.-데이터정리",
    "href": "posts/03wk-013.out.html#a.-데이터정리",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "A. 데이터정리",
    "text": "A. 데이터정리\n\nX = pd.get_dummies(df_train[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]])\ny = df_train[[\"Survived\"]]\n\n\n\nX\n\n\n\n\n\n\n\n\nPclass\nSibSp\nParch\nSex_female\nSex_male\n\n\n\n\n0\n3\n1\n0\n0\n1\n\n\n1\n1\n1\n0\n1\n0\n\n\n2\n3\n0\n0\n1\n0\n\n\n3\n1\n1\n0\n1\n0\n\n\n4\n3\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\n2\n0\n0\n0\n1\n\n\n887\n1\n0\n0\n1\n0\n\n\n888\n3\n1\n2\n1\n0\n\n\n889\n1\n0\n0\n0\n1\n\n\n890\n3\n0\n0\n0\n1\n\n\n\n\n891 rows × 5 columns"
  },
  {
    "objectID": "posts/03wk-013.out.html#b.-predictor-생성-1",
    "href": "posts/03wk-013.out.html#b.-predictor-생성-1",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.out.html#c.-학습",
    "href": "posts/03wk-013.out.html#c.-학습",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X, y)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.out.html#d.-예측",
    "href": "posts/03wk-013.out.html#d.-예측",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "D. 예측",
    "text": "D. 예측\n\n#predictr.predict(X)\ndf_train.assign(Survived_hat=predictr.predict(X)).loc[:,['Survived','Survived_hat']]\n\n\n\n\n\n\n\n\nSurvived\nSurvived_hat\n\n\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n0\n0\n\n\n...\n...\n...\n\n\n886\n0\n0\n\n\n887\n1\n1\n\n\n888\n0\n1\n\n\n889\n1\n0\n\n\n890\n0\n0\n\n\n\n\n891 rows × 2 columns"
  },
  {
    "objectID": "posts/03wk-013.out.html#e.-평가",
    "href": "posts/03wk-013.out.html#e.-평가",
    "title": "[STBDA2023] 03wk-013: 타이타닉, 로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8002244668911336"
  },
  {
    "objectID": "posts/07wk-028.out.html",
    "href": "posts/07wk-028.out.html",
    "title": "[STBDA2023] 07wk-028: 선형모형의 적",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/07wk-028.out.html#a.-결측치의-존재",
    "href": "posts/07wk-028.out.html#a.-결측치의-존재",
    "title": "[STBDA2023] 07wk-028: 선형모형의 적",
    "section": "A. 결측치의 존재",
    "text": "A. 결측치의 존재\n- 문제: 데이터에서 누락된 값이 있는 경우, 선형모델이 돌아가지 않음. (“NaN이 있어서 모형을 적합할 수 없습니다”라는 에러 발생)\n- 해결방법\n\n방법1: 결측치를 제거\n\n결측치가 포함된 열을 제거\n결측치가 포함된 행을 제거\n위의 두 방법을 혼합\n\n방법2: 결측치를 impute\n\ntrain 에서는 fit_transform, test 에서는 transform\ntrain, test에서 모두 fit_transform\n임의의 값 (예를들면 -999)로 일괄 impute\ninterploation (이미지나 시계열 자료)"
  },
  {
    "objectID": "posts/07wk-028.out.html#b.-다중공선성의-존재",
    "href": "posts/07wk-028.out.html#b.-다중공선성의-존재",
    "title": "[STBDA2023] 07wk-028: 선형모형의 적",
    "section": "B. 다중공선성의 존재",
    "text": "B. 다중공선성의 존재\n- 문제: 데이터의 설명변수가 역할이 겹칠경우 선형모형의 일반화 성능이 좋지 않음.\n- 해결방법\n\n방법1: 변수제거\n\nX의 corr을 파악하고 (혹은 히트맵을 그리고) 느낌적으로 제거\nPCA등 차원축소기법을 이용한 제거\n\n방법2: 공선성을 가지는 변수를 모아 새로운 변수로 변환\n\n느낌적으로 변환 (예시 Fsize = Sibsp + Parch + 1, 이후 Sibsp, Parch 는 drop)\nPCA를 이용한 변환\n\n방법3: Ridge, Lasso 등 패널티계열을 사용\n\nRigde\nLasso\nElastic net\n\n\n- 방법1-1 (X의 corr을 파악하고 느낌적으로 제거) 의 예시\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nX = df.loc[:,'gpa':'toeic2']\nX\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\n0\n0.051535\n135\n129.566309\n133.078481\n121.678398\n\n\n1\n0.355496\n935\n940.563187\n935.723570\n939.190519\n\n\n2\n2.228435\n485\n493.671390\n493.909118\n475.500970\n\n\n3\n1.179701\n65\n62.272565\n55.957257\n68.521468\n\n\n4\n3.962356\n445\n449.280637\n438.895582\n433.598274\n\n\n...\n...\n...\n...\n...\n...\n\n\n495\n4.288465\n280\n276.680902\n274.502675\n277.868536\n\n\n496\n2.601212\n310\n296.940263\n301.545000\n306.725610\n\n\n497\n0.042323\n225\n206.793217\n228.335345\n222.115146\n\n\n498\n1.041416\n320\n327.461442\n323.019899\n329.589337\n\n\n499\n3.626883\n375\n370.966595\n364.668477\n371.853566\n\n\n\n\n500 rows × 5 columns\n\n\n\ncorr 조사\n\nX.corr()\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\ngpa\n1.000000\n-0.033983\n-0.035722\n-0.037734\n-0.034828\n\n\ntoeic\n-0.033983\n1.000000\n0.999435\n0.999322\n0.999341\n\n\ntoeic0\n-0.035722\n0.999435\n1.000000\n0.998746\n0.998828\n\n\ntoeic1\n-0.037734\n0.999322\n0.998746\n1.000000\n0.998721\n\n\ntoeic2\n-0.034828\n0.999341\n0.998828\n0.998721\n1.000000\n\n\n\n\n\n\n\nheatmap 플랏\n\nsns.heatmap(X.corr(),annot=True, fmt=\".7f\")  # 숫자 보여주는 옵션\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/07wk-028.out.html#c.-관련이-없는-변수의-존재",
    "href": "posts/07wk-028.out.html#c.-관련이-없는-변수의-존재",
    "title": "[STBDA2023] 07wk-028: 선형모형의 적",
    "section": "C. 관련이 없는 변수의 존재",
    "text": "C. 관련이 없는 변수의 존재\n- 문제: 데이터에서 불필요한 설명변수가 너무 많을 경우 선형모형의 일반화 성능이 좋지 않음.\n\n불필요한 설명변수임의 쉬운 예시: 고객이름, ID, Index 관련 변수\n\n- 해결방법\n\n방법1: 변수제거\n\n(y,X)의 corr을 파악하고 (혹은 히트맵을 그리고) 느낌적으로 제거\nPCA를 이용한 제거\nLasso를 이용한 제거\n\n방법2: 더 많은 데이터를 확보 (궁극기술, 그런데 차원의 저주때문에 힘듬)\n\n- 방법1-1의 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\ndf_train.corr()\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\ntoeic\n1.000000\n-0.033983\n0.260183\n0.002682\n0.110530\n0.024664\n\n\ngpa\n-0.033983\n1.000000\n0.711022\n-0.025197\n0.005272\n0.020794\n\n\nemployment\n0.260183\n0.711022\n1.000000\n-0.007348\n0.036706\n0.032284\n\n\nbalance0\n0.002682\n-0.025197\n-0.007348\n1.000000\n-0.059167\n0.040035\n\n\nbalance1\n0.110530\n0.005272\n0.036706\n-0.059167\n1.000000\n-0.030215\n\n\nbalance2\n0.024664\n0.020794\n0.032284\n0.040035\n-0.030215\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df_train.corr(),annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n설명변수 balance0,1,2(=X3,X4,X5)는 반응변수 employment(=y)와 관련이 없어 -&gt; X3,X4,X5는 제외하자.\n반응변수 employment(=y)와 관련이 있는 설명변수인 toiec,gpa (=X1,X2)는 남기자.\n공선성체크: 설명변수 toeic, gpa (=X1,X2)의 corr은 -0.034 로 높지 않으니 다중공선성문제를 걱정할 필요가 없음.\n\n- 방법1-3의 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\n# step1\nX,y = df_train[['toeic','gpa','balance0','balance1','balance2']], df_train['employment']\n# step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(\n    Cs = [0.1, 1, 10, 100],\n    penalty='l1',\n    solver='liblinear',\n    random_state=42\n)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass\n\nLogisticRegressionCV(Cs=[0.1, 1, 10, 100], penalty='l1', random_state=42,\n                     solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionCVLogisticRegressionCV(Cs=[0.1, 1, 10, 100], penalty='l1', random_state=42,\n                     solver='liblinear')\n\n\n\npredictr.coef_\n\narray([[0.00260249, 1.41401358, 0.        , 0.        , 0.        ]])\n\n\n\ns = pd.Series(predictr.coef_.reshape(-1))\ns.index= X.columns\ns\n\ntoeic       0.002602\ngpa         1.414014\nbalance0    0.000000\nbalance1    0.000000\nbalance2    0.000000\ndtype: float64"
  },
  {
    "objectID": "posts/07wk-028.out.html#d.-이상치의-존재",
    "href": "posts/07wk-028.out.html#d.-이상치의-존재",
    "title": "[STBDA2023] 07wk-028: 선형모형의 적",
    "section": "D. 이상치의 존재",
    "text": "D. 이상치의 존재\n- 문제: 이상치가 존재할 경우 전체 모형이 무너질 수 있음\n- 이상치가 있을 경우 해결할 수 있는 방법\n\n방법1: 이상치를 제거하고 분석한다.\n\n느낌적으로 제거함.\n이상치를 감지하는 지표을 사용하여 제거한 이후 분석\n이상치를 자동으로 감지하는 모형을 사용하여 이상치를 제거한 이후 분석\n\n방법2: 로버스트 선형회귀 계열을 이용\n\nsklearn.linear_model.HuberRegressor 등\n\n\n방법3: 이상치를 완화시키는 변환을 사용\n\nsklearn.preprocessing.PowerTransformer 이용\n\n\n- 방법3-1의 예시\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 50\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})[:10]\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\ntransformr = sklearn.preprocessing.PowerTransformer()\nx,y = transformr.fit_transform(df_train).T\n\n\n변환해보고 나중에 역변환 하면 됨\n\n\nsns.scatterplot(df_train,x='temp',y='ice_sales',label='before')\nsns.scatterplot(x=x,y=y,label='after')\n\n\n\n\nimage.png\n\n\n—참고—\nPowerTransformer()는 자료가 정규분포가 아닌 경우 강제로 정규화하는 변환이다.\n\nx = np.random.exponential(scale=10, size=1000)\ntransformr = sklearn.preprocessing.PowerTransformer(method='box-cox')  # 'box-cox' 또는 'yeo-johnson' 중 선택 가능, 디폴트는 이오존슨\ny = transformr.fit_transform(x.reshape(-1, 1))\n\n\nfig, ax = plt.subplots(1,2)\nax[0].hist(x,bins=25)\nax[1].hist(y,bins=25)\nfig.set_figwidth(12)"
  },
  {
    "objectID": "posts/07wk-028.out.html#e.-교호작용의-존재",
    "href": "posts/07wk-028.out.html#e.-교호작용의-존재",
    "title": "[STBDA2023] 07wk-028: 선형모형의 적",
    "section": "E. 교호작용의 존재",
    "text": "E. 교호작용의 존재\n- 문제: 설명 변수 간의 상호 작용이 있는 경우 이를 고려하지 않으면 모델이 데이터를 잘 설명하지 못할 수 있음.\n- 해결: 고려하면 됩니당.."
  },
  {
    "objectID": "posts/10wk-039.out.html",
    "href": "posts/10wk-039.out.html",
    "title": "[STBDA2023] 10wk-039: 의사결정나무 Discussion",
    "section": "",
    "text": "10wk-039: 의사결정나무 Discussion\n최규빈\n2023-11-10\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-y7ZQE5CtHiEraKV2eZslti&si=QYNee59zfsGgXL_X\n\n\n2. 의사결정나무 Discussions\n- 의사결정나무 vs 선형모형\n\n아이스크림+축제: 이상치에 강했음.\n운동+보조제: 교호작용을 고려하지 않아도 괜찮았음.\n토익유사점수: 다중공선성문제가 발생하는 경우에도 모형이 덜 망함.\n밸런스게임: 필요없는 변수가 있을 경우에도 모형이 덜 망함.\n\n- 의사결정나무의 장점들\n\n시각화가 유리하다. 설명력이 좋다.\n특성(feature)의 중요도를 파악하기 용이하다.\n\\({\\bf y} \\sim {\\bf X}\\) 사이에 존재하는 비선형성을 쉽게 모델링 할 수 있다. \\(\\to\\) 쉽게 말해서 잘 맞춘다는 소리에요\n모형에 대한 가정들이 필요 없다. (넌파라메트릭 모형 특징)\n\n- 의사결정나무의 단점: 오버피팅이 일어나기 너무 쉽다. (모형이 너무 흔들려..)\n- 의사결정나무에 대한 자잘한 개념들 (자격증에서 잘 물어봄)\n최소 샘플 분할(Min Samples Split):\n\n노드를 분할하기 위한 최소 샘플 수.\n적절한 설정으로 과소적합 및 과적합 조절 가능.\n\n가지치기(Pruning):\n\n트리의 불필요한 부분을 제거.\n과적합 방지 및 모델 성능 향상에 도움.\n\n정보 이득(Information Gain):\n\n분할 전후의 엔트로피 차이를 측정.\n높은 정보 이득은 더 좋은 분할을 의미.\n\n지니 불순도(Gini Impurity):\n\n노드의 순도 측정 지표.\n낮은 지니 불순도는 높은 클래스 순도를 의미.\n\n\n결국 “트리를 어디까지 성장시킬래?”라는 물음에 대답하기 위해 고안된 개념들이다. 근본적으로 “트리를 어디까지 성장시킬래?”에 대한 이론적인 명확한 기술은 없다. 이는 넌파라메트릭 모형이 가지는 공통적인 특징임.\n\n- 의사결정나무는 오버피팅을 잡기위해서 지루한 싸움을 시작함.\n\n발전과정: 의사결정나무 \\(\\to\\) 배깅, 랜덤포레스트, 부스팅\n의사결정나무를 응용한 다양한 방법들이 개발되었다. (너무 많아요 진짜) \\(\\to\\) 모든 방법들의 원리를 세세하게 파헤치는건 비효율적이다.\n그러한 다양한 방법들을 적덩히 분류해보면 대체로 배깅, 랜덤포레스트, 부스팅 계열로 나뉜다.[1] \\(\\to\\) 배깅, 랜덤포레스트, 부스팅에 대한 공통적 아이디어를 파악하는건 효율적이다.\n현재 최고로 (state of the art, SOTA) 로 평가받는 알고리즘은 부스팅계열의 XGBoost, LightGBM, CatBoost 이다.\n\n[1] 모든 방법들이 세개의 카테고리중 하나에만 들어가는건 아니다"
  },
  {
    "objectID": "posts/10wk-038.out.html",
    "href": "posts/10wk-038.out.html",
    "title": "[STBDA2023] 10wk-038: 아이스크림 – 의사결정나무 원리",
    "section": "",
    "text": "최규빈\n2023-11-10"
  },
  {
    "objectID": "posts/10wk-038.out.html#a.-분할이-정해졌을때-haty을-결정하는-방법",
    "href": "posts/10wk-038.out.html#a.-분할이-정해졌을때-haty을-결정하는-방법",
    "title": "[STBDA2023] 10wk-038: 아이스크림 – 의사결정나무 원리",
    "section": "A. 분할이 정해졌을때 \\(\\hat{y}\\)을 결정하는 방법?",
    "text": "A. 분할이 정해졌을때 \\(\\hat{y}\\)을 결정하는 방법?\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n- tree 시각화 \\(\\to\\) 분할파악\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.25, 'squared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.75, 0.25, 'squared_error = 49.428\\nsamples = 55\\nvalue = 41.489')]\n\n\n\n\n\n- 분할에 따른 \\(\\hat{y}\\) 계산\n\ndf_train[df_train.temp&lt;= 5.05].sales.mean(),df_train[df_train.temp&gt; 5.05].sales.mean()\n\n(24.787609205775055, 41.489079055828356)"
  },
  {
    "objectID": "posts/10wk-038.out.html#b.-분할을-결정하는-방법",
    "href": "posts/10wk-038.out.html#b.-분할을-결정하는-방법",
    "title": "[STBDA2023] 10wk-038: 아이스크림 – 의사결정나무 원리",
    "section": "B. 분할을 결정하는 방법?",
    "text": "B. 분할을 결정하는 방법?\n- 예비학습\n\npredictr.score(X,y)\n\n0.6167038863844929\n\n\n\n이 값이 내부적으로 어떻게 계산된거지?\n\n\npredictr.score??\n\n\nSignature: predictr.score(X, y, sample_weight=None)\nSource:   \n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the coefficient of determination of the prediction.\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\\\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).\n        \"\"\"\n        from .metrics import r2_score\n        y_pred = self.predict(X)\n        return r2_score(y, y_pred, sample_weight=sample_weight)\nFile:      ~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/base.py\nType:      method\n\n\n\n\ny_hat = y_pred = predictr.predict(X)\nsklearn.metrics.r2_score(y,y_pred)\n\n0.6167038863844929\n\n\n- 좋은 분할을 판단하는 기준? – 여기에서 r2_score가 이용됨\n- 우선 논의를 편하게하기 위해서 \\(({\\bf X},{\\bf y})\\)와 경계값 \\(c\\)를 줄때 \\(\\hat{\\bf y}\\)을 계산해주는 함수를 구현하자.\n\ndef fit_predict(X,y,c):\n    X = np.array(X).reshape(-1)\n    y = np.array(y) \n    yhat = y*0   \n    yhat[X&lt;=c] = y[X&lt;=c].mean()\n    yhat[X&gt;c] = y[X&gt;c].mean()\n    return yhat\n\n- 서로 다른 분할에 대하여 시각화를 진행\n\nyhat_bad = fit_predict(X,y,c=-1)\nyhat_good = fit_predict(X,y,c=5)\nfig, ax = plt.subplots(1,2) \nax[0].plot(X,y,'o',alpha=0.5)\nax[0].plot(X,yhat_bad,'--.')\nax[0].set_title('bad')\nax[1].plot(X,y,'o',alpha=0.5)\nax[1].plot(X,yhat_good,'--.')\nax[1].set_title('good')\n\nText(0.5, 1.0, 'good')\n\n\n\n\n\n\n딱봐도 오른쪽이 좋은 분할같은데, 컴퓨터한테 이걸 어떻게 설명하지?\n\n- 좋은분할을 구하는 이유는 좋은 yhat을 얻기 위함이다. 그렇다면 좋은 yhat을 얻게 해주는 분할이 좋은 분할이라 해석할 수 있다. \\(\\to\\) 아이디어: 그런데 좋은 yhat은 sklearn.metrics.r2_score(y,yhat)의 값이 높지 않을까?\n- 그렇다면 위의 그림에서 왼쪽보다 오른쪽이더 좋은 분할이라면 r2_score(y,yhat_good)의 값이 r2_score(y,yhat_bad) 값보다 높을 것!\n\nsklearn.metrics.r2_score(y,yhat_bad), sklearn.metrics.r2_score(y,yhat_good)\n\n(0.13932141536746745, 0.6167038863844928)\n\n\n- 트리의 max_depth=1 일 경우 분할을 결정하는 방법 – 노가다..\n\n적당한 \\(c\\)를 고른다.\n분할 \\((-\\infty,c), [c,\\infty)\\) 를 생성하고 yhat를 계산한다.\nr2_score(y,yhat)를 계산하고 기록한다.\n1-3의 과정을 무한반복 한다. 그리고 r2_score(y,yhat)의 값을 가장 작게 만드는 \\(c\\)가 무엇인지 찾는다.\n\n\ncuts = np.arange(-5,15)\nfig = plt.figure()\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5)\n    c = cuts[frame] \n    yhat = fit_predict(X,y,c)\n    ax.plot(X,yhat,'.')\n    r2 = sklearn.metrics.r2_score(y,yhat)\n    ax.set_title(f'c={c}, r2_score={r2:.2f}')\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=20\n)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- tree가 찾은 값 5.05를 우리가 직접 찾아보자.\n\ncuts = np.arange(-5,15,0.05).round(2)\nscore = np.array([sklearn.metrics.r2_score(y,fit_predict(X,y,c)) for c in cuts])\nplt.plot(cuts,score)\n\n\n\n\n- 방법1: 시각화로 찾는방법\n\npd.DataFrame({'cut':cuts,'score':score})\\\n.plot.line(x='cut',y='score',backend='plotly')\n\n                                                \n\n\n- 방법2: 정석\n\ncuts[score.argmax()]\n\n5.0\n\n\n- max_depth=2일 경우? max_depth=1의 결과로 발생한 2개의 조각을 각각 전체자료로 생각하고, max_depth=1일 때의 분할방법을 반복적용한다.\n- X=[temp,type] 와 같은 경우라면? 설명변수를 하나씩 고정하여 각각 최적분할을 생성하고 r2_score관점에서 가장 우수한 설명변수를 선택"
  },
  {
    "objectID": "posts/07wk-030.out.html",
    "href": "posts/07wk-030.out.html",
    "title": "[STBDA2023] 07wk-030: 아이스크림(교호작용) / 선형회귀",
    "section": "",
    "text": "07wk-030: 아이스크림(교호작용) / 선형회귀\n최규빈\n2023-10-17\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-zwhGQx-SVKqORNSrWm0TYi&si=igNDnZPiKmBIpvlx\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\nimport sklearn.model_selection\n\n\n\n3. 예비학습\n\ndf = pd.DataFrame({'X1':[2,3,4,1],'X2':['A','B','A','C']})\ndf \n\n\n\n\n\n\n\n\nX1\nX2\n\n\n\n\n0\n2\nA\n\n\n1\n3\nB\n\n\n2\n4\nA\n\n\n3\n1\nC\n\n\n\n\n\n\n\n\npd.get_dummies(df)\n\n\n\n\n\n\n\n\nX1\nX2_A\nX2_B\nX2_C\n\n\n\n\n0\n2\n1\n0\n0\n\n\n1\n3\n0\n1\n0\n\n\n2\n4\n1\n0\n0\n\n\n3\n1\n0\n0\n1\n\n\n\n\n\n\n\n\nX2_A, X2_B, X2_C는 셋다 있을 필요는 없지 않나? –&gt; 공선성문제가 생길수도 있음.\n\n\npd.get_dummies(df,drop_first=True)\n\n\n\n\n\n\n\n\nX1\nX2_B\nX2_C\n\n\n\n\n0\n2\n0\n0\n\n\n1\n3\n1\n0\n\n\n2\n4\n0\n0\n\n\n3\n1\n0\n1\n\n\n\n\n\n\n\n\n\n4. Data\n- load\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n- 시각화 및 해석\n\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales,'o',label='choco')\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales,'o',label='vanilla')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f5549a27a90&gt;\n\n\n\n\n\n\n온도에 따른 아이스크림 판매량이 아이스크림의 tpye에 따라 동일하다면 기울기가 동일하고 절편이 다른 두 직선이 나올것임.\n하지만 현재는 초코보다 바닐라맛이 기온의 영향을 많이 받아보임 \\(\\to\\) (바닐라아이스크림,온도)는 (초코아이스크림,온도)보다 궁합이 좋다. \\(\\to\\) 아이스크림 type과 온도사이에는 교호작용이 존재한다.\n\n\n\n5. 분석1\n- 분석1: 모형을 아래와 같이 본다.\n\n\\({\\bf X}\\): temp, type\n\\({\\bf y}\\): sales\n\n\n# step1 \nX,y = pd.get_dummies(df_train[['temp','type']],drop_first=True), df_train['sales']\n# step2\npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n\n\npredictr.score(X,y)\n\n0.9249530603100549\n\n\n\n점수가 잘나왔다고 너무 좋아하지 마세요.\n시각화를 반드시 해보고 더 맞출수 있는 여지가 있는지 항상 확인할 것\n\n\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales_hat,'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f5549a27a60&gt;\n\n\n\n\n\n\n이 모형은 초코/바닐라에 대한 기울기차이를 “표현”할 수 없다. 이러한 상황은 “모형의 표현력이 약하다” 혹은 “언더피팅”인 상황이라고 한다.\n\n\n\n6. 분석2\n- 모형을 아래와 같이 본다.\n\n\\({\\bf X}\\): temp, type, temp \\(\\times\\) type\n\\({\\bf y}\\): sales\n\n\npd.get_dummies(df_train[['temp','type']],drop_first=True).eval('interaction = temp*type_vanilla')\n\n\n\n\n\n\n\n\ntemp\ntype_vanilla\ninteraction\n\n\n\n\n0\n-4.1\n0\n-0.0\n\n\n1\n-3.7\n0\n-0.0\n\n\n2\n-3.0\n0\n-0.0\n\n\n3\n-1.3\n0\n-0.0\n\n\n4\n-0.5\n0\n-0.0\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n1\n12.4\n\n\n96\n13.4\n1\n13.4\n\n\n97\n14.7\n1\n14.7\n\n\n98\n15.0\n1\n15.0\n\n\n99\n15.2\n1\n15.2\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n# step1 \nX = pd.get_dummies(df_train[['temp','type']],drop_first=True).eval('interaction = temp*type_vanilla')\ny = df_train['sales']\n# step2\npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n\n\npredictr.score(X,y)\n\n0.9865793819066231\n\n\n\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales_hat,'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f554392b490&gt;\n\n\n\n\n\n\nNote: 초코/바닐라에 대한 절편차이는 type로, 초코/바닐라에 대한 기울기 차이는 temp\\(\\times\\)type로 표현한다."
  },
  {
    "objectID": "posts/07wk-035.out.html",
    "href": "posts/07wk-035.out.html",
    "title": "[STBDA2023] 07wk-035: 아이스크림(이상치) / 의사결정나무",
    "section": "",
    "text": "07wk-035: 아이스크림(이상치) / 의사결정나무\n최규빈\n2023-10-16\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-weIAPJ89acBOc2V3_0HMcX&si=Lqw2fusURrQqchzH\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\n\n\n\n\n\n\n4. 분석\n- 분석: 의사결정나무\n\n# step1\nX = df_train[['temp']]\ny = df_train['ice_sales']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_train['ice_sales_hat'] = predictr.predict(X)\n\n\n# plt.plot(df_train.temp,df_train.ice_sales,'o')\n# plt.plot(df_train.temp,df_train.ice_sales_hat,'--')\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\n\n\n\n\n- 12.5~18 구간사이의 unseen data를 가상으로 만들고 예측값을 살펴보자.\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(12.5,18,100)})\n\n\ndf_test['ice_sales_hat'] = predictr.predict(XX)\n\n\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o',color='C0',alpha=0.5)\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--',color='C1',alpha=0.5)\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--',color='C2',linewidth=2)\n\n\n\n\n- -15~0 구간사이의 unseen data를 가상으로 만들고 예측값을 살펴보자.\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(-15,0,100)})\n\n\ndf_test['ice_sales_hat'] = predictr.predict(XX)\n\n\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o',color='C0',alpha=0.5)\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--',color='C1',alpha=0.5)\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--',color='C2',linewidth=2)\n\n\n\n\n\n뭐 이 데이터에서는 최선이지 않을까?\n\n\n\n6. HW\n- 없어요. 다른과목 중간고사 준비 잘하세요!"
  },
  {
    "objectID": "posts/14wk-58.out.html",
    "href": "posts/14wk-58.out.html",
    "title": "[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon) —> 오류!!",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/14wk-58.out.html#a.-step1-데이터의-정리",
    "href": "posts/14wk-58.out.html#a.-step1-데이터의-정리",
    "title": "[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon) —> 오류!!",
    "section": "A. Step1: 데이터의 정리",
    "text": "A. Step1: 데이터의 정리\n\nfrom gluonts.dataset.field_names import FieldName\nfrom gluonts.dataset.common import ListDataset\nimport pandas as pd\n\n\nts_train = TimeSeriesDataFrame(\n    data = df_train,\n    static_features = None, \n    id_column = 'item_id',\n    timestamp_column = 'date'\n)\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 ts_train = TimeSeriesDataFrame(                                                              │\n│   2 │   data = df_train,                                                                         │\n│   3 │   static_features = None,                                                                  │\n│   4 │   id_column = 'item_id',                                                                   │\n│                                                                                                  │\n│ /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/autogluon/timeseries/dataset/ts_dataf │\n│ rame.py:109 in __init__                                                                          │\n│                                                                                                  │\n│   106 │   │   │   if isinstance(data.index, pd.MultiIndex):                                      │\n│   107 │   │   │   │   self._validate_multi_index_data_frame(data)                                │\n│   108 │   │   │   else:                                                                          │\n│ ❱ 109 │   │   │   │   data = self._construct_pandas_frame_from_data_frame(data)                  │\n│   110 │   │   elif isinstance(data, str):                                                        │\n│   111 │   │   │   data = self._load_data_frame_from_file(data)                                   │\n│   112 │   │   elif isinstance(data, Iterable):                                                   │\n│                                                                                                  │\n│ /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/autogluon/timeseries/dataset/ts_dataf │\n│ rame.py:344 in _construct_pandas_frame_from_data_frame                                           │\n│                                                                                                  │\n│   341 │   │   if TIMESTAMP in df.columns:                                                        │\n│   342 │   │   │   df[TIMESTAMP] = pd.to_datetime(df[TIMESTAMP])                                  │\n│   343 │   │                                                                                      │\n│ ❱ 344 │   │   cls._validate_data_frame(df)                                                       │\n│   345 │   │   return df.set_index([ITEMID, TIMESTAMP])                                           │\n│   346 │                                                                                          │\n│   347 │   @classmethod                                                                           │\n│                                                                                                  │\n│ /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/autogluon/timeseries/dataset/ts_dataf │\n│ rame.py:224 in _validate_data_frame                                                              │\n│                                                                                                  │\n│   221 │   │   if ITEMID not in df.columns:                                                       │\n│   222 │   │   │   raise ValueError(f\"data must have a `{ITEMID}` column\")                        │\n│   223 │   │   if TIMESTAMP not in df.columns:                                                    │\n│ ❱ 224 │   │   │   raise ValueError(f\"data must have a `{TIMESTAMP}` column\")                     │\n│   225 │   │   if df[ITEMID].isnull().any():                                                      │\n│   226 │   │   │   raise ValueError(f\"`{ITEMID}` column can not have nan\")                        │\n│   227 │   │   if df[TIMESTAMP].isnull().any():                                                   │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: data must have a `timestamp` column\n\n\n\n\n??? 자꾸 안되네.."
  },
  {
    "objectID": "posts/14wk-58.out.html#b.-step2-timeseriespredictor-생성",
    "href": "posts/14wk-58.out.html#b.-step2-timeseriespredictor-생성",
    "title": "[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon) —> 오류!!",
    "section": "B. Step2: TimeSeriesPredictor 생성",
    "text": "B. Step2: TimeSeriesPredictor 생성\n\npredictr = TimeSeriesPredictor(\n    target = 'temp', # 예측하고 싶은것 \n    known_covariates_names = None, # 온도를 예측할때 쓸 수 있는 다른 시계열 자료 \n    prediction_length = len(df_test), # 예측하고 싶은 자료의 길이\n    freq = 'D' # 주기 (보통은 'D', 'H'를 씀, 일반적으로 명시하지 않아도 오토글루온이 알아서 찾아줌) \n)\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 predictr = TimeSeriesPredictor(                                                              │\n│   2 │   target = 'temp', # 예측하고 싶은것                                                       │\n│   3 │   known_covariates_names = None, # 온도를 예측할때 쓸 수 있는 다른 시계열 자료             │\n│   4 │   prediction_length = len(df_test), # 예측하고 싶은 자료의 길이                            │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nTypeError: __init__() got an unexpected keyword argument 'freq'"
  },
  {
    "objectID": "posts/14wk-58.out.html#c.-step3-학습",
    "href": "posts/14wk-58.out.html#c.-step3-학습",
    "title": "[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon) —> 오류!!",
    "section": "C. Step3: 학습",
    "text": "C. Step3: 학습\n\npredictr.fit(ts_train,presets='medium_quality')\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 predictr.fit(ts_train,presets='medium_quality')                                              │\n│   2                                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'predictr' is not defined"
  },
  {
    "objectID": "posts/14wk-58.out.html#d.-step4-예측",
    "href": "posts/14wk-58.out.html#d.-step4-예측",
    "title": "[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon) —> 오류!!",
    "section": "D. Step4: 예측",
    "text": "D. Step4: 예측\n\npredictr.predict(ts_train)\n\nModel not specified in predict, will default to the model with the best validation score: WeightedEnsemble"
  },
  {
    "objectID": "posts/14wk-58.out.html#e.-시각화",
    "href": "posts/14wk-58.out.html#e.-시각화",
    "title": "[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon) —> 오류!!",
    "section": "E. 시각화",
    "text": "E. 시각화\n\npredictions = predictr.predict(ts_train)\npredictions\n\nModel not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n\n\n\nplt.plot(df_train.date,df_train.temp,label='observed')\nplt.plot(df_test.date,df_test.temp,color='gray',alpha=0.5,label='future value')\nplt.plot(df_test.date,predictions['mean'],color='C1',label='estimated')\nplt.fill_between(df_test.date, predictions['0.1'], predictions['0.9'],color='C1',alpha=0.2)\nplt.legend()\n\n\n\n\n\n이정도 결과는 합리적으로 보임"
  },
  {
    "objectID": "posts/14wk-58.out.html#a.-static_feature-vs-known_covariates",
    "href": "posts/14wk-58.out.html#a.-static_feature-vs-known_covariates",
    "title": "[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon) —> 오류!!",
    "section": "A. static_feature vs known_covariates",
    "text": "A. static_feature vs known_covariates\n- static은 시점에 따라 변화하지 않는 정보, known_covariates는 시점에 따라 변화하지만 이미 알고있는 미래시계열\n\n분석타입1: fit(target의 현재까지자료) / forecast(target의 현재까지자료) = target의 미래\n분석타입2: fit(target의 현재까지자료,static_feature) -&gt; forecast(target의 현재까지자료,static_feature) = target의 미래\n분석타입3: fit(target의 현재까지자료,known_covariates의 현재까지자료) -&gt; forecast(target의 현재까지자료,known_covariates의 미래) = target의 미래"
  },
  {
    "objectID": "posts/14wk-58.out.html#b.-장기예측",
    "href": "posts/14wk-58.out.html#b.-장기예측",
    "title": "[STBDA2023] 14wk-58: 전주시기온 / 시계열자료분석(Autogluon) —> 오류!!",
    "section": "B. 장기예측",
    "text": "B. 장기예측\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,2:4].set_axis(['date','temp'],axis=1).assign(date= lambda df: df.date.apply(pd.to_datetime))\ndf_train = df[:513].assign(item_id = '평균기온')\ndf_test = df[513:].assign(item_id = '평균기온')\n#---#\n## step1 \nts_train = TimeSeriesDataFrame(\n    data = df_train,\n    static_features = None, \n    id_column = 'item_id',\n    timestamp_column = 'date'\n)\n## step2 \npredictr = TimeSeriesPredictor(\n    target = 'temp', # 예측하고 싶은것 \n    known_covariates_names = None, # 온도를 예측할때 쓸 수 있는 다른 시계열 자료 \n    prediction_length = len(df_test), # 예측하고 싶은 자료의 길이\n    freq = 'D' # 주기 (보통은 'D', 'H'를 씀, 일반적으로 명시하지 않아도 오토글루온이 알아서 찾아줌) \n)\n## step3 \npredictr.fit(ts_train)\n## step4 \npredictr.predict(ts_train)\n#---# \npredictions = predictr.predict(ts_train)\nplt.plot(df_train.date,df_train.temp,label='observed')\nplt.plot(df_test.date,df_test.temp,color='gray',alpha=0.5,label='future value')\nplt.plot(df_test.date,predictions['mean'],color='C1',label='estimated')\nplt.fill_between(df_test.date, predictions['0.1'], predictions['0.9'],color='C1',alpha=0.2)\nplt.legend()\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231209_020014\"\nBeginning AutoGluon training...\nAutoGluon will save models to 'AutogluonModels/ag-20231209_020014'\n=================== System Info ===================\nAutoGluon Version:  1.0.0\nPython Version:     3.11.6\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #140-Ubuntu SMP Thu Aug 4 02:23:37 UTC 2022\nCPU Count:          128\nGPU Count:          1\nMemory Avail:       403.68 GB / 503.74 GB (80.1%)\nDisk Space Avail:   1159.48 GB / 1757.88 GB (66.0%)\n===================================================\n\nFitting with arguments:\n{'enable_ensemble': True,\n 'eval_metric': WQL,\n 'freq': 'D',\n 'hyperparameters': 'default',\n 'known_covariates_names': [],\n 'num_val_windows': 1,\n 'prediction_length': 143,\n 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n 'random_seed': 123,\n 'refit_every_n_windows': 1,\n 'refit_full': False,\n 'target': 'temp',\n 'verbosity': 2}\n\nProvided train_data has 513 rows, 1 time series. Median time series length is 513 (min=513, max=513). \n\nProvided dataset contains following columns:\n    target:           'temp'\n\nAutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n===================================================\n\nStarting training. Start time is 2023-12-09 02:00:14\nModels that will be trained: ['SeasonalNaive', 'CrostonSBA', 'NPTS', 'AutoETS', 'DynamicOptimizedTheta', 'AutoARIMA', 'RecursiveTabular', 'DirectTabular', 'DeepAR', 'TemporalFusionTransformer', 'PatchTST']\nTraining timeseries model SeasonalNaive. \n    -0.8225       = Validation score (-WQL)\n    0.00    s     = Training runtime\n    5.26    s     = Validation (prediction) runtime\nTraining timeseries model CrostonSBA. \n    -0.5749       = Validation score (-WQL)\n    0.00    s     = Training runtime\n    5.96    s     = Validation (prediction) runtime\nTraining timeseries model NPTS. \n    -0.5393       = Validation score (-WQL)\n    0.00    s     = Training runtime\n    1.33    s     = Validation (prediction) runtime\nTraining timeseries model AutoETS. \n    -0.7227       = Validation score (-WQL)\n    0.00    s     = Training runtime\n    14.14   s     = Validation (prediction) runtime\nTraining timeseries model DynamicOptimizedTheta. \n    -0.7426       = Validation score (-WQL)\n    0.00    s     = Training runtime\n    16.03   s     = Validation (prediction) runtime\nTraining timeseries model AutoARIMA. \n    -0.7599       = Validation score (-WQL)\n    0.00    s     = Training runtime\n    9.52    s     = Validation (prediction) runtime\nTraining timeseries model RecursiveTabular. \n    -0.8312       = Validation score (-WQL)\n    1.45    s     = Training runtime\n    0.76    s     = Validation (prediction) runtime\nTraining timeseries model DirectTabular. \n    -0.4099       = Validation score (-WQL)\n    1.56    s     = Training runtime\n    0.06    s     = Validation (prediction) runtime\nTraining timeseries model DeepAR. \n    -0.4404       = Validation score (-WQL)\n    21.42   s     = Training runtime\n    0.44    s     = Validation (prediction) runtime\nTraining timeseries model TemporalFusionTransformer. \n    -1.9696       = Validation score (-WQL)\n    36.18   s     = Training runtime\n    0.03    s     = Validation (prediction) runtime\nTraining timeseries model PatchTST. \n    -0.4265       = Validation score (-WQL)\n    11.90   s     = Training runtime\n    0.02    s     = Validation (prediction) runtime\nFitting simple weighted ensemble.\n    Ensemble weights: {'DeepAR': 0.26, 'DirectTabular': 0.18, 'PatchTST': 0.52, 'TemporalFusionTransformer': 0.04}\n    -0.3698       = Validation score (-WQL)\n    0.82    s     = Training runtime\n    0.55    s     = Validation (prediction) runtime\nTraining complete. Models trained: ['SeasonalNaive', 'CrostonSBA', 'NPTS', 'AutoETS', 'DynamicOptimizedTheta', 'AutoARIMA', 'RecursiveTabular', 'DirectTabular', 'DeepAR', 'TemporalFusionTransformer', 'PatchTST', 'WeightedEnsemble']\nTotal runtime: 126.97 s\nBest model: WeightedEnsemble\nBest model score: -0.3698\nModel not specified in predict, will default to the model with the best validation score: WeightedEnsemble\nModel not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n\n\n\n\n\n\n장기예측은 그럴듯하지 않아보임 – 장기예측은 (좁은의미의)시계열분석의 범위를 벗어남\n\n- 통계학과에서 배우는 시계열분석\n\n주식시계열: 독립인듯 하지만, 사실은 독립이 아닌자료. 즉 “관측자료 = 정보 + 오차” 인데, 오차항이 독립이 아닐 경우.\n오차항이 독립이 아니라는 의미: 주식시계열이 비합리적인 관측자료라는 의미임. (사실 오차항은 독립이어야 하지 않냐?)\n주식시계열자료의 새로운해석: 원래 우리가 통계에서 다루는 셋팅은 “관측자료 = 정보 + 오차”이다.[1]. 이를 시계열로 바꿔서 표현하면 “주식시계열자료 = 합리적정보 + 오차 = 합리적정보 + (기세 + 독립오차)”\n시계열분석: 통계학과에서 배우는 시계열모형들은 “기세”를 모델링하는 것임. 즉 비합리적인 오차항을 모델링 하는 것!\n\n1시점 전의 기세의 영향을 얼만큼 강하게 볼지 결정하는 일은, AR(1)의 계수값을 결정하는 일과 같은 일이다.\n몇 시점까지의 기세를 봐야할지 결정하는 일은 AR(p)모형에서 p의 값을 결정하는 일과 같은 일이다.\n100시점 전까지의 기세를 보고싶다면, 원칙적으로는 AR(100)을 적합해야하지만 그렇다면 너무 학습할 파라메터가 많을 것임. 하지만 이론에 따라서 AR(100)과 비슷한 효과를 주고 파라메터를 더 적게 가지는 MA(q), ARMA(p,q) 와 같은 모형이 존재함이 밝혀짐. 그래서 아주 먼 시점의 기세까지 파악하여 분석해야 한다면 ARMA(p,q)를 적합하는 것이 유리함.\n트렌드와 계절성을 가지는 시계열 자료는 기세를 모델링하기 어려운데 어쩌지? 제거하고 분석 (ARIMA, SARIMA 등의 개발)\n기세의 변동성이 시간에 따라 달라진다면 어쩌지? ARCH, GARCH 의 개발\n\n평균정보: 시계열분석에서 합리적정보(평균정보)는 static_feature, known_covariates 에 포함되어있다.\n장기예측: 장기예측은 평균정보를 잘 추정하는것이 중요하며, 이는 기세를 모델링하는 것과는 별 상관이 없다. (즉 “좁은”의미의 시계열분석과 상관이 없다)\n파업선언: 시계열로 장기예측을 해라? 저는 그냥 다른 방법을 썼어요..\n\n[1] 참고로 기존에 우리가 다루었던 자료는 엄밀하게 쓰면 “관측자료 = 정보 + 독립오차” 임"
  },
  {
    "objectID": "posts/09wk-mid.out_solution.html",
    "href": "posts/09wk-mid.out_solution.html",
    "title": "[STBDA2023] 09wk-mid (ver 1.0) – 풀이업로드",
    "section": "",
    "text": "09wk-mid (ver 1.0) – 풀이업로드\n최규빈\n2023-11-01\n\n\nTrue/False를 판단하는 문제는 답만 써도 무방함.\n“자료분석” 문제는 “kaggle style score = 50%”, “분석의 논리 = 50%” 의 배점으로 채점한다.\n“자료분석” 유형의 경우 분석의 논리가 매우 우수하거나, 창의적인 접근법으로 분석을 시도할 경우 가산점을 부여한다. (아이디어 단계에서도 가산점 부여가능) 가산점은 문항점수의 최대 100%까지 부여한다.\n“연구” 문항의 경우 세부문항을 정확하게 수행한 경우에만 100%의 점수를 부여하며 이를 어길시 부분점수를 부여하지 않는다. 연구결과의 시각화나 해석이 우수한 답안은 문항점수의 20%까지 가산점을 부여한다.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.linear_model\nimport sklearn.ensemble\nimport sklearn.preprocessing \nimport sklearn.impute\nimport sklearn.model_selection\n\n\n\n1. True/False (50점)\n\nhttps://www.kaggle.com/t/7911b05108f642e094ce13a77a5a4723\n\n\n링크를 shift + 클릭하면 새 창으로 열려서 문제 풀기 수월합니다.\n\n(1)-(3): 아래는 kaggle에서 titanic 데이터를 불러오는 코드이다.\ndf_train=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# !kaggle competitions download -c titanic\n# !unzip titanic.zip -d ./titanic\n# df_train = pd.read_csv('titanic/train.csv')\n# df_test = pd.read_csv('titanic/test.csv')\n# !rm titanic.zip\n# !rm -rf titanic/\n\n(1) 학습을 할 때는 df_train만을 이용하며 df_test는 전혀 사용하지 않는다.\n\nTrue\n\n(2) df_train과 df_test의 열의 숫자는 동일하며 행의 숫자만 차이가 난다.\n\nFalse\n\n(3) 여성생존자만 모두 생존한다고 가정하면 df_train에서의 accuracy는 78% 이상이다.\n\nTrue\n\n(4)-(7) 아래는 모듈4에서 소개한 Alexis Cook의 분석코드이다.\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n(4) Alexis Cook의 코드에서는 사용된 설명변수는 [“Pclass”, “Sex”, “SibSp”, “Parch”] 이다.\n\nTrue\n\n(5) Alexis Cook의 코드에서는 로지스틱모형을 이용하여 반응변수를 예측하였다.\n\nFalse\n\n(6) model.fit(X,y)는 train data를 학습하기 위한 코드이다.\n\nTrue\n\n(7) test data에서의 성능을 확인하기 위해서는 아래의 코드를 사용하면 된다.\noutput.score(X_test,y)\n\nFalse\n\n(8)-(10) 아래는 모듈8에 소개된 코드중 일부이다.\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n# load data\n...\n...\n\n# analysis\npredictr = TabularPredictor(\"Survived\")\npredictr.fit(df_train,presets='best_quality')\n(df_train.Survived == predictr.predict(df_train)).mean()\n(8) 위의 코드에서 반응변수는 “Survived”로 설정하였다.\n\nTrue\n\n(9) predictr.fit(df_train,presets='best_quality')은 train data를 학습하는 단계이다.\n\nTrue\n\n(10) (df_train.Survived == predictr.predict(df_train)).mean()은 모형의 성능을 train data에서 검증하는 단계이다.\n\nTrue\n\n(11)-(14) 아래는 모듈11에 제시된 데이터프레임으로 선형모형의 계수값을 표현한 것이다.\n\n(11) 여성은 남성보다 평균적으로 보험료를 65.657180 만큼 더 낸다고 해석할 수 있다.\n\nFalse # 65.657180*2 만큼 더낸다\n\n(12) 나이가 1살 증가하면 보험료가 평균 256.856353 만큼 증가한다고 해석할 수 있다.\n\nTrue\n\n(13) 보험료를 상대적으로 많이 내는 지역과 그렇지 않은 지역이 존재한다.\n\nTrue\n\n(14) 흡연유무는 성별보다 보험료에 미치는 영향이 크다.\n\nTrue\n\n(15)-(19) 아래는 모듈13에 대한 설명이다.\n(15) 4.분석-실패에서 아래와 같은 에러메시지가 나오는 이유는 train data에 결측치가 포함되어 있기 때문이다.\n\n\nTrue\n\n(16) 결측치가 하나라도 포함된 모든 열을 제거한다면 에러메시지를 피할수는 있겠으나 오버피팅이 발생할 가능성이 있다.\n\nFalse\n\n(17) 따라서 결측치를 하나라도 포함된 모든 열을 제거한뒤 Lasso를 쓰면 오버피팅을 피할 수 있다.\n\nFalse\n\n(18) 결측치가 존재할 경우, 결측치가 있는 열을 제외하고 분석하는 것이 유일한 해결방법은 아니다. 예를들면 결측치를 적당한 값으로 impute한 뒤 분석할 수도 있다.\n\nTrue\n\n(19) train data와 test data에서 결측치가 포함된 행을 모두 제거한 뒤, train data 를 이용하여 적합을 시키고 test data 에서 예측한뒤 제출하는 것도 가능한 분석방법 중 하나이다. 즉 아래와 같은 의사코드는 가능한 분석방법 중 하나이다.\ndf_train = 결측치가 포함된 행이 제거된 train data\ndf_test = 결측치가 포함된 행이 제거된 test data\nX,y = df_train 을 적당히 잘 정리한 자료의 튜플형태 \nXX = df_test 를 적당히 잘 정리한 자료\npredictr = sklearn.linear_model.LogisticRegression()\npredictr.fit(X,y)\nsubmission = predictr.predict(XX)의 값을 적당히 제출용으로 정리하여 만든 df\n\nFalse # test에도 결측치가 존재한다면, 해당방법을 쓸 수 없다. 결측치가 포함된 행을 예측할 수 없기때문\n\n(20)-(24) 아래는 모듈19, 모듈20 에 대한 설명이다.\n(20) sklearn.preprocessing.MinMaxScaler()를 이용하여 스케일러를 생성하고 train data에 .fit_transform() method를 적용하면 항상 \\([0,1]\\) 사이의 값이 나온다.\n\nTrue\n\n(21) sklearn.preprocessing.MinMaxScaler()를 이용하여 스케일러는 .inverse_transform() method를 가지고 있으며 이는 변환된 값을 원래로 되돌리는 역할을 한다.\n\nTrue\n\n(22) sklearn.preprocessing.StandardScaler()는 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정한다.\n\nTrue\n\n(23) 만약에 train data와 test data로 나누어진 상황에서 스케일러를 이용하여 데이터를 변환하려면 아래와 같은 순서를 따르는 것이 하나의 방법이다.\n\n적당한 scaler 생성이후 train data 를 입력으로 넣고 .fit_transform() method 사용\n1과 동일한 scaler에 test data 를 입력으로 넣고 .transform() method 사용\n\n\nTrue\n\n(24) (23)과 동일한 상황에서 아래와 같은 변환도 해법이 될 수 있다.\n\n적당한 scaler 생성이후 train data, test data를 합쳐서 입력으로 넣고 .fit_transform() method 사용\n\n\nFalse\n\n(25)-(26) 아래는 모듈21 에 대한 설명이다.\n(25) 밸런스게임을 통하여 많은 변수를 모을수록 train data 에 대한 score를 올릴 수 있다.\n\nTrue\n\n(26) 이 모듈의 예시는 불필요한 변수의 추가가 오히려 학습에 방해되어 test score를 낮추는 현상이 생길 수 있음을 시사한다.\n\nTrue\n\n(27)-(33) 아래는 모듈22 에 대한 설명이다.\n(27) 이 모듈의 데이터에서 toeic과 toeic0의 상관계수값은 매우 높게 나올 것이다.\n\nTrue\n\n(28) 이 모듈의 데이터에서 모든변수를 넣고 적합한다면,toeic, toeic0, ... ,toeic499에 적합된 계수값의 합은 0.01 정도의 값을 가진다.\n\nTrue\n\n(29) 이 모듈의 데이터에서 gpa와 toeic만을 남기고 나머지 변수를 제거한뒤 돌리면 test score가 향상된다.\n\nTrue\n\n(30) 즉 이 모듈의 데이터는 변수를 제거할수록 test score가 향상된다. 예를들면 gpa를 제거하여도 test score가 향상된다.\n\nFalse\n\n(31) 이 예제에서 train score는 모든 데이터를 사용하여 적합하였을 경우 가장 높게 나타난다.\n\nTrue # Train score는 높게나온다. Test score가 문제여서 그렇지..\n\n(32) toeic, toeic1부터 toeic499까지의 변수들은 모두 employment_score와 높은 상관계수를 보인다.\n\nTrue\n\n(33) 모듈22에 소개된 현상을 다중공선성이라고 말하며 이때는 계수값의 해석이 용이하지 않다.\n\nTrue\n\n(34)-(38) 아래는 다중공선성과 Rigde, Lasso에 대한 종합적인 설명이다.\n(34) 다중공선성이 발생할 경우 올바른 변수선택으로 언제나 문제를 해결할 수 있다.\n\nFalse\n\n(35) 다중공선성이 발생할 경우 \\({\\bf X}\\)의 correlation matrix는 단위행렬에 가깝게 나온다.\n\nFalse\n\n(36) 다중공선성이 발생한다면 Ridge, Lasso와 같은 방법을 이용하여 해결할 수 있다.\n\nTrue\n\n(37) 관측치의 수가 매우커진다면 (즉 데이터프레임에서 행의 숫자가 매우 커진다면) 다중공선성 문제가 심화될 가능성이 있다.\n\nFalse\n\n(38) 다중공선성을 아래와 같은 시각화 코드로 진단할 수 있는 경우가 있다.\nsns.heatmap(df_train.corr(),annot=True)\n\nTrue\n\n(39)-(42) 아래는 모듈23 에 대한 설명이다.\n(39) toeic, toiec0, …, toeic499 에 대응하는 계수값의 합은 \\(\\alpha\\) 와 관련없이 항상 일정하다.\n\nFalse\n\n(40) 이 모듈에서 큰 \\(\\alpha\\)는 토익계수들의 값을 점점 비슷하게 만들어 주는 효과가 있다.\n\nTrue\n\n(41) toeic, toiec0, …, toeic499 에 대응하는 계수값들의 분산은 \\(\\alpha\\)가 커짐에 따라 점점 작아진다.\n\nTrue\n\n(42) \\(\\alpha\\)를 매우 잘 선택한다면 모듈23-4.True 에서 소개된 바와 같이 적절한 변수선택을 할 수 있다.\n\nFalse\n\n(43)-(44) 아래는 모듈25 에 대한 설명이다.\n(43) \\(\\alpha\\)가 커질수록 0이 아닌 계수값의 수는 점점 증가한다.\n\nFalse\n\n(44) Lasso를 사용하면, 토익과 유사한 변수의 수가 증가할수록 test_score가 좋아진다.\n\nFalse\n\n(45)-(47) 아래는 모듈27 에 대한 설명이다.\n(45) 강의교재에 소개된 아래의 그림은 train score 보다 test score 가 낮은 상황이다.\n\n\nTrue\n\n(46) 하지만 이러한 적합결과가 오버피팅이라고 보기는 어렵다.\n\nTrue\n\n(47) 아웃라이어를 제거한다면 선형모형의 df_train, df_test에 대한 score가 모두 상승할 것이다.\n\nTrue\n\n(48)-(50) 아래는 모듈29에 대한 설명이다.\n(48) 모듈29, 4.분석에 소개된 아래와 같은 모형은\n\n\\({\\bf X}\\): Supplement, Exercise, Supplement \\(\\times\\) Exercise\n\\({\\bf y}\\): Weight_Loss\n\n운동을 했으며, 보조제를 사용한 경우에 발생하는 추가적인 효과를 고려할 수 있다.\n\nTrue\n\n(49) 이처럼 두 종류 이상의 범주형 범수가 존재하는 경우 범주형 범수간의 교호작용을 항상 고려해야 모형의 성능을 올릴 수 있다.\n\nFalse\n\n(50) 실제로 교호작용이 있지만 이를 고려하지 않아서 모형의 성능이 떨어지는 경우는 언터피팅의 일종으로 볼 수 있다.\n\nTrue\n\n\n\n2. 자료분석 (30점)\n(1) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/icesales_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/icesales_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 sales로 설정하고 나머지는 설명변수로 설정하라.\ndf_test에 sales에 대한 예측값을 포함하는 열을 추가하라.\n\n(풀이)\n\ndf_train.head()\n\n\n# step1 \nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\nXX = pd.get_dummies(df_test[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y) \n# step4 \nyhat = predictr.predict(XX) \ndf_test['sales'] = yhat\n\n(더 좋은 풀이)\n\nsns.scatterplot(df_train, x='temp', y='sales', hue='type')\n\n\n\n\n시각화를 해보고 type은 y를 예측할때 도움이 되지 않음을 파악하고 type을 제외하고 예측하면 가산점.\n(2) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 height로 설정하고 나머지는 설명변수로 설정하라.\ndf_test에 height에 대한 예측값을 포함하는 열을 추가하라.\n\n(풀이)\n\ndf_train.head()\n\n\n# step1 \n## 데이터분리\nX = pd.get_dummies(df_train[['weight','sex']],drop_first=True)\nXX = pd.get_dummies(df_test[['weight','sex']],drop_first=True)\ny = df_train['height']\n## 결측치처리\nimputer = sklearn.impute.SimpleImputer()\nX[['weight']] = imputer.fit_transform(X[['weight']])\nXX[['weight']] = imputer.transform(XX[['weight']])\n## 교호작용고려 \nX['interaction'] = X['weight']*X['sex_male']\nXX['interaction'] = XX['weight']*XX['sex_male']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y) \n# step4 \nyhat = predictr.predict(XX) \ndf_test['height'] = predictr.predict(XX)\n\n적합결과 시각화 (train)\n\nsns.scatterplot(df_train, x='weight',y='height',hue='sex',alpha=0.3)\nsns.lineplot(df_train,x='weight',y=predictr.predict(X),hue='sex',linestyle='--')\n\n\n\n\n\n이정도만 풀어도 만점으로 인정\n\n(더 좋은 풀이)\n남여의 몸무게 평균이 다르므로 그룹별로 몸무게를 예측하는것이 더 올바르다.\n\n# step1 \n## 데이터분리\nX = pd.get_dummies(df_train[['weight','sex']],drop_first=True)*1.0\nXX = pd.get_dummies(df_test[['weight','sex']],drop_first=True)*1.0\ny = df_train['height']\n## 결측치처리\n# -- male \nimputer_male = sklearn.impute.SimpleImputer()\nX[X.sex_male==1] = imputer_male.fit_transform(X[X.sex_male==1])\nXX[XX.sex_male==1] = imputer_male.transform(XX[XX.sex_male==1])\n# -- female \nimputer_female = sklearn.impute.SimpleImputer()\nX[X.sex_male==0] = imputer_female.fit_transform(X[X.sex_male==0])\nXX[XX.sex_male==0] = imputer_female.fit_transform(XX[XX.sex_male==0])\n## 교호작용고려 \nX['interaction'] = X['weight']*X['sex_male']\nXX['interaction'] = XX['weight']*XX['sex_male']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y) \n# step4 \nyhat = predictr.predict(XX) \ndf_test['height'] = predictr.predict(XX)\n\n적합결과 시각화 (train)\n\nsns.scatterplot(df_train, x='weight',y='height',hue='sex',alpha=0.3)\nsns.lineplot(df_train,x='weight',y=predictr.predict(X),hue='sex',linestyle='--')\n\n\n\n\n(3) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 y로 설정하고 나머지 X1,X2,X3,X4는 설명변수로 설정하라.\ndf_test에 y에 대한 예측값을 포함하는 열을 추가하라.\n\n(풀이)\n\n# 생성된 데이터 시각화\nsns.heatmap(df_train.corr())\nfig = plt.gcf()\nfig.suptitle(\"Figure1\")\n\nfig,ax = plt.subplots(2, 2)\nax[0,0].plot(df_train.X1, df_train.y,'o')\nax[0,0].set_title(r'$(X_1,y)$')\nax[0,1].plot(df_train.X2, df_train.y,'o')\nax[0,1].set_title(r'$(X_2,y)$')\nax[1,0].plot(df_train.X3, df_train.y,'o')\nax[1,0].set_title(r'$(X_3,y)$')\nax[1,1].plot(df_train.X4, df_train.y,'o')\nax[1,1].set_title(r'$(X_4,y)$')\nfig.suptitle(\"Figure2\")\nfig.tight_layout()\n\n\nfig, ax = plt.subplots()\nax.plot(df_train.X2, df_train.X3,'o')\nax.set_title(r'Figure3: $(X_2,X_3)$')\n#fig.suptitle(\"Figure3\")\n\n\n\n\n\n\n\n\n\n\n- \\((X_1,X_4)\\)은 \\(y\\)와 관련이 없으므로 제외한다. (Figure1, Figure2)\n- \\((X_2,X_3)\\)은 모두 \\(y\\)와 관련이 있으나 둘 사이의 선형관계가 존재하므로 공선성의 관계가 있다. (Figure1, Figure3) 따라서 둘중하나를 제거하고 분석하거나, Lasso/Ridge중 하나를 쓰는것이 합리적으로 보인다. 본 풀이에서는 둘 중 하나를 제거하고 분석하도록 하겠다.\n\n# step1 \nX = df_train[['X3']]\ny = df_train['y']\nXX = df_test[['X3']]\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_test['y'] = predictr.predict(XX)\n\n- 적합결과 시각화\n\nplt.plot(df_train['y'],df_train['y'],'-',label='True')\nplt.plot(df_train['y'],predictr.predict(X),'o',label='Predictted')\nplt.title(f'score ={predictr.score(X,y):.4f}')\nplt.legend()\n\n\n\n\n(더 좋은 풀이) -- 1\n\\((X_3,y)\\) 사이에 비 선형성이 있어보이므로, 의사결정나무를 이용하여 적합하여 보자.\n\n# step1 \nX = df_train[['X3']]\ny = df_train['y']\nXX = df_test[['X3']]\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_test['y'] = predictr.predict(XX)\n\n\nplt.plot(df_train['y'],df_train['y'],'-',label='True')\nplt.plot(df_train['y'],predictr.predict(X),'o',label='Predictted')\nplt.title(f'score ={predictr.score(X,y):.4f}')\nplt.legend()\n\n\n\n\n\n오버핏이 염려되긴 하지만 해봄직하다.\n\n(더 좋은 풀이) -- 2\n\\((X_3,y)\\) 사이에 비 선형성이 있어보이므로, \\((X_3)^2\\)을 모델링하여 적합하자. (배운내용X, 이러한 방법을 ploynomial regression 혹은 자기자신과의 교호작용을 고려한 모형이라고 한다.)\n\n# step1 \nX = df_train.assign(X3_square = df_train['X3']**2)[['X3','X3_square']]\ny = df_train['y']\nXX = df_test.assign(X3_square = df_test['X3']**2)[['X3','X3_square']]\n#---#\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_test['y'] = predictr.predict(XX)\n\n\nplt.plot(df_train['y'],df_train['y'],'-',label='True')\nplt.plot(df_train['y'],predictr.predict(X),'o',label='Predictted')\nplt.title(f'score ={predictr.score(X,y):.4f}')\nplt.legend()\n\n\n\n\n\n사실 이게 트루모델임..\n\n\n\n3. 연구 (20점)\n아래의 코드를 이용하여 자료를 불러오라.\n\nnp.random.seed(42)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n아래의 절차에 따라 자료를 연구하라. 결과를 해석하라.\n\n종속변수 \\({\\bf y}\\)를 employment_score로 설정하고 나머지는 설명변수로 설정하라.\nsklearn.linear_model.Ridge를 이용하여 \\(\\alpha = [10^1, 10^{1.5}, 10^2, 10^{2.5}, \\dots, 10^{9}]\\)에 대해 predictor를 생성하라.\n\\(\\alpha\\)의 변화에 따른 train score 와 test score 의 변화를 시각화하라.\n\\(\\alpha\\)에 변화에 따른 predictor가 학습한 coefficient값 (predictor.coef_) 들의 분산변화와 L2-penalty를 시각화하라.\n\n(풀이) – 생략, 강의노트 참고\n\njupyter: kernelspec: display_name: Python 3 (ipykernel) language: python name: python3 language_info: codemirror_mode: name: ipython version: 3 file_extension: .py mimetype: text/x-python name: python nbconvert_exporter: python pygments_lexer: ipython3 version: 3.10.13"
  },
  {
    "objectID": "posts/10wk-037.out.html",
    "href": "posts/10wk-037.out.html",
    "title": "[STBDA2023] 10wk-037: 아이스크림 – 의사결정나무, max_depth",
    "section": "",
    "text": "최규빈\n2023-11-10"
  },
  {
    "objectID": "posts/10wk-037.out.html#a.-max_depth1",
    "href": "posts/10wk-037.out.html#a.-max_depth1",
    "title": "[STBDA2023] 10wk-037: 아이스크림 – 의사결정나무, max_depth",
    "section": "A. max_depth=1",
    "text": "A. max_depth=1\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n- 결과 시각화\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,predictr.predict(X),'--.',label='Predicted')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc3258d6d90&gt;\n\n\n\n\n\n\n5.XX를 기준으로 24.aaa 값과 41.bbb 값을 예측함\n\n- tree 시각화\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.25, 'squared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.75, 0.25, 'squared_error = 49.428\\nsamples = 55\\nvalue = 41.489')]"
  },
  {
    "objectID": "posts/10wk-037.out.html#b.-max_depth2",
    "href": "posts/10wk-037.out.html#b.-max_depth2",
    "title": "[STBDA2023] 10wk-037: 아이스크림 – 의사결정나무, max_depth",
    "section": "B. max_depth=2",
    "text": "B. max_depth=2\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=2) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=2)\n\n\n- 결과 시각화\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,predictr.predict(X),'.--',label='Predicted')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc32583b940&gt;\n\n\n\n\n\n- tree 시각화\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.8333333333333334, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.5, 'x[0] &lt;= 1.75\\nsquared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.125, 0.16666666666666666, 'squared_error = 15.12\\nsamples = 19\\nvalue = 19.105'),\n Text(0.375, 0.16666666666666666, 'squared_error = 8.587\\nsamples = 26\\nvalue = 28.94'),\n Text(0.75, 0.5, 'x[0] &lt;= 10.7\\nsquared_error = 49.428\\nsamples = 55\\nvalue = 41.489'),\n Text(0.625, 0.16666666666666666, 'squared_error = 19.819\\nsamples = 47\\nvalue = 39.251'),\n Text(0.875, 0.16666666666666666, 'squared_error = 21.051\\nsamples = 8\\nvalue = 54.638')]"
  },
  {
    "objectID": "posts/10wk-037.out.html#c.-애니메이션",
    "href": "posts/10wk-037.out.html#c.-애니메이션",
    "title": "[STBDA2023] 10wk-037: 아이스크림 – 의사결정나무, max_depth",
    "section": "C. 애니메이션",
    "text": "C. 애니메이션\n- step1~4\n\n## step1 \nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictrs = [sklearn.tree.DecisionTreeRegressor(max_depth=k) for k in range(1,11)]\n## step3 \nfor k in range(10):\n    predictrs[k].fit(X,y)\n## step4 -- pass\n\n- 애니메이션\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5) \n    ax.plot(X,predictrs[frame].predict(X),'.--') \n    ax.set_title(f'max_depth={predictrs[frame].max_depth}')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=10\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/03wk-010.out.html",
    "href": "posts/03wk-010.out.html",
    "title": "[STBDA2023] 03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/03wk-010.out.html#a.-데이터",
    "href": "posts/03wk-010.out.html#a.-데이터",
    "title": "[STBDA2023] 03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "A. 데이터",
    "text": "A. 데이터\n\nX = df[['temp','type']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nX = X.assign(type = [type == 'choco' for type in X.type])"
  },
  {
    "objectID": "posts/03wk-010.out.html#b.-predictor-생성",
    "href": "posts/03wk-010.out.html#b.-predictor-생성",
    "title": "[STBDA2023] 03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-010.out.html#c.-학습-fit-learn",
    "href": "posts/03wk-010.out.html#c.-학습-fit-learn",
    "title": "[STBDA2023] 03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-010.out.html#d.-예측-predict",
    "href": "posts/03wk-010.out.html#d.-예측-predict",
    "title": "[STBDA2023] 03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n\nyhat = predictr.predict(X)\n\n\nplt.plot(df.temp,df.sales,'o',alpha=0.5)\nplt.plot(df.temp,yhat, 'x',alpha=0.5)\n\n\n\n\n- 어떻게 맞춘거지?\n\\[\\text{아이스크림 판매량} = 40 + \\text{아이스크림종류} \\times (-20) + \\text{온도} \\times 2.5 + \\text{오차(운)}\\]\n\npredictr.coef_, predictr.intercept_\n\n(array([[  2.52239574, -20.54021854]]), array([40.16877158]))\n\n\n- 온도가 -2이고, type이 초코라면? 예측값은?\n\nXnew = pd.DataFrame({'temp':[-2.0],'type':[1]})\nXnew\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n1\n\n\n\n\n\n\n\n\npredictr.predict(Xnew)\n\narray([[14.58376156]])\n\n\n- 온도가 -2이고, type이 바닐라라면? 예측값은?\n\nXnew = pd.DataFrame({'temp':[-2.0],'type':[0]})\nXnew\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n0\n\n\n\n\n\n\n\n\npredictr.predict(Xnew)\n\narray([[35.1239801]])"
  },
  {
    "objectID": "posts/05wk-020.out.html",
    "href": "posts/05wk-020.out.html",
    "title": "[STBDA2023] 05wk-020: StandardScaler를 이용한 전처리",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임\n\n\n05wk-020: StandardScaler를 이용한 전처리\n최규빈\n2023-10-05\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-zF3fkpXnSHg8H9C9VSq-vC&si=2IRrJzFgKagzxtkO\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing \n\n\n\n3. StandardScaler()\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 스케일러를 생성\n\nsclr = sklearn.preprocessing.StandardScaler()\nsclr.fit_transform(df)\n\narray([[-0.8680409 , -0.98104887],\n       [ 1.81575704, -0.73905505],\n       [ 0.3061207 ,  0.75205327],\n       [-1.10287322, -0.08287854],\n       [ 0.17193081,  2.13248542],\n       [-1.10287322,  0.44828929],\n       [-0.34805505, -0.77533368],\n       [ 1.12803382, -0.75451182]])\n\n\n- 계산식\n\n(df.toeic - df.toeic.mean())/df.toeic.std(ddof=0) # 계산식\n\n0   -0.868041\n1    1.815757\n2    0.306121\n3   -1.102873\n4    0.171931\n5   -1.102873\n6   -0.348055\n7    1.128034\nName: toeic, dtype: float64\n\n\nddof: 자유도 설정하는\n\n\n4. 비교\n- MinMaxScaler와 StandardScaler는 데이터의 스케일을 조정하는 두 가지 일반적인 방법이다.\n\nMinMaxScaler:\n\n작동 원리: 데이터를 0과 1 사이의 값으로 조정\n장점: 원하는 범위 내로 데이터를 조정할 때 유용. 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력 값을 조정하는 데 유용.[1]\n단점: 이상치에 매우 민감하다. 이상치 때문에 전체 데이터의 스케일이 크게 영향받을 수 있음.\n\nStandardScaler:\n\n작동 원리: 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정.\n장점: 이상치에 MinMaxScaler보다 덜 민감함. 많은 통계적 기법들, 특히 PCA 같은 선형 알고리즘에서 잘 작동함.[2]\n단점: MinMaxScaler와 달리, 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.[3]\n\n\n- 무식한 설명 (1)\n\nMinMaxScaler: 컴퓨터공학과, 전자공학과 느낌\nStandardScaler: 통계학과 느낌\n\n- 무식한 설명 (2)\n\nMinMaxScaler: 데이터가 기본적으로 0~1 혹은 -1 ~ 1 사이의 범위에 있다고 가정한다.\nStandardScaler: 데이터가 기본적으로 정규분포를 따른다고 가정하는 모형들과 잘 맞는다.\n\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항:\n\n이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 예를 들어, 신경망은 일반적으로 0과 1 사이의 값이나 -1과 1 사이의 값으로 입력을 받는 활성화 함수를 사용하므로 MinMaxScaler가 적합할 수 있다.\n\n결론적으로, 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다.\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항 – 무식한 설명\n\n보통은 아무거나 해도 큰일 안남.\n아주 특수한 경우[4]를 제외하고는 어차피 이론적인 선택기준은 없음.\n\n[1] sigmoid, tanh와 같은 활성화 함수의 출력값과 맞추는 용도\n[2] 그야 PCA는 정규분포를 가정하고 만든 알고리즘이라~\n[3] MinMaxScaler도 딱히 엄격하게 보장하는건 아니야\n[4] Classical PCA"
  },
  {
    "objectID": "posts/07wk-027.out.html",
    "href": "posts/07wk-027.out.html",
    "title": "[STBDA2023] 07wk-027: 아이스크림(이상치) / 회귀분석",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임\n\n\n07wk-027: 아이스크림(이상치) / 회귀분석\n최규빈\n2023-10-17\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-yf6Ht2bBpj-mf50aPboSsu&si=6dYX9WfoPmbbbGsG\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\n\n\n\n\n\n이상점 한개\n\n\nplt.plot(df_train.temp[1:], df_train.ice_sales[1:],'o')\n\n\n\n\n\n이상점 제외하고 그린 그림\n\n\n상상: 온도가 -4.1인 지점에서 “썰매축제”가 열렸다고 가정하자. 그래서 사람이 갑자기 많이 왔음. 그래서 아이스크림이 많이 팔렸음.\n\n\n\n4. 분석\n- 선형회귀로 적합\n\n# step1 \nX,y = df_train[['temp']], df_train['ice_sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_train['ice_sales_hat'] = predictr.predict(X)\n\n\npredictr.coef_\n\narray([-0.64479089])\n\n\n- 시각화\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\nplt.plot(df_train.temp,df_train.ice_sales_hat,'--')\n\n\n\n\n\n# plt.plot(df_train.temp,df_train.ice_sales,'o')\n# plt.plot(df_train.temp,df_train.ice_sales_hat,'--')\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\n\n\n\n\n- 새로운 unseen data를 가정, 데이터는 온도가 12.5~18 에 걸쳐있다고 가정한다.\n\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\nice_sales_hat\n\n\n\n\n0\n-4.1\n200.000000\n20.989373\n\n\n1\n-3.7\n9.234175\n20.731457\n\n\n2\n-3.0\n9.642778\n20.280103\n\n\n3\n-1.3\n9.657894\n19.183959\n\n\n4\n-0.5\n9.987787\n18.668126\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n17.508688\n10.350324\n\n\n96\n13.4\n17.105376\n9.705533\n\n\n97\n14.7\n17.164930\n8.867305\n\n\n98\n15.0\n18.555388\n8.673867\n\n\n99\n15.2\n18.787014\n8.544909\n\n\n\n\n100 rows × 3 columns\n\n\n\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(12.5,18,50)})\ndf_test['ice_sales_hat'] = predictr.predict(XX)\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--')\n\n\n\n\n\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.0193'\n\n\n\n온도가 올라갈수록 아이스크림 판매량은 줄어든다는 해석 (더 온도가 올라간다면 판매량이 음수가 나올 수도 있겠음 )\n저 정도의 아웃라이어는 모형에서 제외하는게 타당하지 않나? (하지만 저러한 아웃라이어가 데이터의 가치가 있을 수도 있음. 그런데 또 데이터의 가치가 있는지 없는지는 어떻게 판단하지?)\n\n\n\n5. Discussion\n- 딱히 정답이 없음.."
  },
  {
    "objectID": "posts/04wk-018.out.html",
    "href": "posts/04wk-018.out.html",
    "title": "[STBDA2023] 04wk-018: Predictor 깊은 이해 + 기호정리",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/04wk-018.out.html#a.-학습-이후에-예측평가-가능",
    "href": "posts/04wk-018.out.html#a.-학습-이후에-예측평가-가능",
    "title": "[STBDA2023] 04wk-018: Predictor 깊은 이해 + 기호정리",
    "section": "A. 학습 이후에 예측/평가 가능",
    "text": "A. 학습 이후에 예측/평가 가능\n- Predictor의 list생성\n\npredictors = [sklearn.linear_model.LinearRegression() for i in range(2)]\npredictors\n\n[LinearRegression(), LinearRegression()]\n\n\n\n두개의 predictor를 만들어서 리스트로 정리함\n\n- 첫번째 predictor에 접근\n\npredictors[0]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 두번째 predictor에 접근\n\npredictors[1]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 첫번째 predictor를 학습\n\npredictors[0].fit(df_train_X,df_train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 학습이후에는 .coef_, .intercept_ 값이 생성됨\n\npredictors[0].coef_, predictors[0].intercept_\n\n(array([[-0.98919997]]), array([20.74706997]))\n\n\n\npredictors[1].coef_, predictors[1].intercept_\n\nAttributeError: 'LinearRegression' object has no attribute 'coef_'\n\n\n- .coef_와 .intercept_값이 생겨야 .predict(X)를 통하여 예측을 할 수 있음\n\npredictors[0].predict(df_train_X)\n\narray([[ 0.96307061],\n       [-0.02612935],\n       [-1.01532932],\n       [-2.00452929],\n       [-2.99372926],\n       [-3.98292922],\n       [-4.97212919],\n       [-5.96132916]])\n\n\n\npredictors[1].predict(df_train_X)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n\n\n- 예측을 해야 평가를 할 수 있음\n\npredictors[0].score(df_train_X,df_train_y)\n\n0.9981996734604334\n\n\n\npredictors[1].score(df_train_X,df_train_y)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
  },
  {
    "objectID": "posts/04wk-018.out.html#b.-.fitxy에서-xy의-형식",
    "href": "posts/04wk-018.out.html#b.-.fitxy에서-xy의-형식",
    "title": "[STBDA2023] 04wk-018: Predictor 깊은 이해 + 기호정리",
    "section": "B. .fit(X,y)에서 X,y의 형식",
    "text": "B. .fit(X,y)에서 X,y의 형식\n\npredictr = predictors[0]\n\n\nXs = {'DataFrame(2d)': df_train_X, \n      'Seires(1d)': df_train_X.X,\n      'ndarray(2d)': np.array(df_train_X),\n      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n      'list(2d)': np.array(df_train_X).tolist(),\n      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n\n\nys = {'DataFrame(2d)': df_train_y, \n      'Seires(1d)': df_train_y.y,\n      'ndarray(2d)': np.array(df_train_y),\n      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n      'list(2d)': np.array(df_train_y).tolist(),\n      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n\n\ndef test(X,y):\n    try: \n        predictr.fit(X,y)\n        return 'no error'\n    except:\n        return 'error'\n\n\n{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n\n{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\n ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\n ('X=Seires(1d)', 'y=list(2d)'): 'error',\n ('X=Seires(1d)', 'y=list(1d)'): 'error',\n ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\n ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\n ('X=ndarray(1d)', 'y=list(2d)'): 'error',\n ('X=ndarray(1d)', 'y=list(1d)'): 'error',\n ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=list(2d)', 'y=Seires(1d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=list(2d)', 'y=list(2d)'): 'no error',\n ('X=list(2d)', 'y=list(1d)'): 'no error',\n ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=list(1d)', 'y=Seires(1d)'): 'error',\n ('X=list(1d)', 'y=ndarray(2d)'): 'error',\n ('X=list(1d)', 'y=ndarray(1d)'): 'error',\n ('X=list(1d)', 'y=list(2d)'): 'error',\n ('X=list(1d)', 'y=list(1d)'): 'error'}\n\n\n\n결론: X는 2d만 가능, y는 2d,1d 모두 가능"
  },
  {
    "objectID": "posts/07wk-033.out.html",
    "href": "posts/07wk-033.out.html",
    "title": "[STBDA2023] 07wk-033: 취업(다중공선성) / 의사결정나무",
    "section": "",
    "text": "07wk-033: 취업(다중공선성) / 의사결정나무\n최규빈\n2023-10-17\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-y9gv3aEYpTCSbjj97u8zHM&si=YVaZvfWaat_E0tQv\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\n\n\n\n3. Data\n\nnp.random.seed(43052)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX,y = df_train.loc[:,'gpa':],df_train['employment_score']\nXX,yy = df_test.loc[:,'gpa':],df_test['employment_score']\n\n\n실제 kaggle에서는 yy를 모르는 상황임\n\n\n\n4. 분석\n- 분석1: 의사결정나무\n\n## step1 -- pass \n## step2 \npredictr = sklearn.tree.DecisionTreeRegressor(random_state=42)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment_score_hat'] = predictr.predict(X)\ndf_test['employment_score_hat'] = predictr.predict(XX)\n#---#\nprint(f'train score: {predictr.score(X,y):.4f}')\nprint(f'test score: {predictr.score(XX,yy):.4f}')\n\ntrain score: 1.0000\ntest score: 0.8300\n\n\n\n오버핏이긴한데 나쁘지 않음..\n\n- 분석2: Lasso\n\n## step1 -- pass \n## step2 \npredictr = sklearn.linear_model.LassoCV()\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment_score_hat'] = predictr.predict(X)\ndf_test['employment_score_hat'] = predictr.predict(XX)\n#---#\nprint(f'train score: {predictr.score(X,y):.4f}')\nprint(f'test score: {predictr.score(XX,yy):.4f}')\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.405e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.201e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.824e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.914e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.018e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.785e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.099e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.120e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.084e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.171e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.346e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.258e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.844e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.174e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.206e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.447e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.739e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.238e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.673e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.574e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.699e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.640e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.357e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.353e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.341e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.380e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.561e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.144e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.869e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.042e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.362e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.393e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.565e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.660e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.788e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.947e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.008e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.400e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.299e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.387e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.721e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.587e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.865e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.614e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.196e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.200e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.779e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.594e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.524e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.851e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.160e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.410e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.755e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.131e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.388e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.581e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.953e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.392e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\ntrain score: 0.8994\ntest score: 0.8587\n\n\n- 총평: Lasso가 좋긴해요. 그런데 의사결정나무도 나쁘지 않아요.\n\n참고로 Lasso는 엄청 발전된 모델\n의사결정나무는 아주 초기모델임"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html",
    "title": "[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-1-get-started",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-1-get-started",
    "title": "[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 1: Get started",
    "text": "Part 1: Get started\nIn this section, you’ll learn more about the competition and make your first submission.\n\nJoin the competition!\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\nThe challenge\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\nThe data\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n(1) train.csv\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n(2) test.csv\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n(3) gender_submission.csv\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died."
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-2-your-coding-environment",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-2-your-coding-environment",
    "title": "[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 2: Your coding environment",
    "text": "Part 2: Your coding environment\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\nThe Notebook\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\nYour first lines of code\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\nLoad the data\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\ntrain_data = pd.read_csv('~/Desktop/titanic/train.csv')\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\n\ntest_data = pd.read_csv(\"~/Desktop/titanic/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-3-your-first-submission",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-3-your-first-submission",
    "title": "[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 3: Your first submission",
    "text": "Part 3: Your first submission\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\nExplore a pattern\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us."
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#your-first-machine-learning-model",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#your-first-machine-learning-model",
    "title": "[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Your first machine learning model",
    "text": "Your first machine learning model\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-4-learn-more",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-4-learn-more",
    "title": "[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 4: Learn more!",
    "text": "Part 4: Learn more!\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "title": "[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?",
    "text": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nlen(predictions), len(X_test)\n\n(418, 418)\n\n\n- 이렇게 수정하면 될 듯\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nlen(predictions),len(y)\n\n(891, 891)\n\n\n\n(predictions == y).mean()\n\n0.8159371492704826"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#b.-alexis-cook의-코드를-수정해보자",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.out.html#b.-alexis-cook의-코드를-수정해보자",
    "title": "[STBDA2023] 02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "B. Alexis Cook의 코드를 수정해보자!",
    "text": "B. Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n- 모형에서 하이퍼파라메터 조정을 해주면 성능이 좋아진다.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\n(predictions == y).mean()\n\n0.8170594837261503\n\n\n\n내가 만든게 더 좋은데??\n\n- 이것도 제출결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n- 아래와 같이 제출하면 에러가 발생\n\npd.read_csv(\"~/Desktop/titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정_submission.csv\")\n\n- 아래와 같이 제출파일을 저장해야 한다.\n\npd.read_csv(\"~/Desktop/titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정2_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/06wk-026.out.html",
    "href": "posts/06wk-026.out.html",
    "title": "[STBDA2023] 06wk-026: 취업+각종영어점수, LassoCV",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임\n\n\n06wk-026: 취업+각종영어점수, LassoCV\n최규빈\n2023-10-05\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-xANeG7u_3isOkp7zm-2-RM&si=wkbrF2i9zUPcgZbn\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. LassoCV\n- LassoCV 클래스에서 모형을 선택해보자.\n\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.LassoCV(alphas= np.linspace(0,2,100))\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.256e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.606e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.950e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.839e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.182e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.368e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.121e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.062e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.033e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.667e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.202e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.363e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.107e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.023e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.050e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.022e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.211e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.432e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.682e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.873e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.890e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.955e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.196e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.619e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.615e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.663e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.716e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.567e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.070e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.461e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.806e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.742e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.590e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.535e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.552e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.512e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.746e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.761e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.898e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.657e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.876e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.096e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.101e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.186e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.907e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.861e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.073e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.116e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.124e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.071e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.424e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.659e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.496e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.021e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.085e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.212e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.339e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.503e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.684e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.897e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.991e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.252e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.392e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.630e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.790e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.793e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.675e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.340e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.623e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.670e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.597e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.611e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.417e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.999e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.284e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.148e+00, tolerance: 2.707e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.173e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.317e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.132e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.367e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.155e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.074e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.322e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.089e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.867e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.028e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.531e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.686e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.031e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.075e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.099e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.129e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.146e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.089e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.458e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.637e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.979e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.371e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.939e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.322e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.215e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.838e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.373e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.559e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.236e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.641e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.404e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.174e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.833e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.358e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.812e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.197e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.644e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.129e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.555e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.854e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.064e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.247e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.240e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.183e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.156e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.116e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.186e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.308e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.426e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.755e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.950e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.279e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.344e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.413e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.430e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.501e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.526e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.518e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.807e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.185e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.238e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.341e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.402e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.280e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.104e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.067e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.970e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.733e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.357e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.054e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.661e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.290e+00, tolerance: 2.707e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.021e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.739e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.749e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.104e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.181e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.324e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.330e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.041e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.544e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.969e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.370e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.501e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.239e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.585e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.942e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.750e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.008e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.097e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.320e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.642e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.913e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.018e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.223e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.570e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.959e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.804e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.097e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.040e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.837e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.174e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.087e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.304e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.229e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.118e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.808e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.491e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.042e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.231e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.454e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.437e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.412e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.513e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.981e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.463e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.113e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.770e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.355e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.757e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.891e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.085e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.244e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.422e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.914e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.216e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.495e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.764e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.933e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.952e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.845e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.643e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.445e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.606e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.783e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.891e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.877e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.539e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.476e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.625e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.708e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.620e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.362e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.950e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.254e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.393e+00, tolerance: 2.670e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.197e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.270e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.127e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.962e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.102e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.142e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.889e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.129e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.839e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.037e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.159e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.009e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.035e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.507e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.887e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.061e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.625e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.850e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.891e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.744e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.504e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.189e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.201e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.821e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.425e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.580e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.699e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.673e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.553e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.360e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.166e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.074e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.933e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.624e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.675e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.796e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.642e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.234e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.569e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.900e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.798e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.457e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.823e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.293e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.609e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.510e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.789e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.410e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.209e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.365e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.782e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.862e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.911e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.929e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.003e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.040e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.009e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.973e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.105e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.305e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.849e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.026e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.989e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.921e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.289e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.330e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.570e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.744e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.730e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.588e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.491e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.287e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.947e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.513e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.394e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.192e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.801e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.565e+00, tolerance: 2.721e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.107e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.473e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.119e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.834e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.138e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.546e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.917e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.409e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.762e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.701e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.126e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.237e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.311e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.201e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.786e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.040e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.150e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.121e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.338e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.364e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.247e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.355e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.611e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.947e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.317e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.165e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.426e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.748e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.115e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.297e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.679e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.415e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.820e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.534e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.187e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.834e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.166e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.067e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.816e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.875e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.271e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.280e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.259e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.209e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.826e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.719e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.120e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.270e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.457e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.620e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.863e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.935e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.963e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.933e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.607e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.400e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.298e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.439e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.569e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.869e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.773e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.654e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.988e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.395e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.979e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.566e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.866e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.129e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.319e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.439e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.399e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.270e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.128e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.656e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.321e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.010e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.703e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.331e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.059e+00, tolerance: 2.540e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLassoCV(alphas=array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061...\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ]))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCVLassoCV(alphas=array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061...\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ]))\n\n\n\npredictr.score(X,y)\n\n0.9483494983570957\n\n\n\npredictr.score(XX,yy)\n\n0.8749691641081608\n\n\n\n\n5. HW\n- 현재 predictr 에 저장된[1] alpha값을 조사하고, 그것을 바탕으로 아래의 코드를 수정하여 위와 동일한 train_score, test_score가 나오도록 하라.\n수정할코드\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Lasso(alpha=????)\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n나와야할 결과\n[1] LassoCV클래스에서 생성된 predictor\n\npredictr.alpha_\n\n0.36363636363636365\n\n\n\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Lasso(alpha=0.36363636363636365)\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLasso(alpha=0.36363636363636365)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.36363636363636365)\n\n\n\npredictr.score(X,y)\n\n0.9483494983570957\n\n\n\npredictr.score(XX,yy)\n\n0.8749691641081608"
  },
  {
    "objectID": "posts/06wk-022.out.html",
    "href": "posts/06wk-022.out.html",
    "title": "[STBDA2023] 06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/06wk-022.out.html#a.-분석절차",
    "href": "posts/06wk-022.out.html#a.-분석절차",
    "title": "[STBDA2023] 06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "A. 분석절차",
    "text": "A. 분석절차\n- step1: 데이터정리\n\ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX = df_train.loc[:,'gpa':'toeic499']\nXX = df_test.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n\n- step2: predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()\n\n- step3: 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- step4: 예측: 생략"
  },
  {
    "objectID": "posts/06wk-022.out.html#b.-계수해석-및-평가",
    "href": "posts/06wk-022.out.html#b.-계수해석-및-평가",
    "title": "[STBDA2023] 06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "B. 계수해석 및 평가",
    "text": "B. 계수해석 및 평가\n- 계수해석\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns \ns\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n\n실제계수값은 토익*1/100, GPA*1.0, 나머지 toeic0 ~ toeic499 는 모두 계수값이 0임\n그러나 학습된 계수값은 그렇지 않음.\n\n- 평가: train/test score 계산\n\npredictr.score(X,y)\n\n1.0\n\n\n\ntrain 에서는 잘맞음 (퍼펙트) – 모의고사는 기가막히게 잘품\n\n\npredictr.score(XX,yy)\n\n0.11705078212496767\n\n\n\ntest 에서는 잘 맞지 않음 – 수능을 보면 망한다."
  },
  {
    "objectID": "posts/06wk-022.out.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면-오라클..",
    "href": "posts/06wk-022.out.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면-오라클..",
    "title": "[STBDA2023] 06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면? (오라클..)",
    "text": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면? (오라클..)\n- 분석절차수행\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic']\nXX = df_test.loc[:,'gpa':'toeic']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 계수해석\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa      0.972163\ntoeic    0.010063\ndtype: float64\n\n\n\n실제계수값인 GPA*1.0, 토익*1/100이 잘 추정됨\n\n- score도 괜찮음\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.9133\ntest_score: 0.9127"
  },
  {
    "objectID": "posts/06wk-022.out.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "href": "posts/06wk-022.out.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "title": "[STBDA2023] 06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "B. 하다못해 toeic0과 gpa로 적합했다면?",
    "text": "B. 하다못해 toeic0과 gpa로 적합했다면?\n- 분석절차\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,['gpa','toeic0']]\nXX = df_test.loc[:,['gpa','toeic0']]\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 계수해석\n\npredictr.coef_\n\narray([0.98130228, 0.0101011 ])\n\n\n\n합리적으로 추정된것 같음\n\n- 평가\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.9121\ntest_score: 0.9115\n\n\n\n오라클 만큼은 아니지만 이정도만 되어도 합리적임"
  },
  {
    "objectID": "posts/13wk-56.out.html",
    "href": "posts/13wk-56.out.html",
    "title": "[STBDA2023] 13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-56.out.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-56.out.html#a.-y의-분포-xy의-관계-시각화",
    "title": "[STBDA2023] 13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='Survived',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nSurvived\n891\n0.383838\n0.486592\n0.0\n0.0\n0.0\n1.0\n1.0\nint64\n2\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for Survived &gt;= 0.5\n\n\n\n\n\nFeature interaction between Sex/Survived in train_data"
  },
  {
    "objectID": "posts/13wk-56.out.html#target-variable-analysis",
    "href": "posts/13wk-56.out.html#target-variable-analysis",
    "title": "[STBDA2023] 13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-56.out.html#b.-중요한-설명변수",
    "href": "posts/13wk-56.out.html#b.-중요한-설명변수",
    "title": "[STBDA2023] 13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data=df_train,\n    label='Survived',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231203_084504/\"\n\n\nModel Prediction for Survived\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n0.809701\n0.856\n0.002364\n0.002167\n0.142935\n0.002364\n0.002167\n0.142935\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nSex\n0.112687\n0.013033\n0.000021\n5\n0.139522\n0.085851\n\n\nName\n0.055970\n0.009140\n0.000082\n5\n0.074789\n0.037151\n\n\nSibSp\n0.026119\n0.010554\n0.002605\n5\n0.047850\n0.004389\n\n\nFare\n0.012687\n0.009730\n0.021720\n5\n0.032721\n-0.007348\n\n\nEmbarked\n0.011194\n0.006981\n0.011525\n5\n0.025567\n-0.003179\n\n\nAge\n0.010448\n0.003122\n0.000853\n5\n0.016876\n0.004020\n\n\nPassengerId\n0.008955\n0.005659\n0.012022\n5\n0.020607\n-0.002696\n\n\nCabin\n0.002985\n0.006675\n0.186950\n5\n0.016729\n-0.010758\n\n\nPclass\n0.002239\n0.005659\n0.213159\n5\n0.013890\n-0.009413\n\n\nParch\n0.001493\n0.002044\n0.088904\n5\n0.005701\n-0.002716\n\n\nTicket\n0.000000\n0.000000\n0.500000\n5\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nSurvived\n0\n1\nerror\n\n\n\n\n498\n499\n1\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0\n1\n2\n113781\n151.5500\nC22 C26\nS\n0\n0.046788\n0.953212\n0.906424\n\n\n267\n268\n3\nPersson, Mr. Ernst Ulrik\nmale\n25.0\n1\n0\n347083\n7.7750\nNaN\nS\n1\n0.932024\n0.067976\n0.864047\n\n\n569\n570\n3\nJonsson, Mr. Carl\nmale\n32.0\n0\n0\n350417\n7.8542\nNaN\nS\n1\n0.922265\n0.077735\n0.844530\n\n\n283\n284\n3\nDorking, Mr. Edward Arthur\nmale\n19.0\n0\n0\nA/5. 10482\n8.0500\nNaN\nS\n1\n0.921180\n0.078820\n0.842361\n\n\n821\n822\n3\nLulic, Mr. Nikola\nmale\n27.0\n0\n0\n315098\n8.6625\nNaN\nS\n1\n0.919709\n0.080291\n0.839419\n\n\n301\n302\n3\nMcCoy, Mr. Bernard\nmale\nNaN\n2\n0\n367226\n23.2500\nNaN\nQ\n1\n0.918546\n0.081454\n0.837093\n\n\n288\n289\n2\nHosono, Mr. Masabumi\nmale\n42.0\n0\n0\n237798\n13.0000\nNaN\nS\n1\n0.907043\n0.092957\n0.814085\n\n\n36\n37\n3\nMamee, Mr. Hanna\nmale\nNaN\n0\n0\n2677\n7.2292\nNaN\nC\n1\n0.906803\n0.093197\n0.813605\n\n\n127\n128\n3\nMadsen, Mr. Fridtjof Arne\nmale\n24.0\n0\n0\nC 17369\n7.1417\nNaN\nS\n1\n0.906605\n0.093395\n0.813210\n\n\n391\n392\n3\nJansson, Mr. Carl Olof\nmale\n21.0\n0\n0\n350034\n7.7958\nNaN\nS\n1\n0.905367\n0.094633\n0.810734\n\n\n\n\n\n\n\nRows with the least distance vs other class\n\n\nRows in this category are the closest to the decision boundary vs the other class and are good candidates for additional labeling\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nSurvived\n0\n1\nerror\n\n\n\n\n347\n348\n3\nDavison, Mrs. Thomas Henry (Mary E Finck)\nfemale\nNaN\n1\n0\n386525\n16.1000\nNaN\nS\n1\n0.510786\n0.489214\n0.021572\n\n\n192\n193\n3\nAndersen-Jensen, Miss. Carla Christine Nielsine\nfemale\n19.0\n1\n0\n350046\n7.8542\nNaN\nS\n1\n0.512167\n0.487833\n0.024334\n\n\n172\n173\n3\nJohnson, Miss. Eleanor Ileen\nfemale\n1.0\n1\n1\n347742\n11.1333\nNaN\nS\n1\n0.526793\n0.473207\n0.053585\n\n\n328\n329\n3\nGoldsmith, Mrs. Frank John (Emily Alice Brown)\nfemale\n31.0\n1\n1\n363291\n20.5250\nNaN\nS\n1\n0.531574\n0.468426\n0.063149\n\n\n593\n594\n3\nBourke, Miss. Mary\nfemale\nNaN\n0\n2\n364848\n7.7500\nNaN\nQ\n0\n0.463840\n0.536160\n0.072319\n\n\n376\n377\n3\nLandergren, Miss. Aurora Adelia\nfemale\n22.0\n0\n0\nC 7077\n7.2500\nNaN\nS\n1\n0.549471\n0.450529\n0.098942\n\n\n607\n608\n1\nDaniel, Mr. Robert Williams\nmale\n27.0\n0\n0\n113804\n30.5000\nNaN\nS\n1\n0.558802\n0.441198\n0.117605\n\n\n113\n114\n3\nJussila, Miss. Katriina\nfemale\n20.0\n1\n0\n4136\n9.8250\nNaN\nS\n0\n0.439738\n0.560262\n0.120524\n\n\n889\n890\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n1\n0.568476\n0.431524\n0.136952\n\n\n18\n19\n3\nVander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\nfemale\n31.0\n1\n0\n345763\n18.0000\nNaN\nS\n0\n0.418177\n0.581823\n0.163645"
  },
  {
    "objectID": "posts/13wk-56.out.html#c.-관측치별-해석",
    "href": "posts/13wk-56.out.html#c.-관측치별-해석",
    "title": "[STBDA2023] 13wk-56: 타이타닉 / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n- 1번관측치\n\ndf_train.iloc[[1]]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[1]])\n\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f5118548dc0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n1    1\nName: Survived, dtype: int64\n\n\n\npredictr.predict_proba(df_train.iloc[[1]])\n\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f501bed2820&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n1\n0.09824\n0.90176\n\n\n\n\n\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[1]],\n    display_rows=True,\n    plot='waterfall'\n)\n\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f501bed2820&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f501bed2820&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f505c794dc0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f511f4f19d0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n\n\n\n\n\n\n\n\n\n이름이 중요하다..?\n\n- 이름을 남자처럼 바꿔보자.\n\nNote\n“Cumings, Mrs. John Bradley (Florence Briggs Thayer)”라는 이름은 일반적인 형식의 성명이 아닌, 이전 시대에 흔히 사용되던 형식 중 하나입니다. 이러한 형식은 주로 19세기와 20세기 초기에 미국과 다른 서구 국가에서 흔히 사용되었습니다.\n이 이름의 구성 요소를 설명하면 다음과 같습니다:\n\nCumings: 이것은 가족 성입니다. 여성의 경우 결혼 전 성을 나타낼 수 있습니다.\nMrs. John Bradley: 이 부분은 여성의 남편의 이름과 “Mrs.”라는 여성으로서의 결혼 상태를 나타내는 부분입니다. “Mrs.”는 결혼한 여성을 가리키며, “John Bradley”는 그녀의 남편의 이름입니다.\nFlorence Briggs Thayer: 이것은 여성의 본래 이름 또는 결혼 전 이름입니다. 종종 이러한 형식의 이름에서는 여성의 결혼 전 이름을 괄호로 감싸서 표시합니다.\n\n즉, “Cumings, Mrs. John Bradley (Florence Briggs Thayer)”는 Florence Briggs Thayer라는 여성이 John Bradley Cumings와 결혼하여 Mrs. John Bradley Cumings로서의 결혼 상태를 나타내는 방식으로 표시된 이름입니다. 이러한 형식은 오늘날에는 더 이상 널리 사용되지 않으며, 대부분의 경우 현재는 결혼 후에도 자신의 본래 성과 이름을 유지하는 것이 일반적입니다.\n\n\nonerow = df_train.iloc[[1]].copy()\nonerow['Name'] = 'Cumings, Mr. John Bradley'\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=onerow,\n    display_rows=True,\n    plot='waterfall'\n)\n\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f51184b0d30&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f51184b0d30&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f5118553c10&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f505c794d30&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mr. John Bradley\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n\n\n\n\n\n\n\n\n\n이름이 남자…"
  },
  {
    "objectID": "posts/04wk-015.out.html",
    "href": "posts/04wk-015.out.html",
    "title": "[STBDA2023] 04wk-015: 결측치 처리, sklearn.impute",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/04wk-015.out.html#a.-숫자형자료의-impute",
    "href": "posts/04wk-015.out.html#a.-숫자형자료의-impute",
    "title": "[STBDA2023] 04wk-015: 결측치 처리, sklearn.impute",
    "section": "A. 숫자형자료의 impute",
    "text": "A. 숫자형자료의 impute\n- 주어진자료\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n- 빈칸은 대충 아래와 같이 추정하면 되지 않을까?\n\ndf.loc[3,'A'] = df.A.mean()\ndf.loc[2,'B'] = df.B.mean()\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.100\n0.0\n\n\n1\n1.900\n0.0\n\n\n2\n2.200\n0.0\n\n\n3\n2.025\n0.0\n\n\n4\n1.900\n0.0\n\n\n\n\n\n\n\n- 자동으로 하려면?\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n- sklearn.impute.SimpleImputer()\n(방법1)\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit(df)\nimputer.transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2) fit, transform 두개 기능 합친거\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n- 다른방식으로 결측값 대체\n(방법1) – 평균으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='mean')\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2) – 중앙값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='median')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [2. , 0. ],\n       [1.9, 0. ]])\n\n\n(방법3) – 최빈값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [1.9, 0. ],\n       [1.9, 0. ]])\n\n\n(방법4) – 상수대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='constant',fill_value=-999)\nimputer.fit_transform(df)\n\narray([[   2.1,    0. ],\n       [   1.9,    0. ],\n       [   2.2, -999. ],\n       [-999. ,    0. ],\n       [   1.9,    0. ]])"
  },
  {
    "objectID": "posts/04wk-015.out.html#b.-범주형자료의-impute",
    "href": "posts/04wk-015.out.html#b.-범주형자료의-impute",
    "title": "[STBDA2023] 04wk-015: 결측치 처리, sklearn.impute",
    "section": "B. 범주형자료의 impute",
    "text": "B. 범주형자료의 impute\n- 자료\n\ndf = pd.DataFrame({'A':['Y','N','Y','Y',np.nan], 'B':['stat','math',np.nan,'stat','bio']})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- 최빈값 혹은 상수대체만 가능\n(방법1) – 최빈값을 이용\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)\n\n\n(방법2) – 상수로 대체함\n\nimptr1 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='Y')\nimptr1.fit_transform(df[['A']])\n\narray([['Y'],\n       ['N'],\n       ['Y'],\n       ['Y'],\n       ['Y']], dtype=object)\n\n\n\nimptr2 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='math')\nimptr2.fit_transform(df[['B']])\n\narray([['stat'],\n       ['math'],\n       ['math'],\n       ['stat'],\n       ['bio']], dtype=object)\n\n\n\nnp.concatenate([imptr1.fit_transform(df[['A']]),imptr2.fit_transform(df[['B']])],axis=1)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)"
  },
  {
    "objectID": "posts/04wk-015.out.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "href": "posts/04wk-015.out.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "title": "[STBDA2023] 04wk-015: 결측치 처리, sklearn.impute",
    "section": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute",
    "text": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute\n# 예제: 아래의 df에서 결측치를 모두 최빈값으로 impute하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([[2.1, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'N', 'math'],\n       [2.2, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'bio']], dtype=object)"
  },
  {
    "objectID": "posts/04wk-015.out.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "href": "posts/04wk-015.out.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "title": "[STBDA2023] 04wk-015: 결측치 처리, sklearn.impute",
    "section": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute",
    "text": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute\n# 예제: 아래의 df를 숫자형일 경우는 평균대치, 문자형일 경우는 최빈값으로 대치하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n- step1: 복사본 생성\n\ndf_imputed = df.copy()\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n- step2: 데이터프레임 분리\n\ndf_num = df.select_dtypes(include=\"number\")\ndf_num\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n\ndf_cat = df.select_dtypes(exclude=\"number\")\ndf_cat \n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- step3: impute\n\ndf_imputed[df_num.columns] = sklearn.impute.SimpleImputer(strategy='mean').fit_transform(df_num)\ndf_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat)\n\n\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.100\n0.0\nY\nstat\n\n\n1\n1.900\n0.0\nN\nmath\n\n\n2\n2.200\n0.0\nY\nstat\n\n\n3\n2.025\n0.0\nY\nstat\n\n\n4\n1.900\n0.0\nY\nbio"
  },
  {
    "objectID": "posts/13wk-47.out.html",
    "href": "posts/13wk-47.out.html",
    "title": "[STBDA2023] 13wk-47: 평가지표의 계산",
    "section": "",
    "text": "13wk-47: 평가지표의 계산\n최규빈\n2023-12-01\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-zr8gM9nYpQ_0Q_OKIqeU0d&si=lX6pDp3cV_fME8mg\n\n\n2. Imports\n\nimport numpy as np\nimport sklearn.metrics\n\n\n\n3. 12wk-46 숙제풀이\n밀실안에 100명의 사람이 있다고 하자. 이중 이중휴민트는 2명이 있다고 하자. 이중휴민트를 잡기위해서 3명을 사살했다고 하자. 사살된 사람중 실제 이중휴민트는 1명이었다고 하자. 이 경우\n\naccurary\nTPR(=recall)\nprecision\nFPR\n\n값을 계산하라.\n- y, yhat\n\ny = np.array([1]*2 + [0]*98)\ny\n\narray([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nyhat = np.array([0,1,1,1]+[0]*96)\nyhat\n\narray([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n- 혼동행렬(=confusion matrix) 만들기\n\nsklearn.metrics.confusion_matrix(y,yhat)\n\narray([[96,  2],\n       [ 1,  1]])\n\n\n\n(tn,fp),(fn,tp) = sklearn.metrics.confusion_matrix(y,yhat)\n\n- accuracy\n\n(tp+tn)/(tn+fp+fn+tp) # accuracy \n\n0.97\n\n\n\nsklearn.metrics.accuracy_score(y,yhat)\n\n0.97\n\n\n- recall\n\ntp/(tp+fn)\n\n0.5\n\n\n\nsklearn.metrics.recall_score(y,yhat)\n\n0.5\n\n\n- precision\n\ntp/(tp+fp)\n\n0.3333333333333333\n\n\n\nsklearn.metrics.precision_score(y,yhat)\n\n0.3333333333333333\n\n\n- FPR\n\nfp / (fp+tn)\n\n0.02040816326530612"
  },
  {
    "objectID": "posts/07wk-034.out.html",
    "href": "posts/07wk-034.out.html",
    "title": "[STBDA2023] 07wk-034: 취업(오버피팅) / 의사결정나무",
    "section": "",
    "text": "07wk-034: 취업(오버피팅) / 의사결정나무\n최규빈\n2023-10-16\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-xYMQe_6GKus4q8E6c5RNIS&si=QtTWrQUAXDgwhxBp\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n3. Data\n\ndef generating_df(n_balance):\n    df = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n    df_balance = pd.DataFrame((np.random.randn(500,n_balance)).reshape(500,n_balance)*1,columns = ['balance'+str(i) for i in range(n_balance)])\n    return pd.concat([df,df_balance],axis=1)\n\n\ndf = generating_df(10)\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n0\n135\n0.051535\n0\n0.597081\n-0.035129\n0.854562\n0.786060\n-0.418987\n0.656819\n1.054650\n-0.583549\n-0.801648\n0.275704\n\n\n1\n935\n0.355496\n0\n-0.489958\n0.560724\n0.218998\n1.053743\n-0.873298\n0.560064\n-0.337398\n1.580405\n0.859627\n-1.225685\n\n\n2\n485\n2.228435\n0\n0.195608\n0.706463\n1.242647\n1.270597\n0.065658\n0.176606\n-0.496860\n-0.814525\n0.019076\n-0.206196\n\n\n3\n65\n1.179701\n0\n-0.417740\n-1.653030\n-0.420448\n1.585880\n-0.817104\n-1.324694\n-0.803614\n-0.677855\n-0.148533\n2.723495\n\n\n4\n445\n3.962356\n1\n0.040684\n1.544578\n-0.688206\n0.120899\n0.765603\n0.213922\n-0.753956\n-0.720528\n0.569866\n-0.540689\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0.036820\n-0.774982\n-0.862066\n0.456921\n0.443886\n0.951077\n-1.661479\n-0.718166\n-0.009465\n1.320969\n\n\n496\n310\n2.601212\n1\n-0.021882\n-1.008259\n1.126976\n0.201097\n-0.014282\n0.076847\n1.394790\n-1.216803\n-0.618279\n0.732187\n\n\n497\n225\n0.042323\n0\n-0.457187\n-0.366491\n0.217539\n-1.248033\n0.751056\n0.309329\n0.143768\n-0.818435\n1.152402\n0.582863\n\n\n498\n320\n1.041416\n0\n1.342974\n-0.123772\n0.908135\n0.240395\n0.177500\n-0.386220\n-0.569122\n-0.742096\n-0.010123\n-1.268964\n\n\n499\n375\n3.626883\n1\n-1.829297\n-0.911295\n0.616230\n0.401780\n1.619174\n0.192245\n0.332747\n-1.827712\n-0.253461\n1.180119\n\n\n\n\n500 rows × 13 columns\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.7, random_state=42)\n\n\nX,y = df_train.drop(['employment'],axis=1), df_train['employment']\nXX,yy = df_test.drop(['employment'],axis=1), df_test['employment']\n\n\n\n4. 분석\n- 분석1: 의사결정나무\n\n## step1 -- pass\n## step2 \npredictr = sklearn.tree.DecisionTreeClassifier(random_state=42)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 1.0000\ntest_score = 0.7543\n\n\n- 분석2: 로지스틱 + Ridge\n\n## step1 -- pass\n## step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(penalty='l2')\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 0.8667\ntest_score = 0.8829\n\n\n- 분석3: 로지스틱 + Lasso\n\n## step1 -- pass\n## step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(penalty='l1', solver='liblinear')\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 0.8667\ntest_score = 0.8857\n\n\n\n\n5. 연구\n- Balance 변수들의 수가 커짐에 따라서 각 방법들(의사결정나무, 로지스틱+Ridge, 로지스틱+Lasso)의 train/test score는 어떻게 변화할까?\n- df, predictor -&gt; train_score, test_score 와 같은 함수를 만들자.\n\ndef anal(df,predictr):\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.7, random_state=42)\n    X,y = df_train.drop(['employment'],axis=1), df_train['employment']\n    XX,yy = df_test.drop(['employment'],axis=1), df_test['employment']\n    ## step1 -- pass\n    ## step2 -- pass \n    ## step3 \n    predictr.fit(X,y)\n    ## step4 -- pass \n    #--#\n    return predictr.score(X,y),predictr.score(XX,yy)\n\n\npredictr = sklearn.tree.DecisionTreeClassifier()\n\n\nanal(df,predictr)\n\n(1.0, 0.7514285714285714)\n\n\n- 실험해보자.\n\nn_balance_lst = range(0,5000,50)\n\n\npredictrs = [sklearn.tree.DecisionTreeClassifier(random_state=42),\n             sklearn.linear_model.LogisticRegressionCV(penalty='l2'),\n             sklearn.linear_model.LogisticRegressionCV(penalty='l1', solver='liblinear')]\n\n\nlst = [[anal(generating_df(n_balance),predictr) for predictr in predictrs] for n_balance in n_balance_lst]\n\n- 실험결과 정리\n\narr = np.array(lst)\ntr = arr[:,:,0]\ntst = arr[:,:,1]\n\n\ndf1= pd.DataFrame(tr,columns=['tree','ridge','lasso']).eval('dataset = \"train\"').eval('n_balance = @n_balance_lst')\ndf2= pd.DataFrame(tst,columns=['tree','ridge','lasso']).eval('dataset = \"test\"').eval('n_balance = @n_balance_lst')\nresult_df = pd.concat([df1,df2]).set_index(['dataset','n_balance']).stack().reset_index().set_axis(['dataset','n_balance','method','score'],axis=1)\n\n\nsns.lineplot(result_df.query('dataset==\"test\"'),x='n_balance',y='score',hue='method')\n\nTypeError: lineplot() got multiple values for argument 'x'\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/13wk-53.out.html",
    "href": "posts/13wk-53.out.html",
    "title": "[STBDA2023] 13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-53.out.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-53.out.html#a.-y의-분포-xy의-관계-시각화",
    "title": "[STBDA2023] 13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='employment_score',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nemployment_score\n500\n7.227104\n3.115979\n-0.644716\n4.695513\n7.281178\n9.548811\n15.120906\nfloat64\n500\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for employment_score &gt;= 0.5\n\n\n\n\n\nFeature interaction between toeic/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic2/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic4/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic3/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic1/employment_score in train_data\n\n\n\n\n\nFeature interaction between toeic0/employment_score in train_data"
  },
  {
    "objectID": "posts/13wk-53.out.html#target-variable-analysis",
    "href": "posts/13wk-53.out.html#target-variable-analysis",
    "title": "[STBDA2023] 13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-53.out.html#b.-중요한-설명변수",
    "href": "posts/13wk-53.out.html#b.-중요한-설명변수",
    "title": "[STBDA2023] 13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data=df_train,\n    label='employment_score',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231203_081409/\"\n\n\nModel Prediction for employment_score\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-1.012202\n-0.979194\n0.001408\n0.000696\n0.135639\n0.001408\n0.000696\n0.135639\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\ngpa\n1.020679\n0.095581\n0.000009\n5\n1.217483\n0.823876\n\n\ntoeic0\n0.266908\n0.043618\n0.000083\n5\n0.356718\n0.177098\n\n\ntoeic2\n0.252605\n0.038863\n0.000065\n5\n0.332626\n0.172585\n\n\ntoeic\n0.241941\n0.043093\n0.000116\n5\n0.330669\n0.153212\n\n\ntoeic3\n0.161979\n0.035012\n0.000246\n5\n0.234070\n0.089888\n\n\ntoeic1\n0.158796\n0.040579\n0.000470\n5\n0.242350\n0.075242\n\n\ntoeic4\n0.136791\n0.037064\n0.000588\n5\n0.213106\n0.060476\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\nemployment_score\nemployment_score_pred\nerror\n\n\n\n\n55\n0.200267\n450\n450.310311\n464.340472\n458.213429\n456.215452\n448.932120\n2.234912\n4.809618\n2.574706\n\n\n8\n4.191552\n25\n29.000939\n22.725391\n19.529454\n35.896321\n24.151228\n2.514707\n5.069639\n2.554931\n\n\n491\n1.754276\n425\n428.686989\n439.377437\n446.630603\n439.109681\n423.056878\n8.008441\n5.771637\n2.236804\n\n\n144\n2.013480\n520\n519.674312\n521.390587\n531.847782\n511.375625\n525.305439\n8.755093\n6.543661\n2.211432\n\n\n118\n3.585276\n675\n679.425199\n680.429579\n677.878530\n674.812300\n672.177564\n12.785551\n10.599931\n2.185620\n\n\n469\n1.969145\n5\n3.785864\n4.575646\n-8.358037\n1.071854\n7.253616\n4.552622\n2.385054\n2.167567\n\n\n403\n1.678080\n70\n53.993037\n65.691879\n65.135837\n58.510651\n51.307683\n4.136691\n2.040128\n2.096563\n\n\n75\n0.564461\n795\n799.270794\n791.212118\n803.426181\n805.178974\n803.979166\n10.702375\n8.615296\n2.087079\n\n\n293\n4.364023\n990\n973.878219\n966.687506\n991.332887\n987.137768\n989.321286\n15.120906\n13.106976\n2.013931\n\n\n137\n4.248511\n115\n106.422018\n114.653052\n106.830406\n124.361062\n118.754414\n7.183716\n5.199379\n1.984337"
  },
  {
    "objectID": "posts/13wk-53.out.html#c.-관측치별-해석",
    "href": "posts/13wk-53.out.html#c.-관측치별-해석",
    "title": "[STBDA2023] 13wk-53: 취업(다중공선성) / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\ndf_train.iloc[[1]]\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\n\n\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.72357\n939.190519\n938.995672\n945.376482\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[1]])\n\n1    10.531603\nName: employment_score, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data = df_train,\n    model = predictr,\n    rows = df_train.iloc[[1]],\n    display_rows = False,\n    plot='waterfall'\n)"
  },
  {
    "objectID": "posts/07wk-032.out.html",
    "href": "posts/07wk-032.out.html",
    "title": "[STBDA2023] 07wk-032: 아이스크림(교호작용) / 의사결정나무",
    "section": "",
    "text": "07wk-032: 아이스크림(교호작용) / 의사결정나무\n최규빈\n2023-10-17\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-xIhMfXInEIhFMvFPXeM3Tg&si=PFDKhFupDaWnuVsW\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',label='choco')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',label='vanilla')\nplt.legend()\n\n\n\n\n\n\n5. 분석\n- 분석1: 선형회귀\n\n# step1\nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n#---#\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.9250'\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',alpha=0.2,label='choco')\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales_hat[df_train.type=='choco'],'--',color='C0')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',alpha=0.2,label='vanilla')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales_hat[df_train.type=='vanilla'],'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8cfb54f9a0&gt;\n\n\n\n\n\n- 분석2(의사결정나무)\n\n# step1\nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n#---#\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.9964'\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',alpha=0.2,label='choco')\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales_hat[df_train.type=='choco'],'--',color='C0')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',alpha=0.2,label='vanilla')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales_hat[df_train.type=='vanilla'],'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8cf31d6ca0&gt;\n\n\n\n\n\n* 오버피팅에 대한 제 개념: 통계에서 “관측치 = 언더라잉 + 랜덤” 으로 볼 수 있다. 모형이 설명해야할 영역은 “언더라잉” 이다. 만약에 모형이 언더라잉을 잘 설명하지 못한다면 언더피팅이고, 주어진 모형이 언더라잉을 넘어 오차항까지 설명하고 있다면 오버피팅이다.\n\n마음속의 underlying 을 간직한다 – 애매하죠?\n그 underlying 보다 잘 맞추면 오버피팅이다.\n내 마음속의 underlying 제대로 학습못하고 있다고 판단되면 모형미스 혹은 언더피팅이다.\n\n이러한 논리로 인하면 위의 의사결정나무로 적합된 결과는 오버피팅이다. (그렇지만 언더피팅보단 나을지도?)"
  },
  {
    "objectID": "posts/A3.out.html",
    "href": "posts/A3.out.html",
    "title": "[STBDA2023] A3: 개발환경의 변천사",
    "section": "",
    "text": "최규빈\n2023-11-14\n\n주의사항!!\n본 강의노트는 작성자의 상상에 근거하여 작성되었으며, 사실이 아닌 내용이 포함되어 있을 수 있습니다. (특히 옛날사람들의 코딩습관들)"
  },
  {
    "objectID": "posts/A3.out.html#a.-0세대-프로그래머-프롬프트",
    "href": "posts/A3.out.html#a.-0세대-프로그래머-프롬프트",
    "title": "[STBDA2023] A3: 개발환경의 변천사",
    "section": "A. 0세대 프로그래머 (프롬프트)",
    "text": "A. 0세대 프로그래머 (프롬프트)\n- 특징: 프롬프트만 쓴다..\n# 실습1 – python 사용\n- 윈도우에서 anaconda prompt 실행 -&gt; python\n(base) C:\\Users\\python&gt;python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; [1,2,3]+[4]\n[1, 2, 3, 4]\n&gt;&gt;&gt; a=[1,2,3]+[4]\n&gt;&gt;&gt; a\n[1, 2, 3, 4]\n- 2개를 실행할 수도 있음. (두 환경은 각각 서로 독립적인 파이썬, 변수가 공유되지 않음) \\(\\star\\)\n- 아쉬운점: `?list’와 같이 도움말 기능이 동작하지 않음\n&gt;&gt;&gt; ?list\n  File \"&lt;stdin&gt;\", line 1\n    ?list\n    ^\nSyntaxError: invalid syntax\n&gt;&gt;&gt; \n#\n# 실습2 – ipython 사용\n- 윈도우에서 anaconda prompt 실행 -&gt; ipython\n(base) C:\\Users\\python&gt;ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: a=[1,2,3]\n\nIn [2]: a\nOut[2]: [1, 2, 3]\n\nIn [3]: a+[4]\nOut[3]: [1, 2, 3, 4]\n- ?list가 가능\nIn [4]: ?list\nInit signature: list(iterable=(), /)\nDocstring:\nBuilt-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\nType:           type\nSubclasses:     _HashedSeq, StackSummary, DeferredConfigList, SList, _ImmutableLineList, FormattedText, NodeList, _ExplodedList, Stack, _Accumulator, ...\n- 색깔이 알록달록해서 문법을 보기 편하다. (구문강조)\n#\n# 실습3 – 0세대 프로그래머의 삶 with python\n- 1부터 10까지 합을 구하는 프로그램을 만들고 싶음\n- 시도1: python을 키고 아래와 같이 실행\n(base) C:\\Users\\python&gt;python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; total = 0\n&gt;&gt;&gt; for i in range(10):\n...     total=total+i\n...\n&gt;&gt;&gt; total\n45\n&gt;&gt;&gt;\n- 반성: 정답은 55인데 45가 출력되었다! \\(\\to\\) range(10)을 range(1,11)으로 바꿔야겠다!\n- 시도2: range(1,11)을 바꿔야겠다고 생각하고 다시 입력하다가 오타가 발생\n&gt;&gt;&gt; total =0\n&gt;&gt;&gt; for i in range(1,11):\n...     total = totla +i\n...\n\n앗 totla이라고 잘못쳤다.\n\n- 반성: 다음에는 정신을 똑바로 차려야겠다.\n- 불편한점: … 다..\n#\n# 실습4 – 1세대 프로그래머의 삶 with ipython\n- ipython을 사용한 프로그래머는 좀더 상황이 낫다\n(base) C:\\Users\\python&gt;ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: total = 0\n\nIn [2]: for i in range(1,11):\n   ...:     total = total + i\n   ...:\n\nIn [3]: total\nOut[3]: 55\n\n편한점1: 자동으로 들여쓰기가 되어서 편함\n편한점2: 화살표를 이용해서 for문을 쓰는 도중에 위아래로 이동가능\n불편한점1: 화살표로 이동할수는 있는데 마우스로는 이동할 수 없다.\n불편한점2: 내가 작성한 코드를 관리하기 어렵다.\n\n#"
  },
  {
    "objectID": "posts/A3.out.html#b.-1세대-프로그래머-스크립트활용",
    "href": "posts/A3.out.html#b.-1세대-프로그래머-스크립트활용",
    "title": "[STBDA2023] A3: 개발환경의 변천사",
    "section": "B. 1세대 프로그래머 (스크립트활용)",
    "text": "B. 1세대 프로그래머 (스크립트활용)\n- 특징: 스크립트 + 프롬프트를 사용\n# 실습1 – python + .py\n- 메모장을 키고 아래의 내용을 적는다.\ntotal = 0 \nfor i in range(1,11): \n    total = total + i\nprint(total)\n- 파일이름을 mysum.py로 저장한다.(파일형식을 모든파일로)\n- anaconda prompt에서 mysum.py파일이 저장된 폴더로 이동 -&gt; 실행\n(base) C:\\Users\\python&gt;cd Desktop\n\n(base) C:\\Users\\python\\Desktop&gt;dir\n C 드라이브의 볼륨에는 이름이 없습니다.\n 볼륨 일련 번호: 9AFD-A05F\n\n C:\\Users\\python\\Desktop 디렉터리\n\n2022-03-27  오전 11:32    &lt;DIR&gt;          .\n2022-03-27  오전 11:32    &lt;DIR&gt;          ..\n2022-03-27  오전 12:01             2,306 Chrome.lnk\n2022-03-26  오후 08:32             2,332 Microsoft Edge.lnk\n2022-03-27  오전 11:33                71 mysum.py\n               3개 파일               4,709 바이트\n               2개 디렉터리  743,643,467,776 바이트 남음\n\n(base) C:\\Users\\python\\Desktop&gt;python mysum.py\n55\n\n(base) C:\\Users\\python\\Desktop&gt;\n- 소감\n\n편한점1: 마우스를 이용하여 이동가능\n편한점2: 내가 작업한 내용은 바탕화면의 메모장에 저장이 되어있음\n아쉬운점: ipython의 장점은 활용못함 (구문강조, 도움말기능)\n\n#\n# 실습2 – ipython + .py\n- 전체적인 개발방식\n\n메모장: 코드를 편집, 저장\nipython: anaconda prompt처럼 메모장의 코드를 실행하고 결과를 확인 + 구문강조, 도움말확인기능 등을 이용하여 짧은 코드를 빠르게 작성\n\n- 기능\n\nipython에서 !python mysum.py를 입력하면 anaconda prompt에서 python mysum.py를 입력한 것과 같은 효과\nipython에서 %run mysum을 입력하면 메모장에서 mysum.py에 입력된 내용을 복사해서 ipython에 붙여넣어 실행한것과 같은 효과\n\n#"
  },
  {
    "objectID": "posts/A3.out.html#c.-2세대-프로그래머-ide-활용",
    "href": "posts/A3.out.html#c.-2세대-프로그래머-ide-활용",
    "title": "[STBDA2023] A3: 개발환경의 변천사",
    "section": "C. 2세대 프로그래머 (IDE 활용)",
    "text": "C. 2세대 프로그래머 (IDE 활용)\n- 메모장과 ipython을 하나로 통합한 프로그램이 등장!\n\njupyter notebook, jupyter lab\nspyder\nidle\nVScode\n…\n\n- 주피터의 트릭 (실제로 주피터는 ipython에 기생할 뿐 아무런 역할도 안해요)\n\n주피터를 실행\n새노트북을 생성 (파이썬으로 선택)\n\n\n컴퓨터는 내부적으로 ipython을 실행하고 그 ipython이랑 여러분이 방금만든 그 노트북과 연결\n\n\n처음보이는 cell에 1+1을 입력 -&gt; 쉬프트엔터 -&gt; 결과2가 출력\n\n\n처음보이는 cell하나 = 자동으로 열린 하나의 메모장\ncell 1+1을 입력 = 메모장에 1+1을 적음\n쉬프트+엔터후 결과2를 출력 = cell의 내용을 복사 -&gt; ipython에 붙여넣음 -&gt; ipython 계산된 결과를 복사 -&gt; cell로 돌아와 붙여넣기\n\n\n새로운 cell을 추가하고 2+2을 입력 -&gt; 쉬프트엔터 -&gt; 결과4가 출력\n\n\n새로운 cell을 추가 = 새로운 메모장 추가\ncell 2+2을 입력 = 새로운 메모장에 2+2를 적음\n쉬프트+엔터후 결과4를 출력 = cell의 내용을 복사 -&gt; ipython에 붙여넣음 -&gt; ipython 계산된 결과를 복사 -&gt; cell로 돌아와 붙여넣기\n\n- 중요한 사실들\n\nIDE는 내부적으로 연산을 수행하는 능력이 없다. (생각해볼것: 왜 R을 꼭 설치하고 Rstudio를 설치해야 했을까?)\n주피터에서 커널을 재시작한다는 의미는 메모장이 열린채로 ipython을 껐다가 다시 실행한다는 의미\n주피터는 단순히 ’메모장의 내용을 복사하여 붙여넣는 기계’라고 볼 수 있다. 이렇게 생각하면 주피터를 꼭 ipython에 연결할 이유는 없다. 실제로 주피터에 R을 연결해서 쓸 수 있다. 즉 하나의 IDE가 여러개의 언어와 연결될 수 있다.\nJupyterLab이라는 프로그램은 크롬에 있는 내용과 ipython간의 통신을 제어하는 프로그램일 뿐이다."
  },
  {
    "objectID": "posts/A3.out.html#d.-3세대-프로그래머-가상환경",
    "href": "posts/A3.out.html#d.-3세대-프로그래머-가상환경",
    "title": "[STBDA2023] A3: 개발환경의 변천사",
    "section": "D. 3세대 프로그래머 (가상환경)",
    "text": "D. 3세대 프로그래머 (가상환경)"
  },
  {
    "objectID": "posts/A3.out.html#e.-4세대-프로그래머-원격컴퓨터",
    "href": "posts/A3.out.html#e.-4세대-프로그래머-원격컴퓨터",
    "title": "[STBDA2023] A3: 개발환경의 변천사",
    "section": "E. 4세대 프로그래머 (원격컴퓨터)",
    "text": "E. 4세대 프로그래머 (원격컴퓨터)"
  },
  {
    "objectID": "posts/05wk-019.out.html",
    "href": "posts/05wk-019.out.html",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/05wk-019.out.html#a.-모티브",
    "href": "posts/05wk-019.out.html#a.-모티브",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 모티브",
    "text": "A. 모티브\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 모형을 돌려보고 해석한 결과\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n토익이 중요해? 아니면 학점이 중요해?\n\n토익은 990점 만점이고 gpa는 4.5만점이므로 위의 수식에서 바로 계산하는 것보단 스케일링을 해주자\n\n얼만큼 중요해?\n\n- 모티브: 토익점수를 0-1사이로 맞추고 gpa도 0-1사이로 맞추면 해석이 쉽지 않을까?"
  },
  {
    "objectID": "posts/05wk-019.out.html#b.-사용방법",
    "href": "posts/05wk-019.out.html#b.-사용방법",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. 사용방법",
    "text": "B. 사용방법\n- 스케일러 생성\n\nsclr = sklearn.preprocessing.MinMaxScaler()\n\n- fit, transform\n\nsclr.fit(df)\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\n제일 큰값은 1 제일 작은 값은 0으로 맞춰줌\n\n- fit_transform\n\nsclr.fit_transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])"
  },
  {
    "objectID": "posts/05wk-019.out.html#c.-잘못된-사용",
    "href": "posts/05wk-019.out.html#c.-잘못된-사용",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "C. 잘못된 사용",
    "text": "C. 잘못된 사용\n- sclr.fit()와 sclr.fit_transform()은 입력으로 2차원 자료구조를 기대한다. (그중에서도 은근히 numpy array를 기대함)\n\nsclr.fit_transform(df['toeic']) # df['toeic']는 1차원 자료구조\n\nValueError: Expected 2D array, got 1D array instead:\narray=[135. 935. 485.  65. 445.  65. 290. 730.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n\n\ndf['toeic']\n\n0    135\n1    935\n2    485\n3     65\n4    445\n5     65\n6    290\n7    730\nName: toeic, dtype: int64\n\n\n\nsclr.fit_transform(df[['toeic']]) # df[['toeic']]는 2차원 자료구조\n\narray([[0.08045977],\n       [1.        ],\n       [0.48275862],\n       [0.        ],\n       [0.43678161],\n       [0.        ],\n       [0.25862069],\n       [0.76436782]])\n\n\n\ndf[['toeic']]\n\n\n\n\n\n\n\n\ntoeic\n\n\n\n\n0\n135\n\n\n1\n935\n\n\n2\n485\n\n\n3\n65\n\n\n4\n445\n\n\n5\n65\n\n\n6\n290\n\n\n7\n730"
  },
  {
    "objectID": "posts/05wk-019.out.html#a.-사용방법",
    "href": "posts/05wk-019.out.html#a.-사용방법",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 사용방법",
    "text": "A. 사용방법\n- scaler를 오브젝트로 따로 만들지 않고 함수형으로 구현\n\nsklearn.preprocessing.minmax_scale(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- 이것은 심지어 1차원 자료구조에도 적용가능하다.\n\nsklearn.preprocessing.minmax_scale(df['toeic'])\n\narray([0.08045977, 1.        , 0.48275862, 0.        , 0.43678161,\n       0.        , 0.25862069, 0.76436782])\n\n\n- 열별로 스케일링을 하는게 아니라 행별로 스케일링을 하는 것도 가능하다. (여기서는 필요없지만..)\n\nsklearn.preprocessing.minmax_scale(df,axis=1)\n\narray([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.]])"
  },
  {
    "objectID": "posts/05wk-019.out.html#b.-discussions",
    "href": "posts/05wk-019.out.html#b.-discussions",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. discussions",
    "text": "B. discussions\n- 언뜻 보기에는 MinMaxScaler 보다 minmax_scale이 좋아보이는데, 생각보다 일반적으로 minmax_scale을 사용하지는 않음. 이유는 아래와 같음.\n\n파이썬을 쓰는 사람들이 함수형 접근방식보다 객체지향 접근방식을 선호한다. (이건 제생각)\n학습데이터와 테스트데이터의 스케일링시 동일한 변환을 유지하는 상황에서는 MinMaxScaler 가 유리함.\ninverse_transform 메서드를 같은 부가기능을 제공함."
  },
  {
    "objectID": "posts/05wk-019.out.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "href": "posts/05wk-019.out.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 잘못된 스케일링 방법 – 비효율의 문제",
    "text": "A. 잘못된 스케일링 방법 – 비효율의 문제\n\nsklearn.preprocessing.minmax_scale(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsklearn.preprocessing.minmax_scale(XX)\n\narray([[0. ],\n       [0.5],\n       [1. ]])\n\n\n- 이 방법은 전략적으로 비효율적인 문제이지, 치팅과 관련된 치명적인 잘못은 아니다.\n\n만약에 어떠한 경우에 이러한 전처리 방식이 오히려 전략적이라고 판단될 경우 사용할수도 있음."
  },
  {
    "objectID": "posts/05wk-019.out.html#b.-올바른-스케일링-방법",
    "href": "posts/05wk-019.out.html#b.-올바른-스케일링-방법",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. 올바른 스케일링 방법",
    "text": "B. 올바른 스케일링 방법\n- 방법1\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit(X) # \n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])\n\n\n- 방법2\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])"
  },
  {
    "objectID": "posts/05wk-019.out.html#c.-scaled_value-in-01",
    "href": "posts/05wk-019.out.html#c.-scaled_value-in-01",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "C. scaled_value \\(\\in\\) [0,1]?",
    "text": "C. scaled_value \\(\\in\\) [0,1]?\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\nsclr.transform(XX)\n\narray([[0.16666667],\n       [0.5       ],\n       [1.33333333]])\n\n\n\n스케일링된 값이 1보다 클 수도 있다."
  },
  {
    "objectID": "posts/05wk-019.out.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "href": "posts/05wk-019.out.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "title": "[STBDA2023] 05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "D. 아주 잘못된 스케일링 방법 – 정보누수",
    "text": "D. 아주 잘못된 스케일링 방법 – 정보누수\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n- 데이터를 합친다.. (미쳤어??)\n\nnp.concatenate([X,XX])\n\narray([[1. ],\n       [2. ],\n       [3. ],\n       [4. ],\n       [3.5],\n       [1.5],\n       [2.5],\n       [5. ]])\n\n\n- 합친데이터에서 스케일링\n\nsklearn.preprocessing.minmax_scale(np.concatenate([X,XX]))[:5]\n\narray([[0.   ],\n       [0.25 ],\n       [0.5  ],\n       [0.75 ],\n       [0.625]])\n\n\n- 이러한 전처리 방식을 정보누수라고 한다. (대회 규정에 따라서 탈락사유에 해당함)"
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html",
    "title": "[STBDA2023] 02wk-007: 타이타닉, Autogluon(Fsize,Dropout)",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#a.-데이터",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#a.-데이터",
    "title": "[STBDA2023] 02wk-007: 타이타닉, Autogluon(Fsize,Dropout)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"~/Desktop/titanic/train.csv\")\ntst = TabularDataset(\"~/Desktop/titanic/test.csv\")\n\n- 피처엔지니어링\n\n_tr = tr.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_tst = tst.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#b.-predictor-생성",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#b.-predictor-생성",
    "title": "[STBDA2023] 02wk-007: 타이타닉, Autogluon(Fsize,Dropout)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230917_141245/\""
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#c.-적합fit",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#c.-적합fit",
    "title": "[STBDA2023] 02wk-007: 타이타닉, Autogluon(Fsize,Dropout)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_tr) # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230917_141245/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   775.53 GB / 982.82 GB (78.9%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    38785.27 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.17s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f2dd48085e0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    0.6536   = Validation score   (accuracy)\n    0.57s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f2dd230f0d0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    0.6536   = Validation score   (accuracy)\n    0.02s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8101   = Validation score   (accuracy)\n    0.21s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8268   = Validation score   (accuracy)\n    0.21s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.56s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.7989   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    0.62s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.67s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8268   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 6.31s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230917_141245/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f2da6b8c490&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.039553  2.707510                0.000509           0.333600            2       True         14\n1        NeuralNetTorch   0.837989       0.007419  1.666336                0.007419           1.666336            1       True         12\n2         LightGBMLarge   0.826816       0.002828  0.336571                0.002828           0.336571            1       True         13\n3              LightGBM   0.826816       0.003033  0.210566                0.003033           0.210566            1       True          4\n4              CatBoost   0.826816       0.003410  0.561348                0.003410           0.561348            1       True          7\n5       NeuralNetFastAI   0.826816       0.006794  0.623764                0.006794           0.623764            1       True         10\n6               XGBoost   0.821229       0.004909  0.202243                0.004909           0.202243            1       True         11\n7      RandomForestEntr   0.821229       0.022999  0.271596                0.022999           0.271596            1       True          6\n8      RandomForestGini   0.815642       0.025080  0.294840                0.025080           0.294840            1       True          5\n9            LightGBMXT   0.810056       0.002956  0.207451                0.002956           0.207451            1       True          3\n10       ExtraTreesGini   0.804469       0.022679  0.268605                0.022679           0.268605            1       True          8\n11       ExtraTreesEntr   0.798883       0.025636  0.289557                0.025636           0.289557            1       True          9\n12       KNeighborsDist   0.653631       0.013097  0.015496                0.013097           0.015496            1       True          2\n13       KNeighborsUnif   0.653631       0.033604  0.567649                0.033604           0.567649            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.039553\n2.707510\n0.000509\n0.333600\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.007419\n1.666336\n0.007419\n1.666336\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.826816\n0.002828\n0.336571\n0.002828\n0.336571\n1\nTrue\n13\n\n\n3\nLightGBM\n0.826816\n0.003033\n0.210566\n0.003033\n0.210566\n1\nTrue\n4\n\n\n4\nCatBoost\n0.826816\n0.003410\n0.561348\n0.003410\n0.561348\n1\nTrue\n7\n\n\n5\nNeuralNetFastAI\n0.826816\n0.006794\n0.623764\n0.006794\n0.623764\n1\nTrue\n10\n\n\n6\nXGBoost\n0.821229\n0.004909\n0.202243\n0.004909\n0.202243\n1\nTrue\n11\n\n\n7\nRandomForestEntr\n0.821229\n0.022999\n0.271596\n0.022999\n0.271596\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.025080\n0.294840\n0.025080\n0.294840\n1\nTrue\n5\n\n\n9\nLightGBMXT\n0.810056\n0.002956\n0.207451\n0.002956\n0.207451\n1\nTrue\n3\n\n\n10\nExtraTreesGini\n0.804469\n0.022679\n0.268605\n0.022679\n0.268605\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.798883\n0.025636\n0.289557\n0.025636\n0.289557\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.013097\n0.015496\n0.013097\n0.015496\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.033604\n0.567649\n0.033604\n0.567649\n1\nTrue\n1\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#d.-예측-predict",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#d.-예측-predict",
    "title": "[STBDA2023] 02wk-007: 타이타닉, Autogluon(Fsize,Dropout)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(_tr)).mean()\n\n0.9438832772166106\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(_tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize,Drop)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.out.html",
    "href": "posts/02wk-005-타이타닉, Autogluon.out.html",
    "title": "[STBDA2023] 02wk-005: 타이타닉, Autogluon",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.out.html#a.-데이터",
    "href": "posts/02wk-005-타이타닉, Autogluon.out.html#a.-데이터",
    "title": "[STBDA2023] 02wk-005: 타이타닉, Autogluon",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"~/Desktop/titanic/train.csv\")\ntst = TabularDataset(\"~/Desktop/titanic/test.csv\")\n\nLoaded data from: ~/Desktop/titanic/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ~/Desktop/titanic/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n\ntype(tr)\n\nautogluon.core.dataset.TabularDataset"
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.out.html#b.-predictor-생성",
    "href": "posts/02wk-005-타이타닉, Autogluon.out.html#b.-predictor-생성",
    "title": "[STBDA2023] 02wk-005: 타이타닉, Autogluon",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\nTabularDataset??\n\n\nInit signature: TabularDataset(data, **kwargs)\nSource:        \nclass TabularDataset(pd.DataFrame):\n    \"\"\"\n    A dataset in tabular format (with rows = samples, columns = features/variables).\n    This object is essentially a pandas DataFrame (with some extra attributes) and all existing pandas methods can be applied to it.\n    For full list of methods/attributes, see pandas Dataframe documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n    Parameters\n    ----------\n    data : :class:`pd.DataFrame` or str\n        If str, path to data file (CSV or Parquet format).\n        If you already have your data in a :class:`pd.DataFrame`, you can specify it here.\n    Attributes\n    ----------\n    file_path: (str)\n        Path to data file from which this `TabularDataset` was created.\n        None if `data` was a :class:`pd.DataFrame`.\n    Note: In addition to these attributes, `TabularDataset` also shares all the same attributes and methods of a pandas Dataframe.\n    For a detailed list, see:  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n    Examples\n    --------\n    &gt;&gt;&gt; from autogluon.core.dataset import TabularDataset\n    &gt;&gt;&gt; train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n    &gt;&gt;&gt; train_data.head(30)\n    &gt;&gt;&gt; train_data.columns\n    \"\"\"\n    _metadata = [\"file_path\"]  # preserved properties that will be copied to a new instance of TabularDataset\n    @property\n    def _constructor(self):\n        return TabularDataset\n    @property\n    def _constructor_sliced(self):\n        return pd.Series\n    def __init__(self, data, **kwargs):\n        if isinstance(data, str):\n            file_path = data\n            data = load_pd.load(file_path)\n        else:\n            file_path = None\n        super().__init__(data, **kwargs)\n        self.file_path = file_path\nFile:           ~/anaconda3/envs/py38/lib/python3.8/site-packages/autogluon/core/dataset.py\nType:           type\nSubclasses:     \n\n\n\n\nclass상속!! Dataframe의 기능을 다 쓸 수 있다.\n\n\nTabularPredictor?\n\n\nInit signature:\nTabularPredictor(\n    label,\n    problem_type=None,\n    eval_metric=None,\n    path=None,\n    verbosity=2,\n    log_to_file=False,\n    log_file_path='auto',\n    sample_weight=None,\n    weight_evaluation=False,\n    groups=None,\n    **kwargs,\n)\nDocstring:     \nAutoGluon TabularPredictor predicts values in a column of a tabular dataset (classification or regression).\nParameters\n----------\nlabel : str\n    Name of the column that contains the target variable to predict.\nproblem_type : str, default = None\n    Type of prediction problem, i.e. is this a binary/multiclass classification or regression problem (options: 'binary', 'multiclass', 'regression', 'quantile').\n    If `problem_type = None`, the prediction problem type is inferred based on the label-values in provided dataset.\neval_metric : function or str, default = None\n    Metric by which predictions will be ultimately evaluated on test data.\n    AutoGluon tunes factors such as hyperparameters, early-stopping, ensemble-weights, etc. in order to improve this metric on validation data.\n    If `eval_metric = None`, it is automatically chosen based on `problem_type`.\n    Defaults to 'accuracy' for binary and multiclass classification, 'root_mean_squared_error' for regression, and 'pinball_loss' for quantile.\n    Otherwise, options for classification:\n        ['accuracy', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted',\n        'roc_auc', 'roc_auc_ovo_macro', 'average_precision', 'precision', 'precision_macro', 'precision_micro',\n        'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'log_loss', 'pac_score']\n    Options for regression:\n        ['root_mean_squared_error', 'mean_squared_error', 'mean_absolute_error', 'median_absolute_error', 'mean_absolute_percentage_error', 'r2']\n    For more information on these options, see `sklearn.metrics`: https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n    For metric source code, see `autogluon.core.metrics`.\n    You can also pass your own evaluation function here as long as it follows formatting of the functions defined in folder `autogluon.core.metrics`.\n    For detailed instructions on creating and using a custom metric, refer to https://auto.gluon.ai/stable/tutorials/tabular_prediction/tabular-custom-metric.html\npath : Union[str, pathlib.Path], default = None\n    Path to directory where models and intermediate outputs should be saved.\n    If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n    Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n    Otherwise files from first `fit()` will be overwritten by second `fit()`.\nverbosity : int, default = 2\n    Verbosity levels range from 0 to 4 and control how much information is printed.\n    Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n    If using logging, you can alternatively control amount of information printed via `logger.setLevel(L)`,\n    where `L` ranges from 0 to 50 (Note: higher values of `L` correspond to fewer print statements, opposite of verbosity levels).\n    Verbosity levels:\n        0: Only log exceptions\n        1: Only log warnings + exceptions\n        2: Standard logging\n        3: Verbose logging (ex: log validation score every 50 iterations)\n        4: Maximally verbose logging (ex: log validation score every iteration)\nlog_to_file: bool, default = True\n    Whether to save the logs into a file for later reference\nlog_file_path: str, default = \"auto\"\n    File path to save the logs.\n    If auto, logs will be saved under `predictor_path/logs/predictor_log.txt`.\n    Will be ignored if `log_to_file` is set to False\nsample_weight : str, default = None\n    If specified, this column-name indicates which column of the data should be treated as sample weights. This column will NOT be considered as a predictive feature.\n    Sample weights should be non-negative (and cannot be nan), with larger values indicating which rows are more important than others.\n    If you want your usage of sample weights to match results obtained outside of this Predictor, then ensure sample weights for your training (or tuning) data sum to the number of rows in the training (or tuning) data.\n    You may also specify two special strings: 'auto_weight' (automatically choose a weighting strategy based on the data) or 'balance_weight' (equally weight classes in classification, no effect in regression). If specifying your own sample_weight column, make sure its name does not match these special strings.\nweight_evaluation : bool, default = False\n    Only considered when `sample_weight` column is not None. Determines whether sample weights should be taken into account when computing evaluation metrics on validation/test data.\n    If True, then weighted metrics will be reported based on the sample weights provided in the specified `sample_weight` (in which case `sample_weight` column must also be present in test data).\n    In this case, the 'best' model used by default for prediction will also be decided based on a weighted version of evaluation metric.\n    Note: we do not recommend specifying `weight_evaluation` when `sample_weight` is 'auto_weight' or 'balance_weight', instead specify appropriate `eval_metric`.\ngroups : str, default = None\n    [Experimental] If specified, AutoGluon will use the column named the value of groups in `train_data` during `.fit` as the data splitting indices for the purposes of bagging.\n    This column will not be used as a feature during model training.\n    This parameter is ignored if bagging is not enabled. To instead specify a custom validation set with bagging disabled, specify `tuning_data` in `.fit`.\n    The data will be split via `sklearn.model_selection.LeaveOneGroupOut`.\n    Use this option to control the exact split indices AutoGluon uses.\n    It is not recommended to use this option unless it is required for very specific situations.\n    Bugs may arise from edge cases if the provided groups are not valid to properly train models, such as if not all classes are present during training in multiclass classification. It is up to the user to sanitize their groups.\n    As an example, if you want your data folds to preserve adjacent rows in the table without shuffling, then for 3 fold bagging with 6 rows of data, the groups column values should be [0, 0, 1, 1, 2, 2].\n**kwargs :\n    learner_type : AbstractLearner, default = DefaultLearner\n        A class which inherits from `AbstractLearner`. This dictates the inner logic of predictor.\n        If you don't know what this is, keep it as the default.\n    learner_kwargs : dict, default = None\n        Kwargs to send to the learner. Options include:\n        positive_class : str or int, default = None\n            Used to determine the positive class in binary classification.\n            This is used for certain metrics such as 'f1' which produce different scores depending on which class is considered the positive class.\n            If not set, will be inferred as the second element of the existing unique classes after sorting them.\n                If classes are [0, 1], then 1 will be selected as the positive class.\n                If classes are ['def', 'abc'], then 'def' will be selected as the positive class.\n                If classes are [True, False], then True will be selected as the positive class.\n        ignored_columns : list, default = None\n            Banned subset of column names that predictor may not use as predictive features (e.g. unique identifier to a row or user-ID).\n            These columns are ignored during `fit()`.\n        label_count_threshold : int, default = 10\n            For multi-class classification problems, this is the minimum number of times a label must appear in dataset in order to be considered an output class.\n            AutoGluon will ignore any classes whose labels do not appear at least this many times in the dataset (i.e. will never predict them).\n        cache_data : bool, default = True\n            When enabled, the training and validation data are saved to disk for future reuse.\n            Enables advanced functionality in predictor such as `fit_extra()` and feature importance calculation on the original data.\n        trainer_type : AbstractTrainer, default = AutoTrainer\n            A class inheriting from `AbstractTrainer` that controls training/ensembling of many models.\n            If you don't know what this is, keep it as the default.\nAttributes\n----------\npath : str\n    Path to directory where all models used by this Predictor are stored.\nproblem_type : str\n    What type of prediction problem this Predictor has been trained for.\neval_metric : function or str\n    What metric is used to evaluate predictive performance.\nlabel : str\n    Name of table column that contains data from the variable to predict (often referred to as: labels, response variable, target variable, dependent variable, Y, etc).\nfeature_metadata : :class:`autogluon.common.features.feature_metadata.FeatureMetadata`\n    Inferred data type of each predictive variable after preprocessing transformation (i.e. column of training data table used to predict `label`).\n    Contains both raw dtype and special dtype information. Each feature has exactly 1 raw dtype (such as 'int', 'float', 'category') and zero to many special dtypes (such as 'datetime_as_int', 'text', 'text_ngram').\n    Special dtypes are AutoGluon specific feature types that are used to identify features with meaning beyond what the raw dtype can convey.\n        `feature_metadata.type_map_raw`: Dictionary of feature name -&gt; raw dtype mappings.\n        `feature_metadata.type_group_map_special`: Dictionary of lists of special feature names, grouped by special feature dtype.\npositive_class : str or int\n    Returns the positive class name in binary classification. Useful for computing metrics such as F1 which require a positive and negative class.\n    In binary classification, :meth:`TabularPredictor.predict_proba` returns the estimated probability that each row belongs to the positive class.\n    Will print a warning and return None if called when `predictor.problem_type != 'binary'`.\nclass_labels : list\n    For multiclass problems, this list contains the class labels in sorted order of `predict_proba()` output.\n    For binary problems, this list contains the class labels in sorted order of `predict_proba(as_multiclass=True)` output.\n        `class_labels[0]` corresponds to internal label = 0 (negative class), `class_labels[1]` corresponds to internal label = 1 (positive class).\n        This is relevant for certain metrics such as F1 where True and False labels impact the metric score differently.\n    For other problem types, will equal None.\n    For example if `pred = predict_proba(x, as_multiclass=True)`, then ith index of `pred` provides predicted probability that `x` belongs to class given by `class_labels[i]`.\nclass_labels_internal : list\n    For multiclass problems, this list contains the internal class labels in sorted order of internal `predict_proba()` output.\n    For binary problems, this list contains the internal class labels in sorted order of internal `predict_proba(as_multiclass=True)` output.\n        The value will always be `class_labels_internal=[0, 1]` for binary problems, with 0 as the negative class, and 1 as the positive class.\n    For other problem types, will equal None.\nclass_labels_internal_map : dict\n    For binary and multiclass classification problems, this dictionary contains the mapping of the original labels to the internal labels.\n    For example, in binary classification, label values of 'True' and 'False' will be mapped to the internal representation `1` and `0`.\n        Therefore, class_labels_internal_map would equal {'True': 1, 'False': 0}\n    For other problem types, will equal None.\n    For multiclass, it is possible for not all of the label values to have a mapping.\n        This indicates that the internal models will never predict those missing labels, and training rows associated with the missing labels were dropped.\nFile:           ~/anaconda3/envs/py38/lib/python3.8/site-packages/autogluon/tabular/predictor/predictor.py\nType:           type\nSubclasses:     _TabularPredictorExperimental, InterpretableTabularPredictor\n\n\n\n\n“label”:target variable\n\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230917_135346/\""
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.out.html#c.-적합fit",
    "href": "posts/02wk-005-타이타닉, Autogluon.out.html#c.-적합fit",
    "title": "[STBDA2023] 02wk-005: 타이타닉, Autogluon",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr) \n# 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230917_135346/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   775.59 GB / 982.82 GB (78.9%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    39350.68 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.16s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f291690c3a0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    0.6536   = Validation score   (accuracy)\n    0.03s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f291690c3a0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    0.6536   = Validation score   (accuracy)\n    0.02s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.21s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.36s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8324   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    0.12s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8212   = Validation score   (accuracy)\n    1.19s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.41s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8324   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 4.87s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230917_135346/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f2899b57f70&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge   0.832402       0.002893  0.406132                0.002893           0.406132            1       True         13\n1       NeuralNetFastAI   0.832402       0.006791  0.452260                0.006791           0.452260            1       True         10\n2   WeightedEnsemble_L2   0.832402       0.007321  0.783443                0.000530           0.331183            2       True         14\n3              CatBoost   0.826816       0.003689  0.362406                0.003689           0.362406            1       True          7\n4              LightGBM   0.821229       0.003210  0.207445                0.003210           0.207445            1       True          4\n5        NeuralNetTorch   0.821229       0.007692  1.186131                0.007692           1.186131            1       True         12\n6            LightGBMXT   0.815642       0.003147  0.195812                0.003147           0.195812            1       True          3\n7      RandomForestEntr   0.815642       0.024688  0.289305                0.024688           0.289305            1       True          6\n8      RandomForestGini   0.815642       0.024689  0.285907                0.024689           0.285907            1       True          5\n9        ExtraTreesGini   0.815642       0.024699  0.280003                0.024699           0.280003            1       True          8\n10              XGBoost   0.810056       0.004088  0.119255                0.004088           0.119255            1       True         11\n11       ExtraTreesEntr   0.810056       0.024130  0.290494                0.024130           0.290494            1       True          9\n12       KNeighborsUnif   0.653631       0.006800  0.025122                0.006800           0.025122            1       True          1\n13       KNeighborsDist   0.653631       0.009379  0.023463                0.009379           0.023463            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMLarge\n0.832402\n0.002893\n0.406132\n0.002893\n0.406132\n1\nTrue\n13\n\n\n1\nNeuralNetFastAI\n0.832402\n0.006791\n0.452260\n0.006791\n0.452260\n1\nTrue\n10\n\n\n2\nWeightedEnsemble_L2\n0.832402\n0.007321\n0.783443\n0.000530\n0.331183\n2\nTrue\n14\n\n\n3\nCatBoost\n0.826816\n0.003689\n0.362406\n0.003689\n0.362406\n1\nTrue\n7\n\n\n4\nLightGBM\n0.821229\n0.003210\n0.207445\n0.003210\n0.207445\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.821229\n0.007692\n1.186131\n0.007692\n1.186131\n1\nTrue\n12\n\n\n6\nLightGBMXT\n0.815642\n0.003147\n0.195812\n0.003147\n0.195812\n1\nTrue\n3\n\n\n7\nRandomForestEntr\n0.815642\n0.024688\n0.289305\n0.024688\n0.289305\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.024689\n0.285907\n0.024689\n0.285907\n1\nTrue\n5\n\n\n9\nExtraTreesGini\n0.815642\n0.024699\n0.280003\n0.024699\n0.280003\n1\nTrue\n8\n\n\n10\nXGBoost\n0.810056\n0.004088\n0.119255\n0.004088\n0.119255\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.024130\n0.290494\n0.024130\n0.290494\n1\nTrue\n9\n\n\n12\nKNeighborsUnif\n0.653631\n0.006800\n0.025122\n0.006800\n0.025122\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.653631\n0.009379\n0.023463\n0.009379\n0.023463\n1\nTrue\n2"
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.out.html#d.-예측-predict",
    "href": "posts/02wk-005-타이타닉, Autogluon.out.html#d.-예측-predict",
    "title": "[STBDA2023] 02wk-005: 타이타닉, Autogluon",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.8810325476992144\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/14wk-60.out.html",
    "href": "posts/14wk-60.out.html",
    "title": "[STBDA2023] 14wk-60: 자전거대여 / 하이퍼파라메터 튜닝",
    "section": "",
    "text": "14wk-60: 자전거대여 / 하이퍼파라메터 튜닝\n최규빈\n2023-12-01\n\n\n1. 강의영상\n???\n\n\n2. Imports\n\n#!pip install autogluon.multimodal \n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing\n#---#}\nfrom autogluon.tabular import TabularPredictor\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\nfrom autogluon.common import space\n#---#\nimport IPython\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n2023-12-10 17:19:01.469099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-12-10 17:19:02.107172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\n\n3. Data\n- 자료 다운로드\n\n!kaggle competitions download -c bike-sharing-demand\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\nDownloading bike-sharing-demand.zip to /home/coco/Dropbox/Class/STBDA23/posts\n100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 822kB/s]\n100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 821kB/s]\n\n\n\n!unzip bike-sharing-demand.zip -d data\n\nArchive:  bike-sharing-demand.zip\n  inflating: data/sampleSubmission.csv  \n  inflating: data/test.csv           \n  inflating: data/train.csv          \n\n\n\nsampleSubmission = pd.read_csv('data/sampleSubmission.csv')\ndf_train = pd.read_csv('data/train.csv')\ndf_test = pd.read_csv('data/test.csv') \n\n\n!rm -rf data\n!rm bike-sharing-demand.zip\n\n\n\n4. 기본전처리 및 분석 프로세스\n- 전처리\n\ndef preprocessing(df_train,df_test):\n    df_train_featured = df_train.copy()\n    df_test_featured = df_test.copy()\n    #----# \n    df_train_featured = df_train_featured.drop(['casual','registered'],axis=1)\n    #--#\n    df_train_featured['hour'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.hour\n    df_test_featured['hour'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.hour\n    df_train_featured['weekday'] = df_train_featured['datetime'].apply(pd.to_datetime).dt.weekday\n    df_test_featured['weekday'] = df_test_featured['datetime'].apply(pd.to_datetime).dt.weekday\n    #--#\n    df_train_featured = df_train_featured.drop(['datetime'],axis=1)\n    df_test_featured = df_test_featured.drop(['datetime'],axis=1)\n    #--#\n    df_train_featured = df_train_featured.drop(['atemp'],axis=1)\n    df_test_featured = df_test_featured.drop(['atemp'],axis=1)\n    return df_train_featured, df_test_featured\n\n- 함수들\n\ndef plot(yhat,yyhat):\n    df = pd.concat([\n        df_train.assign(count_hat = yhat, dataset_type = 'train'),\n        df_test.assign(count_hat = yyhat, dataset_type = 'test')\n    ])\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    sns.lineplot(\n        df.sort_values('datetime')[:(24*28)],\n        x='datetime',y='count',\n        hue='dataset_type',\n        linestyle='--',\n        lw=0.8\n    )\n    sns.lineplot(\n        df.sort_values('datetime')[:(24*28)],\n        x='datetime',y='count_hat',\n        hue='dataset_type',\n        alpha=0.5,\n        lw=3\n    )\n    fig = plt.gcf()\n    fig.set_size_inches(8,2)\n    plt.xticks(rotation=15); \n    fig.show()\n\n\ndef submit(yyhat):\n    sampleSubmission['count'] = yyhat \n    sampleSubmission['count'] = sampleSubmission['count'].apply(lambda x: x if x&gt;0 else 0)\n    sampleSubmission.to_csv(\"submission.csv\",index=False)\n    !kaggle competitions submit -c bike-sharing-demand -f submission.csv -m \"Message\"\n    !rm submission.csv\n\n\ndef auto(df_train, df_test):\n    # step1 \n    df_train_featured, df_test_featured = preprocessing(df_train, df_test) # preprocessing\n    df_train_featured['count'] = np.log1p(df_train_featured['count']) # transform \n    # step2~4 \n    yhat,yyhat = fit_predict(df_train_featured,df_test_featured)\n    yhat = np.expm1(yhat) # inverse_trans\n    yyhat = np.expm1(yyhat) # inverse_trans\n    # 시각화 \n    plot(yhat,yyhat)\n    # 제출 \n    submit(yyhat)\n\n\n\n5. 하이퍼파라메터 튜닝\n- 기본 HP\n{\n    \"NN_TORCH\": {},\n    \"GBM\": [\n        {\"extra_trees\": True, \"ag_args\": {\"name_suffix\": \"XT\"}},\n        {},\n        \"GBMLarge\"\n    ],\n    \"CAT\": {},\n    \"XGB\": {},\n    \"FASTAI\": {},\n    \"RF\": [\n        {\"criterion\": \"gini\", \"ag_args\": {\"name_suffix\": \"Gini\", \"problem_types\": [\"binary\", \"multiclass\"]}},\n        {\"criterion\": \"entropy\", \"ag_args\": {\"name_suffix\": \"Entr\", \"problem_types\": [\"binary\", \"multiclass\"]}},\n        {\"criterion\": \"squared_error\", \"ag_args\": {\"name_suffix\": \"MSE\", \"problem_types\": [\"regression\"]}}\n    ],\n    \"XT\": [\n        {\"criterion\": \"gini\", \"ag_args\": {\"name_suffix\": \"Gini\", \"problem_types\": [\"binary\", \"multiclass\"]}},\n        {\"criterion\": \"entropy\", \"ag_args\": {\"name_suffix\": \"Entr\", \"problem_types\": [\"binary\", \"multiclass\"]}},\n        {\"criterion\": \"squared_error\", \"ag_args\": {\"name_suffix\": \"MSE\", \"problem_types\": [\"regression\"]}}\n    ],\n    \"KNN\": [\n        {\"weights\": \"uniform\", \"ag_args\": {\"name_suffix\": \"Unif\"}},\n        {\"weights\": \"distance\", \"ag_args\": {\"name_suffix\": \"Dist\"}}\n    ]\n}\n- fit_predict 함수 수정\n\ndef fit_predict(df_train_featured, df_test_featured):\n    # step1 \n    # step2\n    predictr= TabularPredictor(label='count',verbosity=False)\n    # step3 \n    hp = {\n        \"RF\": [\n            {\"criterion\": \"squared_error\", \"ag_args\": {\"name_suffix\": \"MSE\", \"problem_types\": [\"regression\"]}}\n        ]\n    }\n    predictr.fit(\n        df_train_featured,\n        hyperparameters = hp\n    )\n    # step4 \n    yhat = predictr.predict(df_train_featured)\n    yyhat = predictr.predict(df_test_featured)\n    # display\n    display(predictr.leaderboard())\n    return yhat, yyhat \n\n\nauto(df_train,df_test)\n\n                 model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0      RandomForestMSE  -0.401983       0.032302  1.019892                0.032302           1.019892            1       True          1\n1  WeightedEnsemble_L2  -0.401983       0.032602  1.022945                0.000299           0.003053            2       True          2\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 243k/243k [00:02&lt;00:00, 102kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nRandomForestMSE\n-0.401983\n0.032302\n1.019892\n0.032302\n1.019892\n1\nTrue\n1\n\n\n1\nWeightedEnsemble_L2\n-0.401983\n0.032602\n1.022945\n0.000299\n0.003053\n2\nTrue\n2\n\n\n\n\n\n\n\n\n\n\nref: https://auto.gluon.ai/0.8.1/api/autogluon.tabular.models.html\n\nLightGBM model: https://lightgbm.readthedocs.io/en/latest/\nCatBoost model: https://catboost.ai/\nXGBoost model: https://xgboost.readthedocs.io/en/latest/\nRandom Forest model (scikit-learn): https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\nExtra Trees model (scikit-learn): https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\nLinear model (scikit-learn): https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n\n- 방금 돌린것은 아래와 결과가 동일함.\n\ndef fit_predict(df_train_featured, df_test_featured):\n    # step1 \n    # step2\n    predictr= TabularPredictor(label='count',verbosity=False)\n    # step3 \n    hp = {\n        \"RF\": [\n            {\"n_estimators\":300, \"criterion\": \"squared_error\", \"ag_args\": {\"name_suffix\": \"MSE\", \"problem_types\": [\"regression\"]}}\n        ]\n    }\n    predictr.fit(\n        df_train_featured,\n        hyperparameters = hp\n    )\n    # step4 \n    yhat = predictr.predict(df_train_featured)\n    yyhat = predictr.predict(df_test_featured)\n    # display\n    display(predictr.leaderboard())\n    return yhat, yyhat \n\n\nauto(df_train,df_test)\n\n                 model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0      RandomForestMSE  -0.401983       0.031664  0.914731                0.031664           0.914731            1       True          1\n1  WeightedEnsemble_L2  -0.401983       0.031927  0.917581                0.000263           0.002850            2       True          2\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 243k/243k [00:02&lt;00:00, 123kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nRandomForestMSE\n-0.401983\n0.031664\n0.914731\n0.031664\n0.914731\n1\nTrue\n1\n\n\n1\nWeightedEnsemble_L2\n-0.401983\n0.031927\n0.917581\n0.000263\n0.002850\n2\nTrue\n2\n\n\n\n\n\n\n\n\n\n\n- 알아낸 방법?\n\ndf_train_featured, df_test_featured = preprocessing(df_train,df_test)\n\n\npredictr= TabularPredictor(label='count',verbosity=False)\n# step3 \nhp = {\n    \"RF\": [\n        {\"criterion\": \"squared_error\", \"ag_args\": {\"name_suffix\": \"MSE\", \"problem_types\": [\"regression\"]}}\n    ]\n}\npredictr.fit(\n    df_train_featured,\n    hyperparameters = hp\n)\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f33b3eedbe0&gt;\n\n\n\npredictr.info()['model_info']['RandomForestMSE']['hyperparameters']\n\n{'n_estimators': 300,\n 'max_leaf_nodes': 15000,\n 'n_jobs': -1,\n 'random_state': 0,\n 'bootstrap': True,\n 'criterion': 'squared_error'}\n\n\n- RF에서 더 다양한 파라메터를 실험해보자.\n\ndef fit_predict(df_train_featured, df_test_featured):\n    # step1 \n    # step2\n    predictr= TabularPredictor(label='count',verbosity=False)\n    # step3 \n    hp = {\n        \"RF\": [ {\"criterion\": \"squared_error\", \"n_estimators\":i, \"max_leaf_nodes\":j, \"ag_args\": {\"name_suffix\": f\"({i},{j})\"}} for i in [300,400,500] for j in [10000,15000]]\n    }\n    predictr.fit(\n        df_train_featured,\n        hyperparameters = hp\n    )\n    # step4 \n    yhat = predictr.predict(df_train_featured)\n    yyhat = predictr.predict(df_test_featured)\n    # display\n    display(predictr.leaderboard())\n    return yhat, yyhat \n\n\nauto(df_train,df_test)\n\n                     model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0  RandomForest(500,10000)  -0.401733       0.058357  2.050068                0.058357           2.050068            1       True          5\n1      WeightedEnsemble_L2  -0.401733       0.058645  2.149383                0.000288           0.099315            2       True          7\n2  RandomForest(500,15000)  -0.401733       0.061309  2.016577                0.061309           2.016577            1       True          6\n3  RandomForest(300,10000)  -0.401983       0.031878  1.039235                0.031878           1.039235            1       True          1\n4  RandomForest(300,15000)  -0.401983       0.032004  1.069615                0.032004           1.069615            1       True          2\n5  RandomForest(400,15000)  -0.402192       0.040670  1.483031                0.040670           1.483031            1       True          4\n6  RandomForest(400,10000)  -0.402192       0.041378  1.327342                0.041378           1.327342            1       True          3\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/coco/.kaggle/kaggle.json'\n100%|█████████████████████████████████████████| 242k/242k [00:01&lt;00:00, 152kB/s]\nSuccessfully submitted to Bike Sharing Demand\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nRandomForest(500,10000)\n-0.401733\n0.058357\n2.050068\n0.058357\n2.050068\n1\nTrue\n5\n\n\n1\nWeightedEnsemble_L2\n-0.401733\n0.058645\n2.149383\n0.000288\n0.099315\n2\nTrue\n7\n\n\n2\nRandomForest(500,15000)\n-0.401733\n0.061309\n2.016577\n0.061309\n2.016577\n1\nTrue\n6\n\n\n3\nRandomForest(300,10000)\n-0.401983\n0.031878\n1.039235\n0.031878\n1.039235\n1\nTrue\n1\n\n\n4\nRandomForest(300,15000)\n-0.401983\n0.032004\n1.069615\n0.032004\n1.069615\n1\nTrue\n2\n\n\n5\nRandomForest(400,15000)\n-0.402192\n0.040670\n1.483031\n0.040670\n1.483031\n1\nTrue\n4\n\n\n6\nRandomForest(400,10000)\n-0.402192\n0.041378\n1.327342\n0.041378\n1.327342\n1\nTrue\n3"
  },
  {
    "objectID": "posts/07wk-031.out.html",
    "href": "posts/07wk-031.out.html",
    "title": "[STBDA2023] 07wk-031: 체중감량(교호작용) / 의사결정나무",
    "section": "",
    "text": "07wk-031: 체중감량(교호작용) / 의사결정나무\n최규빈\n2023-10-17\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-xhSSJ1GlUjFhUgzvVa3aIH&si=JA3pd69Mv9QGXx0z\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport sklearn.linear_model\nimport sklearn.tree\n\n\n\n3. Data\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n- 운동과 체중감량보조제를 병행하면 시너지가 나는 것 같음\n\n\n4. 분석\n- 분석1: 선형회귀 (교호작용 고려 X)\n\n# step 1\nX,y = df_train[['Supplement','Exercise']], df_train['Weight_Loss']\n# step 2 \npredictr = sklearn.linear_model.LinearRegression()\n# step 3 \npredictr.fit(X,y)\n# step 4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n- 분석2: 의사결정나무\n\n# step 1\nX,y = df_train[['Supplement','Exercise']], df_train['Weight_Loss']\n# step 2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step 3 \npredictr.fit(X,y)\n# step 4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n\n\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nWeight_Loss_hat\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n0.021673\n\n\n1\nTrue\nFalse\n1.604542\n0.497573\n\n\n2\nTrue\nTrue\n13.824148\n14.966363\n\n\n3\nTrue\nTrue\n13.004505\n14.966363\n\n\n4\nTrue\nTrue\n13.701128\n14.966363\n\n\n...\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n0.497573\n\n\n9996\nFalse\nFalse\n-0.217816\n0.021673\n\n\n9997\nFalse\nTrue\n4.072701\n4.991314\n\n\n9998\nTrue\nFalse\n-0.253796\n0.497573\n\n\n9999\nFalse\nFalse\n-1.399092\n0.021673\n\n\n\n\n10000 rows × 4 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363"
  },
  {
    "objectID": "posts/03wk-011.out.html",
    "href": "posts/03wk-011.out.html",
    "title": "[STBDA2023] 03wk-011: Medical Cost, 회귀분석",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/03wk-011.out.html#a.-data-정리",
    "href": "posts/03wk-011.out.html#a.-data-정리",
    "title": "[STBDA2023] 03wk-011: Medical Cost, 회귀분석",
    "section": "A. Data 정리",
    "text": "A. Data 정리\n\ndf.columns\n\nIndex(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'], dtype='object')\n\n\n\nX = pd.get_dummies(df.drop(['charges'],axis=1))\ny = df[['charges']]\n\n\nX\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\n1\n0\n0\n1\n0\n0\n0\n1\n\n\n1\n18\n33.770\n1\n0\n1\n1\n0\n0\n0\n1\n0\n\n\n2\n28\n33.000\n3\n0\n1\n1\n0\n0\n0\n1\n0\n\n\n3\n33\n22.705\n0\n0\n1\n1\n0\n0\n1\n0\n0\n\n\n4\n32\n28.880\n0\n0\n1\n1\n0\n0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n3\n0\n1\n1\n0\n0\n1\n0\n0\n\n\n1334\n18\n31.920\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n1335\n18\n36.850\n0\n1\n0\n1\n0\n0\n0\n1\n0\n\n\n1336\n21\n25.800\n0\n1\n0\n1\n0\n0\n0\n0\n1\n\n\n1337\n61\n29.070\n0\n1\n0\n0\n1\n0\n1\n0\n0\n\n\n\n\n1338 rows × 11 columns\n\n\n\n\ny\n\n\n\n\n\n\n\n\ncharges\n\n\n\n\n0\n16884.92400\n\n\n1\n1725.55230\n\n\n2\n4449.46200\n\n\n3\n21984.47061\n\n\n4\n3866.85520\n\n\n...\n...\n\n\n1333\n10600.54830\n\n\n1334\n2205.98080\n\n\n1335\n1629.83350\n\n\n1336\n2007.94500\n\n\n1337\n29141.36030\n\n\n\n\n1338 rows × 1 columns"
  },
  {
    "objectID": "posts/03wk-011.out.html#b.-predictor-생성",
    "href": "posts/03wk-011.out.html#b.-predictor-생성",
    "title": "[STBDA2023] 03wk-011: Medical Cost, 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()"
  },
  {
    "objectID": "posts/03wk-011.out.html#c.-학습",
    "href": "posts/03wk-011.out.html#c.-학습",
    "title": "[STBDA2023] 03wk-011: Medical Cost, 회귀분석",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-011.out.html#d.-예측",
    "href": "posts/03wk-011.out.html#d.-예측",
    "title": "[STBDA2023] 03wk-011: Medical Cost, 회귀분석",
    "section": "D. 예측",
    "text": "D. 예측\n\ndf.assign(yhat = predictr.predict(X))\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\nyhat\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n25293.713028\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n3448.602834\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n6706.988491\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n3754.830163\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n5592.493386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n12351.323686\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n3511.930809\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n4149.132486\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n1246.584939\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n37085.623268\n\n\n\n\n1338 rows × 8 columns"
  },
  {
    "objectID": "posts/03wk-011.out.html#e.-평가",
    "href": "posts/03wk-011.out.html#e.-평가",
    "title": "[STBDA2023] 03wk-011: Medical Cost, 회귀분석",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y) # R^2\n\n0.7509130345985207\n\n\n\n0.7 이상이면 망한모형까지는 아님 (대회용으로는 부적절할 수 있으나 대충 쓸 수는 있는 정도)"
  },
  {
    "objectID": "posts/03wk-009.out.html",
    "href": "posts/03wk-009.out.html",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/03wk-009.out.html#a.-질문",
    "href": "posts/03wk-009.out.html#a.-질문",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "A. 질문",
    "text": "A. 질문\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?"
  },
  {
    "objectID": "posts/03wk-009.out.html#b.-답1",
    "href": "posts/03wk-009.out.html#b.-답1",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "B. 답1",
    "text": "B. 답1\n- \\(x=-2.0\\) 근처의 데이터를 살펴보자.\n\ndf[(-3.0 &lt; df.temp) & (df.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n대충 17.67 근처이지 않을까?.."
  },
  {
    "objectID": "posts/03wk-009.out.html#c.-답2",
    "href": "posts/03wk-009.out.html#c.-답2",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "C. 답2",
    "text": "C. 답2\n- 자료를 바탕으로 그림을 그려보자.\n\nplt.plot(df.temp,df.sales,'o')\nplt.plot([-2.0],[17.67],'x')\n\n\n\n\n\n저거 보다 못팔 것 같은데?"
  },
  {
    "objectID": "posts/03wk-009.out.html#d.-아이디어",
    "href": "posts/03wk-009.out.html#d.-아이디어",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "D. 아이디어",
    "text": "D. 아이디어\n- 선을 기가 막히게 그어서 추세선을 만들고, 그 추세선 위의 점으로 예측하자.\n- 속마음: 사실 추세선을 알고 있긴함\n\nplt.plot(df.temp,df.sales,'o')\nplt.plot(df.temp,20+df.temp*2.5,'--')\n\n\n\n\n- 사실 \\(y=20+2.5x\\) 라는 추세선을 그으면 된다는 것을 알고 있다.\n- 그래서 \\(x=-2\\) 이라면 \\(y=20-2.5\\times 2=15\\) 라고 보는게 합리적임. (물론 오차가 있을 수 있지만 그건 운이므로 어쩔수 없는것임, 랜덤으로 뭐가 나올지까지 맞출 수는 없음[1])\n- 그렇지만 우리는 사실 \\(20, 2.5\\) 라는 숫자를 모른다. (이 숫자만 안다면 임의의 \\(x\\)에 대한 \\(y\\)값을 알 수 있을 텐데…)\n- 게임셋팅\n\n원래게임: 임의의 \\(x\\)에 대하여 합리적인 \\(y\\)를 잘 찾는 게임\n변형된게임: \\(20,2.5\\) 라는 숫자를 잘 찾는 게임, 즉 데이터를 보고 최대한 \\(y_i \\approx ax_i+b\\) 이 되도록 \\(a,b\\)를 잘 선택하는 게임"
  },
  {
    "objectID": "posts/03wk-009.out.html#a.-데이터",
    "href": "posts/03wk-009.out.html#a.-데이터",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 변수 설정\n[1] 만약 그렇다면 랜덤이 아니겠지?\n\nX = df[['temp']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nplt.plot(X,y,'o')\n\n\n\n\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?\n\ndf[(-3.0 &lt; df.temp) & (df.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n- 답1: 대충 17.67 근처이지 않을까?..\n- 답2: 17.67 보다 작지 않을까?\n- 아이디어: 추세선을 그리고 거기서 예측해보면 어떨까?\n- 데이터를 학습하여 추세선을 적절히 그릴 수 있고, 그려진 추세선으로 예측까지 해줄수 있는 아이(predictor)를 만들자."
  },
  {
    "objectID": "posts/03wk-009.out.html#b.-predictor-생성",
    "href": "posts/03wk-009.out.html#b.-predictor-생성",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlinear regression 이라는 방법으로 추세선을 만들고 예측하는 아이(predictor)를 만드는 코드"
  },
  {
    "objectID": "posts/03wk-009.out.html#c.-학습-fit-learn",
    "href": "posts/03wk-009.out.html#c.-학습-fit-learn",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-009.out.html#d.-예측-predict",
    "href": "posts/03wk-009.out.html#d.-예측-predict",
    "title": "[STBDA2023] 03wk-009: 아이스크림, 회귀분석",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- Predictor: 데이터를 살펴보니 원래 true는 이럴것 같아요\n\nyhat = predictr.predict(X)\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat, 'o--',alpha=0.5)\n\n\n\n\n- 최규빈: 저런 추세선을 그었다면, \\(y=ax+b\\) 꼴의 식에서 \\(a\\), \\(b\\)를 적당한 값으로 찾았다는 의미인데, 그 값은 어디있지?\n- Predictor: 아래에 있어요\n\na = predictr.coef_,\nb = predictr.intercept_\n\n\na,b\n\n((array([[2.51561216]]),), array([19.66713127]))\n\n\n- 최규빈: 확인해보자..\n\n(df.temp * 2.51561216 + 19.66713127)[:5], yhat[:5]\n\n(0     9.353121\n 1    10.359366\n 2    12.120295\n 3    16.396835\n 4    18.409325\n Name: temp, dtype: float64,\n array([[ 9.35312141],\n        [10.35936628],\n        [12.12029479],\n        [16.39683546],\n        [18.40932519]]))\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (1) – 수식위주로\n\n2.51561216*(-2) + 19.66713127\n\n14.635906949999999\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot([-2],[14.635906949999999],'xr')\n\n\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (2) – 코드위주로 (\\(\\star\\))\n\nXnew = pd.DataFrame({'temp':[-2.0]})\n\n\npredictr.predict(Xnew)\n\narray([[14.63590695]])\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot(Xnew, predictr.predict(Xnew),'xr')"
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html",
    "title": "[STBDA2023] 02wk-006: 타이타닉, Autogluon(Fsize)",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html#a.-데이터",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html#a.-데이터",
    "title": "[STBDA2023] 02wk-006: 타이타닉, Autogluon(Fsize)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"~/Desktop/titanic/train.csv\")\ntst = TabularDataset(\"~/Desktop/titanic/test.csv\")\n\n- 피처엔지니어링\n\ntr.eval('Fsize = SibSp + Parch')\ntst.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html#b.-predictor-생성",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html#b.-predictor-생성",
    "title": "[STBDA2023] 02wk-006: 타이타닉, Autogluon(Fsize)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230917_140239/\""
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html#c.-적합fit",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html#c.-적합fit",
    "title": "[STBDA2023] 02wk-006: 타이타닉, Autogluon(Fsize)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr.eval('Fsize = SibSp + Parch'))\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230917_140239/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   775.56 GB / 982.82 GB (78.9%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    39207.34 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.17s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd2d4238280&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    0.648    = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd2d14bbc10&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n    0.648    = Validation score   (accuracy)\n    0.02s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.19s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    0.63s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    1.38s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.36s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8547   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 5.82s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230917_140239/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fd2a6761d00&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.854749       0.047879  2.622710                0.000539           0.327875            2       True         14\n1         LightGBMLarge   0.832402       0.002897  0.362443                0.002897           0.362443            1       True         13\n2       NeuralNetFastAI   0.832402       0.007049  0.626138                0.007049           0.626138            1       True         10\n3        NeuralNetTorch   0.832402       0.008281  1.377705                0.008281           1.377705            1       True         12\n4              CatBoost   0.826816       0.003585  0.415325                0.003585           0.415325            1       True          7\n5               XGBoost   0.826816       0.004826  0.200229                0.004826           0.200229            1       True         11\n6              LightGBM   0.821229       0.002808  0.190213                0.002808           0.190213            1       True          4\n7      RandomForestEntr   0.821229       0.023621  0.274035                0.023621           0.274035            1       True          6\n8            LightGBMXT   0.815642       0.002782  0.195089                0.002782           0.195089            1       True          3\n9      RandomForestGini   0.815642       0.024148  0.285713                0.024148           0.285713            1       True          5\n10       ExtraTreesEntr   0.810056       0.024155  0.271298                0.024155           0.271298            1       True          9\n11       ExtraTreesGini   0.804469       0.024840  0.271464                0.024840           0.271464            1       True          8\n12       KNeighborsDist   0.648045       0.008390  0.016957                0.008390           0.016957            1       True          2\n13       KNeighborsUnif   0.648045       0.030974  0.549377                0.030974           0.549377            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.854749\n0.047879\n2.622710\n0.000539\n0.327875\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.832402\n0.002897\n0.362443\n0.002897\n0.362443\n1\nTrue\n13\n\n\n2\nNeuralNetFastAI\n0.832402\n0.007049\n0.626138\n0.007049\n0.626138\n1\nTrue\n10\n\n\n3\nNeuralNetTorch\n0.832402\n0.008281\n1.377705\n0.008281\n1.377705\n1\nTrue\n12\n\n\n4\nCatBoost\n0.826816\n0.003585\n0.415325\n0.003585\n0.415325\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.004826\n0.200229\n0.004826\n0.200229\n1\nTrue\n11\n\n\n6\nLightGBM\n0.821229\n0.002808\n0.190213\n0.002808\n0.190213\n1\nTrue\n4\n\n\n7\nRandomForestEntr\n0.821229\n0.023621\n0.274035\n0.023621\n0.274035\n1\nTrue\n6\n\n\n8\nLightGBMXT\n0.815642\n0.002782\n0.195089\n0.002782\n0.195089\n1\nTrue\n3\n\n\n9\nRandomForestGini\n0.815642\n0.024148\n0.285713\n0.024148\n0.285713\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.810056\n0.024155\n0.271298\n0.024155\n0.271298\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.804469\n0.024840\n0.271464\n0.024840\n0.271464\n1\nTrue\n8\n\n\n12\nKNeighborsDist\n0.648045\n0.008390\n0.016957\n0.008390\n0.016957\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.648045\n0.030974\n0.549377\n0.030974\n0.549377\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html#d.-예측-predict",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).out.html#d.-예측-predict",
    "title": "[STBDA2023] 02wk-006: 타이타닉, Autogluon(Fsize)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr.eval('Fsize = SibSp + Parch'))).mean()\n\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7fd215321c10&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n0.9438832772166106\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst.eval('Fsize = SibSp + Parch'))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/07wk-029.out.html",
    "href": "posts/07wk-029.out.html",
    "title": "[STBDA2023] 07wk-029: 체중감량(교호작용) / 회귀분석",
    "section": "",
    "text": "07wk-029: 체중감량(교호작용) / 회귀분석\n최규빈\n2023-10-17\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-y6dmKt32J5hobALnT8wigT&si=ScK7ryQpemMS4cJd\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\nimport sklearn.model_selection\n\n\n\n3. Data\n\n# n = 10000\n# Supplement = np.random.choice([True, False], n)\n# Exercise = np.random.choice([False, True], n)\n# Weight_Loss = np.where(\n#     (~Supplement & (~Exercise)),\n#     np.random.normal(loc=0, scale=1, size=n),  \n#     np.where(\n#         (Supplement & (Exercise)),\n#         np.random.normal(loc=15.00, scale=1, size=n),\n#         np.where(\n#             (~Supplement & (Exercise)),\n#             np.random.normal(loc=5.00, scale=1, size=n),\n#             np.random.normal(loc=0.5, scale=1, size=n)\n#         )\n#     )\n# )\n# df = pd.DataFrame({\n#     'Supplement': Supplement,\n#     'Exercise': Exercise,\n#     'Weight_Loss': Weight_Loss\n# })\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\n\n\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n- 운동과 체중감량보조제를 병행하면 시너지가 나는 것 같음\n\n\n4. 분석\n- 분석1: 모형을 아래와 같이 본다. – 언더피팅\n\n\\({\\bf X}\\): Supplement, Exercise\n\\({\\bf y}\\): Weight_Loss\n\n\n# step1\nX = df_train[['Supplement','Exercise']]\ny = df_train['Weight_Loss']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n#---#\nprint(f'train score = {predictr.score(X,y):.4f}')\n\ntrain score = 0.8208\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n\n운동을 하면 10키로 감량효과가 있다고 추정하고 있음.\n보충제를 먹으면 5키로 감량효과가 있다고 추정하고 있음.\n대충 (10,5)의 숫자를 바꿔가면서 적합해봤는데 이게 최선이라는 의미임\n\n- 분석2: 모형을 아래와 같이 본다. – 딱 맞아요\n\n\\({\\bf X}\\): Supplement, Exercise, Supplement \\(\\times\\) Exercise\n\\({\\bf y}\\): Weight_Loss\n\n\nNote: 기본적인 운동의 효과 및 보조제의 효과는 각각 Supplement, Exercise 로 적합하고 운동과 보조제의 시너지는 Supplement\\(\\times\\)Exercise 로 적합한다.\n\n\n# step1 \nX = df_train.eval('Interaction = Supplement * Exercise')[['Supplement','Exercise','Interaction']]\ny = df_train['Weight_Loss']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n#---#\nprint(f'train score = {predictr.score(X,y):.4f}')\n\ntrain score = 0.9728\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n운동의 효과는 5정도 감량효과가 있다고 추정함.\n보충제를 먹으면 0.5키로 감량효과가 있다고 추정함.\n다만 운동을 하면서 보충제를 함께 먹을 경우 발생하는 추가적인 시너지효과가 9.5정도라고 추정하는 것임."
  },
  {
    "objectID": "posts/13wk-55.out.html",
    "href": "posts/13wk-55.out.html",
    "title": "[STBDA2023] 13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "",
    "text": "최규빈\n2023-12-01"
  },
  {
    "objectID": "posts/13wk-55.out.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13wk-55.out.html#a.-y의-분포-xy의-관계-시각화",
    "title": "[STBDA2023] 13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='charges',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\ncharges\n1338\n13270.422265\n12110.011237\n1121.8739\n4740.28715\n9382.033\n16639.912515\n63770.42801\nfloat64\n1337\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for charges &gt;= 0.5\n\n\n\n\n\nFeature interaction between smoker/charges in train_data\n\n\n\n\n\nFeature interaction between age/charges in train_data"
  },
  {
    "objectID": "posts/13wk-55.out.html#target-variable-analysis",
    "href": "posts/13wk-55.out.html#target-variable-analysis",
    "title": "[STBDA2023] 13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13wk-55.out.html#b.-중요한-설명변수",
    "href": "posts/13wk-55.out.html#b.-중요한-설명변수",
    "title": "[STBDA2023] 13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data=df_train,\n    label='charges',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231203_083619/\"\n\n\nModel Prediction for charges\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-4250.255585\n-4759.009823\n0.00177\n0.001411\n0.183049\n0.00177\n0.001411\n0.183049\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nsmoker\n11076.822803\n324.831922\n8.864565e-08\n5\n11745.656221\n10407.989386\n\n\nbmi\n3192.326797\n150.875820\n5.969502e-07\n5\n3502.982196\n2881.671399\n\n\nage\n2328.261296\n82.062303\n1.848886e-07\n5\n2497.228712\n2159.293881\n\n\nchildren\n77.495531\n20.011431\n4.892476e-04\n5\n118.699345\n36.291717\n\n\nregion\n39.652780\n26.510840\n1.435724e-02\n5\n94.238965\n-14.933406\n\n\nsex\n-21.116027\n18.796163\n9.670434e-01\n5\n17.585533\n-59.817587\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\ncharges_pred\nerror\n\n\n\n\n1019\n21\nfemale\n32.680\n2\nno\nnorthwest\n26018.95052\n5010.268555\n21008.681965\n\n\n140\n34\nmale\n22.420\n2\nno\nnortheast\n27375.90478\n6853.432129\n20522.472651\n\n\n1027\n23\nmale\n18.715\n0\nno\nnorthwest\n21595.38229\n2316.816650\n19278.565640\n\n\n526\n19\nfemale\n30.590\n2\nno\nnorthwest\n24059.68019\n5120.955078\n18938.725112\n\n\n1039\n19\nmale\n27.265\n2\nno\nnorthwest\n22493.65964\n4008.429199\n18485.230441\n\n\n959\n48\nmale\n36.670\n1\nno\nnorthwest\n28468.91901\n10121.377930\n18347.541080\n\n\n1008\n25\nmale\n24.985\n2\nno\nnortheast\n23241.47453\n5985.646484\n17255.828046\n\n\n539\n53\nmale\n31.350\n0\nno\nsoutheast\n27346.04207\n11388.868164\n15957.173906\n\n\n9\n60\nfemale\n25.840\n0\nno\nnorthwest\n28923.13692\n14113.998047\n14809.138873\n\n\n443\n59\nfemale\n36.520\n1\nno\nsoutheast\n28287.89766\n14497.187500\n13790.710160"
  },
  {
    "objectID": "posts/13wk-55.out.html#c.-관측치별-해석",
    "href": "posts/13wk-55.out.html#c.-관측치별-해석",
    "title": "[STBDA2023] 13wk-55: Medical Cost / 자료분석(Autogluon)",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n- 1번째 관측치\n\ndf_train.iloc[[1]]\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1\n18\nmale\n33.77\n1\nno\nsoutheast\n1725.5523\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[1]])\n\n1    4040.983398\nName: charges, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[1]],\n    display_rows=True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1\n18\nmale\n33.77\n1\nno\nsoutheast\n1725.5523\n\n\n\n\n\n\n\n\n\n\n- 6번째 관측치\n\ndf_train.iloc[[6]]\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n6\n46\nfemale\n33.44\n1\nno\nsoutheast\n8240.5896\n\n\n\n\n\n\n\n\npredictr.predict(df_train.iloc[[6]])\n\n6    9329.165039\nName: charges, dtype: float32\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[6]],\n    display_rows=True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n6\n46\nfemale\n33.44\n1\nno\nsoutheast\n8240.5896"
  },
  {
    "objectID": "posts/09wk-mid.out.html",
    "href": "posts/09wk-mid.out.html",
    "title": "[STBDA2023] 09wk-mid",
    "section": "",
    "text": "최규빈\n2023-11-01\nhttps://youtu.be/h5WnvWsqgtI?si=6-GRGyscR2_7BErA\n\n\nTrue/False를 판단하는 문제는 답만 써도 무방함.\n“자료분석” 문제는 “kaggle style score = 50%”, “분석의 논리 = 50%” 의 배점으로 채점한다.\n“자료분석” 유형의 경우 분석의 논리가 매우 우수하거나, 창의적인 접근법으로 분석을 시도할 경우 가산점을 부여한다. (아이디어 단계에서도 가산점 부여가능) 가산점은 문항점수의 최대 100%까지 부여한다.\n“연구” 문항의 경우 세부문항을 정확하게 수행한 경우에만 100%의 점수를 부여하며 이를 어길시 부분점수를 부여하지 않는다. 연구결과의 시각화나 해석이 우수한 답안은 문항점수의 20%까지 가산점을 부여한다.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing \nimport sklearn.model_selection\nimport sklearn.impute"
  },
  {
    "objectID": "posts/09wk-mid.out.html#아래의-자료를-해석하고-세부지침에-맞추어-분석하라.",
    "href": "posts/09wk-mid.out.html#아래의-자료를-해석하고-세부지침에-맞추어-분석하라.",
    "title": "[STBDA2023] 09wk-mid",
    "section": "(1) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.",
    "text": "(1) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/icesales_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/icesales_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 sales로 설정하고 나머지는 설명변수로 설정하라.\ndf_test에 sales에 대한 예측값을 포함하는 열을 추가하라.\n\n\n데이터 살펴보기\n\ndf_train.head()\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n0\n19.4\nchoco\n64.807407\n\n\n1\n0.9\nvanilla\n25.656697\n\n\n2\n7.4\nvanilla\n34.756650\n\n\n3\n4.5\nchoco\n27.265442\n\n\n4\n21.1\nchoco\n70.606946\n\n\n\n\n\n\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n9.6\nvanilla\n\n\n1\n17.4\nvanilla\n\n\n2\n21.1\nvanilla\n\n\n3\n21.5\nchoco\n\n\n4\n23.2\nchoco\n\n\n\n\n\n\n\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 280 entries, 0 to 279\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   temp    280 non-null    float64\n 1   type    280 non-null    object \n 2   sales   280 non-null    float64\ndtypes: float64(2), object(1)\nmemory usage: 6.7+ KB\n\n\n\n# plt.plot(df_train['temp'],df_train['sales'],'o')\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales,'o',label='choco')\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales,'o',label='vanilla')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f82bde397f0&gt;\n\n\n\n\n\n\n온도에 따라서 아이스크림의 판매량이 증가하고 있다.\n아이스크림 타입별로 구분이 무의미해 보인다. (교호작용이 없어 보임)\n\n\n\n데이터분석\n\n## step1\nX = pd.get_dummies(df_train[['temp','type']])\nXX = pd.get_dummies(df_test[['temp', 'type']])\ny = df_train[['sales']]\n## step2\npredictr = sklearn.linear_model.LinearRegression() \n## step3\npredictr.fit(X,y)\n## step4\nyhat = predictr.predict(XX)\n\n\ndf_test = df_test.assign(yhat=yhat)\ndf_test\n\n\n\n\n\n\n\n\ntemp\ntype\nyhat\n\n\n\n\n0\n9.6\nvanilla\n44.004419\n\n\n1\n17.4\nvanilla\n63.697748\n\n\n2\n21.1\nvanilla\n73.039456\n\n\n3\n21.5\nchoco\n73.876470\n\n\n4\n23.2\nchoco\n78.168606\n\n\n...\n...\n...\n...\n\n\n115\n5.1\nchoco\n32.469982\n\n\n116\n12.8\nvanilla\n52.083733\n\n\n117\n22.9\nchoco\n77.411170\n\n\n118\n8.4\nvanilla\n40.974676\n\n\n119\n6.0\nvanilla\n34.915190\n\n\n\n\n120 rows × 3 columns\n\n\n\n\nplt.plot(df_train['temp'],df_train['sales'],'o')\nplt.plot(df_test['temp'],df_test['yhat'], 'o')\n\n\n\n\n\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales_hat,'--',color='C1')\nplt.plot(df_train['temp'],df_train['sales'],'o')\nplt.plot(df_test['temp'],df_test['yhat'], 'o')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f82bdb6df10&gt;"
  },
  {
    "objectID": "posts/09wk-mid.out.html#아래의-자료를-해석하고-세부지침에-맞추어-분석하라.-1",
    "href": "posts/09wk-mid.out.html#아래의-자료를-해석하고-세부지침에-맞추어-분석하라.-1",
    "title": "[STBDA2023] 09wk-mid",
    "section": "(2) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.",
    "text": "(2) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n세부지침\n\n반응변수 \\({\\bf y}\\)를 height로 설정하고 나머지는 설명변수로 설정하라.\ndf_test에 height에 대한 예측값을 포함하는 열을 추가하라.\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_test.csv')\n\n\ndf_train\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n0\n71.169041\nmale\n180.906857\n\n\n1\n69.204748\nmale\n178.123281\n\n\n2\n49.037293\nfemale\n165.106085\n\n\n3\n74.472874\nmale\n177.467439\n\n\n4\n74.239599\nmale\n177.439925\n\n\n...\n...\n...\n...\n\n\n275\n72.105841\nmale\n180.579718\n\n\n276\n72.008144\nmale\n183.042456\n\n\n277\n48.589997\nfemale\n159.937014\n\n\n278\n80.803971\nmale\n183.308227\n\n\n279\n42.634990\nfemale\n153.325140\n\n\n\n\n280 rows × 3 columns\n\n\n\n\n데이터 살펴보기\n- df_train 정보 확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 280 entries, 0 to 279\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   weight  225 non-null    float64\n 1   sex     280 non-null    object \n 2   height  280 non-null    float64\ndtypes: float64(2), object(1)\nmemory usage: 6.7+ KB\n\n\n\ndf_train.describe()\n\n\n\n\n\n\n\n\nweight\nheight\n\n\n\n\ncount\n225.000000\n280.000000\n\n\nmean\n65.773407\n174.605431\n\n\nstd\n14.014774\n9.430102\n\n\nmin\n40.047170\n148.975298\n\n\n25%\n51.157668\n167.572671\n\n\n50%\n69.270680\n175.186487\n\n\n75%\n76.642564\n181.132612\n\n\nmax\n93.505001\n195.797169\n\n\n\n\n\n\n\n\ndf_train.groupby('sex').describe().T\n\n\n\n\n\n\n\n\nsex\nfemale\nmale\n\n\n\n\nweight\ncount\n82.000000\n143.000000\n\n\nmean\n49.567061\n75.066557\n\n\nstd\n4.067440\n7.858717\n\n\nmin\n40.047170\n57.802771\n\n\n25%\n46.981788\n70.126587\n\n\n50%\n49.372279\n74.953426\n\n\n75%\n52.217482\n80.492811\n\n\nmax\n59.052006\n93.505001\n\n\nheight\ncount\n102.000000\n178.000000\n\n\nmean\n165.234795\n179.975122\n\n\nstd\n6.059964\n6.294408\n\n\nmin\n148.975298\n162.860389\n\n\n25%\n161.318427\n176.045502\n\n\n50%\n165.276189\n180.058265\n\n\n75%\n168.794055\n183.693824\n\n\nmax\n183.625644\n195.797169\n\n\n\n\n\n\n\n\ndf_train[df_train['weight'].isna()].head()\n\n\n\n\n\n\n\n\nweight\nsex\nheight\n\n\n\n\n10\nNaN\nmale\n181.022441\n\n\n11\nNaN\nfemale\n168.038585\n\n\n14\nNaN\nfemale\n160.925313\n\n\n16\nNaN\nmale\n170.981140\n\n\n29\nNaN\nfemale\n173.825733\n\n\n\n\n\n\n\n\ndf_train에는 NaN값이 55개 있다. NaN값을 처리하기 이전에, 해당 데이터가 어떻게 되어있는지 nan값을 제외하고 시각화를 통해 확인해보자.\n\n\ndf_train_n = df_train.dropna().reset_index(drop=True)\n\n\nplt.plot(df_train_n['weight'], df_train_n['height'], 'o')\n\n\n\n\n\n시각화를 해보았을 때, 성별 구분을 하지 않고 데이터를 살펴보면 일차 선형 추세보다 2차 추세가 있어보인다.\n\n\n# step1\nX = pd.get_dummies(df_train_n[['weight','sex']])\ny = df_train_n['height']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train_n['height_hat'] = predictr.predict(X)\n\n\nplt.plot(df_train_n[df_train_n.sex=='male'].weight,df_train_n[df_train_n.sex=='male'].height,'o',label='male')\nplt.plot(df_train_n[df_train_n.sex=='male'].weight,df_train_n[df_train_n.sex=='male'].height_hat,'--')\nplt.plot(df_train_n[df_train_n.sex=='female'].weight,df_train_n[df_train_n.sex=='female'].height,'o',label='female')\nplt.plot(df_train_n[df_train_n.sex=='female'].weight,df_train_n[df_train_n.sex=='female'].height_hat,'--')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f82bde89d90&gt;\n\n\n\n\n\n\n성별을 구분하여 시각화를 해보았을때, 성별에 따라 키와 몸무게에 영향을 끼쳐 보인다.\n여성의 경우 이상값들이 몇 개 보인다. ——&gt; 추후 NaN값을 대치할 때, 성별로 구분하고 이상값들은 제외하고 Impute하는게 좋을 것 같다.\n\n\ndf_test.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 120 entries, 0 to 119\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   weight  95 non-null     float64\n 1   sex     120 non-null    object \ndtypes: float64(1), object(1)\nmemory usage: 2.0+ KB\n\n\n\ndf_test데이터에도 weight값이 있다.\n\n\nlen(df_test[df_test['sex'] == 'female'])\n\n50\n\n\n\ndf_test.describe()\n\n\n\n\n\n\n\n\nweight\n\n\n\n\ncount\n95.000000\n\n\nmean\n64.545461\n\n\nstd\n14.916583\n\n\nmin\n38.830724\n\n\n25%\n50.107303\n\n\n50%\n67.385533\n\n\n75%\n77.459491\n\n\nmax\n94.486442\n\n\n\n\n\n\n\n\ndf_test.groupby('sex').describe().T\n\n\n\n\n\n\n\n\nsex\nfemale\nmale\n\n\n\n\nweight\ncount\n40.000000\n55.000000\n\n\nmean\n48.780459\n76.010917\n\n\nstd\n4.820260\n7.249173\n\n\nmin\n38.830724\n59.387691\n\n\n25%\n45.780719\n73.739790\n\n\n50%\n48.173989\n76.400941\n\n\n75%\n51.791133\n81.496914\n\n\nmax\n60.793693\n94.486442\n\n\n\n\n\n\n\n\n\n결측값 처리 후 데이터 분석\n\n- 방법1 —-&gt; drop\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_test.csv')\n\n\ndf_train_n = df_train.dropna().reset_index(drop=True)\ndf_test_n = df_test.dropna().reset_index(drop=True)\n\n\n## step1\nX = pd.get_dummies(df_train_n[['weight','sex']])\nXX = pd.get_dummies(df_test_n[['weight', 'sex']])\ny = df_train_n[['height']]\n## step2\npredictr = sklearn.linear_model.LinearRegression() \n## step3\npredictr.fit(X,y)\n## step4\nyhat = predictr.predict(XX)\n\n\npredictr.score(X,y)\n\n0.8957997115030892\n\n\n\ndf_test_n['yhat'] = yhat\ndf_test_n\n\n\n\n\n\n\n\n\nweight\nsex\nyhat\n\n\n\n\n0\n74.405638\nmale\n179.442840\n\n\n1\n45.054381\nfemale\n160.702172\n\n\n2\n74.040455\nmale\n179.136628\n\n\n3\n59.655090\nmale\n167.074268\n\n\n4\n48.318398\nfemale\n163.439103\n\n\n...\n...\n...\n...\n\n\n90\n82.496975\nmale\n186.227556\n\n\n91\n38.830724\nfemale\n155.483536\n\n\n92\n48.029580\nfemale\n163.196924\n\n\n93\n77.731773\nmale\n182.231857\n\n\n94\n80.193741\nmale\n184.296257\n\n\n\n\n95 rows × 3 columns\n\n\n\n\nplt.plot(df_train_n['weight'], df_train_n['height'], 'o')\nplt.plot(df_test_n['weight'], df_test_n['yhat'],'x')\n\n\n\n\n\n시각화를 확인해보면, 여성의 yhat은 기울기가 좀더 가파라야 할 것 같다.\n\n\n\n- 방법2 —-&gt; 평균(성별구분) 대치\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_test.csv')\n\n- 평균값으로 nan값 대치\n\nfemale_mean = df_train[df_train['sex'] == 'female']['weight'].mean()\nmale_mean = df_train[df_train['sex'] == 'male']['weight'].mean()\n(female_mean, male_mean)\n\n(49.567060917121516, 75.06655705102078)\n\n\n\ndf_train.loc[(df_train['sex'] == 'female') & (df_train['weight'].isna()), 'weight'] = female_mean\ndf_train.loc[(df_train['sex'] == 'male') & (df_train['weight'].isna()), 'weight'] = male_mean\ndf_test.loc[(df_test['sex'] == 'female') & (df_test['weight'].isna()), 'weight'] = female_mean\ndf_test.loc[(df_test['sex'] == 'male') & (df_test['weight'].isna()), 'weight'] = male_mean\n\n\nX = pd.get_dummies(df_train[['weight','sex']])\nXX = pd.get_dummies(df_test[['weight', 'sex']])\ny = df_train[['height']]\npredictr = sklearn.linear_model.LinearRegression() \npredictr.fit(X,y)\nyhat = predictr.predict(XX)\n\n\npredictr.score(X,y)\n\n0.8543631938325703\n\n\n\ndf_test['yhat'] = yhat\ndf_test\n\n\n\n\n\n\n\n\nweight\nsex\nyhat\n\n\n\n\n0\n74.405638\nmale\n179.420931\n\n\n1\n45.054381\nfemale\n161.450840\n\n\n2\n74.040455\nmale\n179.114719\n\n\n3\n59.655090\nmale\n167.052359\n\n\n4\n48.318398\nfemale\n164.187771\n\n\n...\n...\n...\n...\n\n\n115\n48.029580\nfemale\n163.945592\n\n\n116\n49.567061\nfemale\n165.234795\n\n\n117\n75.066557\nmale\n179.975122\n\n\n118\n77.731773\nmale\n182.209948\n\n\n119\n80.193741\nmale\n184.274348\n\n\n\n\n120 rows × 3 columns\n\n\n\n\nplt.plot(df_train['weight'], df_train['height'], 'o')\nplt.plot(df_test['weight'], df_test['yhat'],'x')\n\n\n\n\n\n시각화를 확인해보면, drop한 시각화 결과값과 비슷해 보임\n결측치를 모두 평균값으로 대치하다 보니 분산이 더 커진 느낌\n\n\n\n- 방법3 —-&gt; drop 이후 이차선형을 해보자.\n\n데이터를 처음 살펴보았을 때, 데이터가 2차 선형 추세로 보였다. 결측값을 방법1에서 진행한 drop으로 처리하고 이차선형을 진행해보자\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_test.csv')\n\ndf_train_n = df_train.dropna().reset_index(drop=True)\ndf_test_n = df_test.dropna().reset_index(drop=True)\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# step 1\nX = pd.get_dummies(df_train_n[['weight', 'sex']])\nXX = pd.get_dummies(df_test_n[['weight', 'sex']])\ny = df_train_n['height']\n\n# step 2\npoly = PolynomialFeatures(degree=2)\nX_ = poly.fit_transform(X)\nXX_ = poly.fit_transform(XX)\n\n# step 3\n\npredictr = LinearRegression()\npredictr.fit(X_, y)\n\n# Step 4: 예측\nyhat = predictr.predict(XX_)\n\n\ndf_test_n['yhat'] = yhat\ndf_test_n\n\n\n\n\n\n\n\n\nweight\nsex\nyhat\n\n\n\n\n0\n74.405638\nmale\n179.541829\n\n\n1\n45.054381\nfemale\n158.629433\n\n\n2\n74.040455\nmale\n179.260442\n\n\n3\n59.655090\nmale\n167.990971\n\n\n4\n48.318398\nfemale\n162.879433\n\n\n...\n...\n...\n...\n\n\n90\n82.496975\nmale\n185.716853\n\n\n91\n38.830724\nfemale\n150.474275\n\n\n92\n48.029580\nfemale\n162.504119\n\n\n93\n77.731773\nmale\n182.094037\n\n\n94\n80.193741\nmale\n183.970729\n\n\n\n\n95 rows × 3 columns\n\n\n\n\nplt.plot(df_train_n['weight'], df_train_n['height'], 'o')\nplt.plot(df_test_n['weight'], df_test_n['yhat'],'x')\n\n\n\n\n\n그래프로 확인했을 때 좀 더 잘 적합된것 같다. (성별구분도 잘 되어 잇는거 같다.)\n\n\n\n- 방법4 —-&gt; 이상값을 제거한 평균(성별구분) 대치 (설명만..)\n\n여자의 데이터를 살펴보면, 이상값들이 있어보인다. 이상값들을 제거한 새로운 df값에서 결측치를 처리하고 분석하는 것이 나아보인다.\n이상값을 제거하기 위한 방법으로 standardscaler를 이용해 적합하는 방법도 있고, 혹은 결측치를 제거한 값에서 적합한 선형모델의 신뢰구간을 구하고 신뢰구간에서 벗어난 값들을 이상치로 추정하여 제거하는 방법도 있을 것 같다.\n제거 후에 결측값을 처리한다.(평균 등..)"
  },
  {
    "objectID": "posts/09wk-mid.out.html#아래의-자료를-해석하고-세부지침에-맞추어-분석하라.-2",
    "href": "posts/09wk-mid.out.html#아래의-자료를-해석하고-세부지침에-맞추어-분석하라.-2",
    "title": "[STBDA2023] 09wk-mid",
    "section": "(3) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.",
    "text": "(3) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 y로 설정하고 나머지 X1,X2,X3,X4는 설명변수로 설정하라.\ndf_test에 y에 대한 예측값을 포함하는 열을 추가하라.\n\n\n데이터 살펴보기\n\n결측값은 없다.\n\n\ndf_train.corr()\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\ny\n\n\n\n\nX1\n1.000000\n0.076188\n0.081018\n-0.016346\n0.022207\n\n\nX2\n0.076188\n1.000000\n0.908744\n-0.076805\n0.799140\n\n\nX3\n0.081018\n0.908744\n1.000000\n-0.083198\n0.912513\n\n\nX4\n-0.016346\n-0.076805\n-0.083198\n1.000000\n-0.027986\n\n\ny\n0.022207\n0.799140\n0.912513\n-0.027986\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df_train.corr(),annot=True, fmt=\".7f\")  # 숫자 보여주는 옵션\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n히트맵으로 확인해 보니 (X2,X3) 의 corr이 높아보인다.\n종속변수 y와는 X2와 X3가 corr이 높아보인다.\n히트맵으로 확인했을 때, X2와 X3중 하나만 남겨도 좋을 것 같다.\n\n\n\n데이터분석\n\n데이터에 다중공산성이 있어 보인다. ridge와 lasso를 사용해보자\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_test.csv')\n\n## step1 \nX = df_train.drop(['y'],axis=1)\nXX = df_test\ny = df_train[['y']]\n\n## step2\npredictr = sklearn.linear_model.RidgeCV()\n\n## step3 \npredictr.fit(X,y)\n\n## step4\nyhat = predictr.predict(XX)\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\n\ntrain_score: 0.8420\n\n\n\n(predictr.coef_, predictr.intercept_)\n\n(array([[-2.33391464, -1.66406577, 13.52591532,  2.19008175]]),\n array([-10.69072811]))\n\n\n\npredictr.coef_.sum()\n\n11.718016648676223\n\n\n\ndf_test['yhat'] = yhat\ndf_test\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nyhat\n\n\n\n\n0\n0.848150\n2.120672\n2.464867\n0.418109\n18.056099\n\n\n1\n0.577140\n2.339529\n2.704477\n0.277340\n21.257076\n\n\n2\n0.968286\n1.570282\n2.236010\n0.791880\n16.414684\n\n\n3\n0.065605\n1.332800\n1.591895\n0.175372\n8.854202\n\n\n4\n0.997962\n2.216314\n3.020515\n0.932561\n26.189631\n\n\n...\n...\n...\n...\n...\n...\n\n\n115\n0.543678\n1.008793\n2.192418\n0.751022\n17.660934\n\n\n116\n0.693956\n1.978183\n2.719313\n0.267207\n21.764218\n\n\n117\n0.405942\n0.997256\n1.913545\n0.364545\n13.383162\n\n\n118\n0.160230\n0.253617\n1.435839\n0.139073\n8.238895\n\n\n119\n0.080532\n0.235460\n1.123899\n0.211455\n4.394358\n\n\n\n\n120 rows × 5 columns\n\n\n\n\n릿지를 이용했을 때 X3의 계수값이 다른 계수들 값에 비해 큰 값을 가진다.\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_test.csv')\n\n## step1 \nX = df_train.drop(['y'],axis=1)\nXX = df_test\ny = df_train[['y']]\n\n## step2\npredictr = sklearn.linear_model.LassoCV()\n\n## step3 \npredictr.fit(X,y)\n\n## step4\nyhat = predictr.predict(XX)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:1568: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\n\ntrain_score: 0.8321\n\n\n\n(predictr.coef_, predictr.intercept_)\n\n(array([-0.        , -0.        , 11.85387587,  0.        ]),\n -9.006433120791534)\n\n\n\npredictr.coef_.sum()\n\n11.853875870263957\n\n\n\ndf_test['yhat'] = yhat\ndf_test\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nyhat\n\n\n\n\n0\n0.848150\n2.120672\n2.464867\n0.418109\n20.211794\n\n\n1\n0.577140\n2.339529\n2.704477\n0.277340\n23.052106\n\n\n2\n0.968286\n1.570282\n2.236010\n0.791880\n17.498951\n\n\n3\n0.065605\n1.332800\n1.591895\n0.175372\n9.863691\n\n\n4\n0.997962\n2.216314\n3.020515\n0.932561\n26.798372\n\n\n...\n...\n...\n...\n...\n...\n\n\n115\n0.543678\n1.008793\n2.192418\n0.751022\n16.982217\n\n\n116\n0.693956\n1.978183\n2.719313\n0.267207\n23.227970\n\n\n117\n0.405942\n0.997256\n1.913545\n0.364545\n13.676486\n\n\n118\n0.160230\n0.253617\n1.435839\n0.139073\n8.013828\n\n\n119\n0.080532\n0.235460\n1.123899\n0.211455\n4.316122\n\n\n\n\n120 rows × 5 columns\n\n\n\n\n라쏘를 이용했을 때 X3외에 다른 계수 값들은 다 0으로 추정하였다.\n릿지, 라쏘를 확인했을 때 X2보다 X3가 더 유의미한 변수로 보인다. X2를 제거해 보자.\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_test.csv')\n\n## step1 \nX = df_train.drop(['y','X2'],axis=1)\nXX = df_test.drop(['X2'],axis=1)\ny = df_train[['y']]\n\n## step2\npredictr = sklearn.linear_model.LinearRegression() \n\n## step3 \npredictr.fit(X,y)\n\n## step4\nyhat = predictr.predict(XX)\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\n\ntrain_score: 0.8376\n\n\n\n(predictr.coef_, predictr.intercept_)\n\n(array([[-2.74165853, 12.28614251,  2.58080696]]), array([-9.88442786]))\n\n\n\ndf_test['yhat'] = yhat\ndf_test\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nyhat\n\n\n\n\n0\n0.848150\n2.120672\n2.464867\n0.418109\n19.152999\n\n\n1\n0.577140\n2.339529\n2.704477\n0.277340\n22.476606\n\n\n2\n0.968286\n1.570282\n2.236010\n0.791880\n16.976486\n\n\n3\n0.065605\n1.332800\n1.591895\n0.175372\n9.946554\n\n\n4\n0.997962\n2.216314\n3.020515\n0.932561\n26.896733\n\n\n...\n...\n...\n...\n...\n...\n\n\n115\n0.543678\n1.008793\n2.192418\n0.751022\n17.499593\n\n\n116\n0.693956\n1.978183\n2.719313\n0.267207\n22.312462\n\n\n117\n0.405942\n0.997256\n1.913545\n0.364545\n13.453517\n\n\n118\n0.160230\n0.253617\n1.435839\n0.139073\n7.676122\n\n\n119\n0.080532\n0.235460\n1.123899\n0.211455\n4.248884\n\n\n\n\n120 rows × 5 columns\n\n\n\n\nX2를 제거한 선형 모델이 릿지, 라쏘를 이용한 모델과 비슷한 계수값의 합이 보인다. 즉 X2와 X3는 유의한 변수로 서로 영향을 끼친다."
  },
  {
    "objectID": "posts/12wk-045.out.html",
    "href": "posts/12wk-045.out.html",
    "title": "[STBDA2023] 12wk-045: 아이스크림 / 부스팅",
    "section": "",
    "text": "최규빈\n2023-11-21"
  },
  {
    "objectID": "posts/12wk-045.out.html#a.-재현의-확인",
    "href": "posts/12wk-045.out.html#a.-재현의-확인",
    "title": "[STBDA2023] 12wk-045: 아이스크림 / 부스팅",
    "section": "A. 재현의 확인",
    "text": "A. 재현의 확인\n- 아이디어:\n\n처음부터 yhat을 강하게 학습하지 않고 약하게 조금씩 학습하자.\n부족한 공부는 (=학습이 덜 되어있는 부분 =y-yhat)은 조금씩 강화하면서 보완하자.\n\n- 구현: my_trees, my_residuals를 직접구현\n\nmy_trees = [] \nmy_residuals = [] \n\n\nres = y - y.mean()\n# 첫공부 \nfor i in range(100):\n    tree = sklearn.tree.DecisionTreeRegressor(\n        criterion = 'friedman_mse',\n        max_depth=3\n    )\n    tree.fit(X,res)\n    yhat = tree.predict(X) \n    res = res - yhat * 0.1 # 학습한걸 다 반영하지 말고 0.1정도만 반영. 여기서 0.1은 학습율\n    my_trees.append(tree)\n    my_residuals.append(res)\n\n- 비교: my_trees와 trees의 비교 (고정된 \\(i\\))\n\ni=10\nfig = plt.figure()\nax = fig.subplots(2,2)\nax[0,0].plot(X,y,'o',alpha=0.5)\nax[0,0].plot(X,ensemble(my_trees,i))\nax[0,1].plot(X,y,'o',alpha=0.5)\nax[0,1].plot(X,ensemble(trees,i))\nsklearn.tree.plot_tree(my_trees[i],max_depth=0,ax=ax[1,0]);\nsklearn.tree.plot_tree(trees[i],max_depth=0,ax=ax[1,1]);\n\n\n\n\n- 비교: my_trees와 trees의 비교 (애니메이션)\n\n#i=10\nfig = plt.figure()\nax = fig.subplots(2,2)\nplt.close()\n#---#\ndef func(i):\n    ax[0,0].clear()\n    ax[0,0].plot(X,y,'o',alpha=0.5)\n    ax[0,0].plot(X,ensemble(my_trees,i))\n    #--#\n    ax[0,1].clear()\n    ax[0,1].plot(X,y,'o',alpha=0.5)\n    ax[0,1].plot(X,ensemble(trees,i))\n    #--#\n    ax[1,0].clear()\n    sklearn.tree.plot_tree(my_trees[i],max_depth=0,ax=ax[1,0]);\n    #--#\n    ax[1,1].clear()\n    sklearn.tree.plot_tree(trees[i],max_depth=0,ax=ax[1,1]);\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=100)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/12wk-045.out.html#b.-step별-분석",
    "href": "posts/12wk-045.out.html#b.-step별-분석",
    "title": "[STBDA2023] 12wk-045: 아이스크림 / 부스팅",
    "section": "B. Step별 분석",
    "text": "B. Step별 분석\n\nfig,ax = plt.subplots(1,4,figsize=(10,3))\nplt.close()\n\n\ndef func(i):\n    ax[0].clear();\n    ax[0].plot(X,y,'o',alpha=0.5)\n    ax[0].plot(X,ensemble(my_trees,i),'--')\n    ax[0].set_title(\"Step0\")\n    ax[1].clear();\n    ax[1].set_ylim(-20,20)\n    ax[1].plot(X,my_residuals[i],'o',alpha=0.5)\n    ax[1].set_title(\"Step1:Residual\")\n    ax[2].clear();\n    ax[2].set_ylim(-20,20)\n    ax[2].plot(X,my_residuals[i],'o',alpha=0.5)\n    ax[2].plot(X,my_trees[i].predict(X),'--')\n    ax[2].set_title(\"Step2:Fit\")\n    ax[3].clear();\n    ax[3].plot(X,y,'o',alpha=0.5)\n    ax[3].plot(X,ensemble(my_trees,i),'--',color='C1')\n    ax[3].plot(X,ensemble(my_trees,i+1),'--',color='C3')\n    ax[3].set_title(\"Step3:Update\")        \n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames = 50\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n관찰1: “Step1: Residual”은 점점 단순오차차럼 변화한다.\n관찰2: “Step2: Fit”의 분기점들은 고정된 값이 아니다. (계속 변한다)\n관찰3: “Step3: Update” 업데이터되는 양은 반복이 진행될수록 점점 작아진다.\n\n- 위의 그림에서\n\nStep0: 공부할 자료, 현재까지 공부량\nStep1: 남은 공부량\nStep2: 공부! (이해O / 암기X)\nStep3: 공부의 10%의 기억.. 기억나는 것만 두뇌에 update되어있음.\n\n- 느낌: 조금씩 데이터를 학습한다. 학습할 자료가 오차항처럼 보인다면? 그때는 적합을 멈춘다. (오차항을 적합할 필요는 없잖아?)"
  },
  {
    "objectID": "posts/03wk-012.out.html",
    "href": "posts/03wk-012.out.html",
    "title": "[STBDA2023] 03wk-012: 취업, 로지스틱",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/03wk-012.out.html#a.-데이터-정리",
    "href": "posts/03wk-012.out.html#a.-데이터-정리",
    "title": "[STBDA2023] 03wk-012: 취업, 로지스틱",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df[['toeic','gpa']])\ny = df[['employment']]\n\n\nX\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n...\n...\n...\n\n\n495\n280\n4.288465\n\n\n496\n310\n2.601212\n\n\n497\n225\n0.042323\n\n\n498\n320\n1.041416\n\n\n499\n375\n3.626883\n\n\n\n\n500 rows × 2 columns"
  },
  {
    "objectID": "posts/03wk-012.out.html#b.-predictor-starstarstar",
    "href": "posts/03wk-012.out.html#b.-predictor-starstarstar",
    "title": "[STBDA2023] 03wk-012: 취업, 로지스틱",
    "section": "B. Predictor (\\(\\star\\star\\star\\))",
    "text": "B. Predictor (\\(\\star\\star\\star\\))\n- 여기가 중요함. \\(y\\)가 연속형이 아니라 범주형(취업/미취업)으로 이루어진 경우는 sklearn.linear_model.LogisticRegression() 이용하여 predictor를 만들 것\n\npredictr = sklearn.linear_model.LogisticRegression()\n\n\npredictr\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-012.out.html#c.-학습",
    "href": "posts/03wk-012.out.html#c.-학습",
    "title": "[STBDA2023] 03wk-012: 취업, 로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-012.out.html#d.-예측",
    "href": "posts/03wk-012.out.html#d.-예측",
    "title": "[STBDA2023] 03wk-012: 취업, 로지스틱",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X) \n\narray([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1])"
  },
  {
    "objectID": "posts/03wk-012.out.html#e.-평가",
    "href": "posts/03wk-012.out.html#e.-평가",
    "title": "[STBDA2023] 03wk-012: 취업, 로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.882\n\n\n\n(predictr.predict(X) == y.employment).mean()\n\n0.882\n\n\n\nplt.plot(df.toeic,df.gpa,'o')\ndf_filtered = df[predictr.predict(X)==1]\nplt.plot(df_filtered.toeic,df_filtered.gpa,'o') \n\n\n\n\n\n이 정도면 합리적임"
  },
  {
    "objectID": "posts/05wk-021.out.html",
    "href": "posts/05wk-021.out.html",
    "title": "[STBDA2023] 05wk-021: 취업+밸런스게임, 오버피팅",
    "section": "",
    "text": "해당 자료는 전북대학교 최규빈 교수님 2023학년도 2학기 빅데이터분석특강 자료임"
  },
  {
    "objectID": "posts/05wk-021.out.html#a.-학부12학년-수준의-설명",
    "href": "posts/05wk-021.out.html#a.-학부12학년-수준의-설명",
    "title": "[STBDA2023] 05wk-021: 취업+밸런스게임, 오버피팅",
    "section": "A. 학부1~2학년 수준의 설명",
    "text": "A. 학부1~2학년 수준의 설명\n- 과적합(Overfitting): 머신러닝과 통계에서 자주 나타나는 문제로, 모델이 학습데이터에 과도하게 최적화가 되어서 실제로 새로운 데이터나 테스트데이터에서 성능이 저하되는 현상을 말함.\n- 오버피팅의 원인:\n\n불필요한 특징: 불필요한 특징이 데이터에 포함되어 있다면 오버피팅이 발생할 수 있음."
  },
  {
    "objectID": "posts/05wk-021.out.html#b.-일반인-수준의-설명",
    "href": "posts/05wk-021.out.html#b.-일반인-수준의-설명",
    "title": "[STBDA2023] 05wk-021: 취업+밸런스게임, 오버피팅",
    "section": "B. 일반인 수준의 설명",
    "text": "B. 일반인 수준의 설명\n- 시험 공부(1): 공부를 하랬더니 외우고 있음..\n- 시험 공부(2): (시험 하루 전날에) 공부 그만하고 술이나 먹으러 가자.. 더 공부하면 train error만 줄일 뿐이야..\n- 운전: 특정도로에서만 운전연습을 했음. 그래서 그 도로의 구멍, 곡률, 신호등의 위치까지 완벽하게 숙지하였음. 그 결과 그 도로에서는 잘하게 되었지만, 그 도로 이외의 다른도로에서 운전을 한다면 문제가 발생함.\n- 언어: 특정 주제나 특정 상황에 대한 대화만을 반복적으로 연습하여, 그 상황에서는 완벽한 대화가 가능하지만 그 외의 상황에서는 대화를 제대로 이어나갈 수 없음."
  },
  {
    "objectID": "posts/11wk-043.out.html",
    "href": "posts/11wk-043.out.html",
    "title": "[STBDA2023] 11wk-043: 아이스크림 판매량 / 배깅",
    "section": "",
    "text": "최규빈\n2023-11-16"
  },
  {
    "objectID": "posts/11wk-043.out.html#a.-원리",
    "href": "posts/11wk-043.out.html#a.-원리",
    "title": "[STBDA2023] 11wk-043: 아이스크림 판매량 / 배깅",
    "section": "A. 원리",
    "text": "A. 원리\n- 알고리즘\n\n80개의 샘플에서 80개를 중복을 허용하여 뽑는다.\n1에서 뽑힌 샘플들을 이용하여 tree를 적합시킨다.\n1-2를 10번 반복하고 10개의 tree의 평균값을 yhat으로 선택한다."
  },
  {
    "objectID": "posts/11wk-043.out.html#b.-plot_tree-체크",
    "href": "posts/11wk-043.out.html#b.-plot_tree-체크",
    "title": "[STBDA2023] 11wk-043: 아이스크림 판매량 / 배깅",
    "section": "B. plot_tree 체크",
    "text": "B. plot_tree 체크\n- 10개의 트리들의 리스트\n\ntrees = predictr.estimators_\ntrees\n\n[DecisionTreeRegressor(random_state=1644635363),\n DecisionTreeRegressor(random_state=1304269235),\n DecisionTreeRegressor(random_state=1794000214),\n DecisionTreeRegressor(random_state=1273087880),\n DecisionTreeRegressor(random_state=995922005),\n DecisionTreeRegressor(random_state=1372517728),\n DecisionTreeRegressor(random_state=1087222928),\n DecisionTreeRegressor(random_state=3687756),\n DecisionTreeRegressor(random_state=1772778467),\n DecisionTreeRegressor(random_state=92158766)]\n\n\n- 재표본데이터셋\n\npredictr.estimators_samples_[0] # (X,y)의 쌍을 80개 중복을 허용하여 뽑기 위한 인덱스\n\narray([19, 10, 25, 29, 50,  7, 46, 31, 10, 39, 78, 14, 54, 79, 28, 35, 73,\n        0, 74, 72, 66, 36, 55, 24, 41, 11, 68, 65, 71, 36, 54, 41, 76, 34,\n        0, 59,  5,  7, 67, 61, 64, 21, 27, 26, 43, 55, 49, 23, 29, 27, 41,\n       14, 58,  5, 12, 40, 12, 38,  8, 19, 63,  4, 35, 75, 64,  9, 69, 17,\n       32, 15, 60, 55, 18, 55, 22, 73, 28, 48, 57, 63])\n\n\n\nsamples = predictr.estimators_samples_\n\n- 첫번째 트리 재현\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[0],\n    feature_names=X.columns,\n    max_depth=1\n);\n\n\n\n\n\nX_array = np.array(X)\ny_array = np.array(y)\n\n\ntree = sklearn.tree.DecisionTreeRegressor()\ntree.fit(X_array[samples[0]],y_array[samples[0]])\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nsklearn.tree.plot_tree(\n    tree,\n    feature_names=X.columns,\n    max_depth=1\n);\n\n\n\n\n- tree 비교 (고정된 \\(i\\))\n\ni=4\nfig, ax = plt.subplots(2,1)\n#---#\nsklearn.tree.plot_tree(\n    predictr.estimators_[i],\n    feature_names=X.columns,\n    max_depth=1,\n    ax=ax[0]\n)\nax[0].set_title('predictr.estimator')\n#---#\nmy_tree = sklearn.tree.DecisionTreeRegressor()\nmy_tree.fit(X_array[samples[i]],y_array[samples[i]])\nsklearn.tree.plot_tree(\n    my_tree,\n    feature_names=X.columns,\n    max_depth=1,\n    ax=ax[1]\n);\nax[1].set_title('my_tree')\n\nText(0.5, 1.0, 'my_tree')\n\n\n\n\n\n- tree 비교 (애니메이션)\n\nfig, ax = plt.subplots(2,1)\nplt.close()\n#---#\ndef func(i):\n    ax[0].clear()\n    sklearn.tree.plot_tree(\n        predictr.estimators_[i],\n        feature_names=X.columns,\n        max_depth=1,\n        ax=ax[0]\n    )\n    ax[0].set_title('predictr.estimator')\n    #---#\n    ax[1].clear()\n    my_tree = sklearn.tree.DecisionTreeRegressor()\n    my_tree.fit(X_array[samples[i]],y_array[samples[i]])\n    sklearn.tree.plot_tree(\n        my_tree,\n        feature_names=X.columns,\n        max_depth=1,\n        ax=ax[1]\n    );\n    ax[1].set_title('my_tree')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/11wk-043.out.html#c.-resampling-fit",
    "href": "posts/11wk-043.out.html#c.-resampling-fit",
    "title": "[STBDA2023] 11wk-043: 아이스크림 판매량 / 배깅",
    "section": "C. ReSampling + Fit",
    "text": "C. ReSampling + Fit\n- 고정된 \\(i\\)\n\ni=4\nplt.plot(X,y,'o',alpha=0.2,color='gray')\nplt.plot(X_array[samples[i]],y_array[samples[i]],'o',alpha=1/3)\nplt.plot(X,trees[i].predict(X),'--')\n\n\n\n\n- 애니매이션\n\n#---#\nfig = plt.figure()\nax = fig.gca() \nplt.close()\n#---#\ndef func(i):\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.2,color='gray')\n    ax.plot(X_array[samples[i]],y_array[samples[i]],'o',alpha=1/3)\n    ax.plot(X,trees[i].predict(X),'--')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/11wk-043.out.html#d.-앙상블결과-재현",
    "href": "posts/11wk-043.out.html#d.-앙상블결과-재현",
    "title": "[STBDA2023] 11wk-043: 아이스크림 판매량 / 배깅",
    "section": "D. 앙상블결과 재현",
    "text": "D. 앙상블결과 재현\n- 최종결과물 (손으로..)\n\npredictr.predict(X)\n\narray([11.88782962, 14.05941305, 15.02231867, 18.03161729, 19.62619066,\n       19.86214551, 15.84293717, 15.95940294, 15.95940294, 20.30137042,\n       20.30137042, 22.51278676, 22.51278676, 23.68899036, 20.7954938 ,\n       26.45727462, 26.45727462, 20.48421278, 20.48421278, 25.08188452,\n       25.08188452, 25.08188452, 31.42611771, 25.99393577, 25.99393577,\n       25.99393577, 27.05912187, 27.05912187, 29.60439358, 29.94005816,\n       29.18760881, 29.18760881, 30.75340115, 30.82608162, 32.48384789,\n       31.03678302, 29.02978839, 31.17487146, 31.17487146, 31.05349512,\n       29.147739  , 29.147739  , 29.147739  , 30.40843883, 30.40843883,\n       33.53154643, 34.26668831, 33.20982041, 33.20982041, 36.82818648,\n       36.82818648, 34.66545508, 34.66545508, 34.24047203, 33.0829342 ,\n       33.0829342 , 35.29894866, 35.50366771, 35.47938512, 35.47938512,\n       38.8116606 , 38.8116606 , 37.74794717, 34.84063828, 39.73515434,\n       40.01130524, 40.05274675, 41.9980937 , 42.26869452, 40.81707653,\n       40.16985211, 41.5373848 , 39.69311797, 42.97563198, 45.99122302,\n       49.35681519, 43.64765096, 45.32629064, 47.10042494, 46.28105912])\n\n\n\nnp.stack([tree.predict(X) for tree in predictr.estimators_]).mean(axis=0)\n\narray([11.88782962, 14.05941305, 15.02231867, 18.03161729, 19.62619066,\n       19.86214551, 15.84293717, 15.95940294, 15.95940294, 20.30137042,\n       20.30137042, 22.51278676, 22.51278676, 23.68899036, 20.7954938 ,\n       26.45727462, 26.45727462, 20.48421278, 20.48421278, 25.08188452,\n       25.08188452, 25.08188452, 31.42611771, 25.99393577, 25.99393577,\n       25.99393577, 27.05912187, 27.05912187, 29.60439358, 29.94005816,\n       29.18760881, 29.18760881, 30.75340115, 30.82608162, 32.48384789,\n       31.03678302, 29.02978839, 31.17487146, 31.17487146, 31.05349512,\n       29.147739  , 29.147739  , 29.147739  , 30.40843883, 30.40843883,\n       33.53154643, 34.26668831, 33.20982041, 33.20982041, 36.82818648,\n       36.82818648, 34.66545508, 34.66545508, 34.24047203, 33.0829342 ,\n       33.0829342 , 35.29894866, 35.50366771, 35.47938512, 35.47938512,\n       38.8116606 , 38.8116606 , 37.74794717, 34.84063828, 39.73515434,\n       40.01130524, 40.05274675, 41.9980937 , 42.26869452, 40.81707653,\n       40.16985211, 41.5373848 , 39.69311797, 42.97563198, 45.99122302,\n       49.35681519, 43.64765096, 45.32629064, 47.10042494, 46.28105912])\n\n\n- 최종결과물 (코드로 정리)\n\ndef ensemble(trees,i=None):\n    if i is None:\n        i = len(trees)\n    else: \n        i = i+1\n    yhat = np.stack([tree.predict(X) for tree in trees[:i]]).mean(axis=0)\n    return yhat\n\n\nensemble(trees,0) # 0번트리만 적용\n\narray([10.90026146, 10.90026146, 10.90026146, 19.46336233, 19.46336233,\n       20.31785349, 16.3076088 , 16.3076088 , 16.3076088 , 20.27763408,\n       20.27763408, 21.52796629, 21.52796629, 21.52796629, 18.34698175,\n       27.5369675 , 27.5369675 , 20.30881248, 20.30881248, 25.04963215,\n       25.04963215, 25.04963215, 32.42440294, 26.49340711, 26.49340711,\n       26.49340711, 26.40925726, 26.40925726, 29.55903213, 30.75418385,\n       29.70592592, 29.70592592, 31.45007539, 32.89828946, 32.89828946,\n       31.12503261, 25.9552363 , 33.12203011, 33.12203011, 30.60313283,\n       29.45886461, 29.45886461, 29.45886461, 30.60789344, 30.60789344,\n       30.60789344, 36.5245913 , 34.24458444, 34.24458444, 37.4829917 ,\n       37.4829917 , 37.4829917 , 37.4829917 , 31.13974993, 31.13974993,\n       31.13974993, 31.13974993, 36.58400962, 35.1723381 , 35.1723381 ,\n       39.75311187, 39.75311187, 39.75311187, 34.68877582, 44.47780794,\n       39.1744058 , 40.19626989, 42.86734269, 42.60143843, 40.80476673,\n       40.80476673, 42.1996627 , 38.72741866, 41.43992372, 45.95732063,\n       50.81374143, 42.30473921, 42.30473921, 48.7391566 , 46.00793717])\n\n\n\nensemble(trees,1) # 0번트리,1번트리의 예측값 평균\n\narray([10.90026146, 12.45139248, 12.45139248, 18.56852168, 19.46336233,\n       20.31785349, 16.03419127, 16.57964463, 16.57964463, 21.02420483,\n       21.02420483, 21.3736233 , 21.3736233 , 23.07741787, 22.94197463,\n       27.5369675 , 27.5369675 , 19.83347885, 19.83347885, 26.16305209,\n       26.16305209, 26.16305209, 32.42440294, 28.7554569 , 28.7554569 ,\n       28.7554569 , 27.61337612, 27.61337612, 29.55903213, 30.75418385,\n       28.54972991, 28.54972991, 31.45007539, 30.82608162, 30.82608162,\n       31.66094517, 29.07604701, 32.65944392, 32.65944392, 30.60313283,\n       29.40787056, 29.40787056, 29.40787056, 30.5566788 , 30.5566788 ,\n       33.57934676, 36.5245913 , 35.63234869, 35.63234869, 37.25155232,\n       37.25155232, 35.85263528, 35.85263528, 32.46466946, 33.6755663 ,\n       33.6755663 , 35.78403852, 35.87817386, 35.1723381 , 35.1723381 ,\n       40.62427057, 40.62427057, 40.62427057, 34.68877582, 44.47780794,\n       41.82610687, 40.50051831, 41.83605471, 41.70310258, 40.80476673,\n       39.92694722, 40.6243952 , 40.08367119, 41.43992372, 43.69862217,\n       50.81374143, 42.30473921, 43.9833789 , 48.7391566 , 47.37354689])"
  },
  {
    "objectID": "posts/11wk-043.out.html#e.-학습과정-시각화",
    "href": "posts/11wk-043.out.html#e.-학습과정-시각화",
    "title": "[STBDA2023] 11wk-043: 아이스크림 판매량 / 배깅",
    "section": "E. 학습과정 시각화",
    "text": "E. 학습과정 시각화\n- 고정된 \\(i\\)\n\ni=9\nfig,ax = plt.subplots(1,4,figsize=(8,2))\n#--#\nax[0].set_title(\"Step0\")\nax[0].plot(X,y,'o',color='gray',alpha=0.2)\n#--#\nax[1].set_title(\"Step1:ReSampling\")\nax[1].plot(X,y,'o',color='gray',alpha=0.2)\nax[1].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n#--#\nax[2].set_title(\"Step2:Fit\")\nax[2].plot(X,y,'o',color='gray',alpha=0.2)\nax[2].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\nax[2].plot(X,trees[i].predict(X),'--')\n#--#\nax[3].set_title(\"Step3:Update(?)\")\nax[3].plot(X,y,'o',color='gray',alpha=0.2)\nax[3].plot(X,ensemble(trees,i),'--',color='C1')\n\n\n\n\n- 애니메이션\n\nfig,ax = plt.subplots(1,4,figsize=(8,2))\nplt.close()\n#---#\ndef func(i):\n    for a in ax:\n        a.clear()\n    #--#\n    ax[0].set_title(\"Step0\")\n    ax[0].plot(X,y,'o',color='gray',alpha=0.2)\n    #--#\n    ax[1].set_title(\"Step1:ReSampling\")\n    ax[1].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[1].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n    #--#\n    ax[2].set_title(\"Step2:Fit\")\n    ax[2].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[2].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n    ax[2].plot(X,trees[i].predict(X),'--')\n    #--#\n    ax[3].set_title(\"Step3:Update(?)\")\n    ax[3].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[3].plot(X,ensemble(trees,i),'--',color='C1')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/12wk-046.out.html",
    "href": "posts/12wk-046.out.html",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "",
    "text": "최규빈\n2023-11-21"
  },
  {
    "objectID": "posts/12wk-046.out.html#a.-accuracy",
    "href": "posts/12wk-046.out.html#a.-accuracy",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "A. Accuracy",
    "text": "A. Accuracy\n- 느낌: 정답율\n- 정의: \\(\\text{accuracy}=\\frac{TP+TN}{total}=\\frac{\\#O/O+ \\#X/X}{total}\\)\n\n한국말로는 정확도, 정분류율이라고 한다.\n한국말이 헷갈리므로 그냥 영어를 외우는게 좋다.\n\n- (상확극 시점1) 왜 애큐러시는 불충분한가?\n\n회사: 퇴사자예측프로그램 개발해. 잘 맞출수록 좋고, 계산에 많은 시간이 걸리지 않는 가벼운 predictor일수록 좋아.\n최규빈: 귀찮은데 다 안나간다고 하자! -&gt; 99퍼의 accuracy\n\n\n모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다?"
  },
  {
    "objectID": "posts/12wk-046.out.html#b.-tpr-recallsensitivity",
    "href": "posts/12wk-046.out.html#b.-tpr-recallsensitivity",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "B. TPR (=Recall=Sensitivity)",
    "text": "B. TPR (=Recall=Sensitivity)\n- 느낌: 이상상황 감지율!\n- 정의: \\(\\text{recall}=\\text{sensitivity}=\\text{TPR}=\\frac{TP}{TP+FN}=\\frac{\\# O/O}{\\# O/O+\\# O/X}\\)\n\n분모: 실제 O인 관측치 수\n분자: 실제 O를 O라고 예측한 관측치 수\n뜻: 실제 O를 O라고 예측한 비율\n\n- (상황극 시점2) 우리가 recall을 봐야하는 이유\n\n인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!)\n최규빈: 계산빠르고 잘 맞추는 predictor 만들어 달라면서요? 제 predictor는 계산시간은 0이고, accuracy는 0.99에요.\n인사팀: (고민중..) 사실 생각해보니까 이 경우는 accuracy는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 recall을 보겠다!\n\n\n예시1: 실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 recall=0.5.\n\n\n예시2: 최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우는 recall=0.\n\n\n인사팀 결론: 우리가 필요한건 recall 이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다.)"
  },
  {
    "objectID": "posts/12wk-046.out.html#c.-precision",
    "href": "posts/12wk-046.out.html#c.-precision",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "C. Precision",
    "text": "C. Precision\n- 느낌: 적중률..\n- 정의: \\(\\text{precision}=\\frac{TP}{TP+FP}=\\frac{\\# O/O}{\\# O/O+\\# X/O}\\)\n\n분모: O라고 예측한 관측치\n분자: O라고 예측한 관측치중 진짜 O인 관측치\n뜻: O라고 예측한 관측치중 진짜 O인 비율\n\n- (상황극 시점3) Recall 만으로 불충분한 이유\n\n최규빈: 에휴.. 귀찮은데 그냥 좀 만 수틀리면 다 나갈것 같다고 해야겠다. -&gt; 한 100명 나간다고 했음 -&gt; 실제로 최규빈이 찍은 100명중에 10명이 다 나감!\n\n\n이 경우 accuracy는 91%, recall은 100% (퇴사자 10명을 일단은 다 맞췄으므로).\n\n\n인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요.\n인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요.\n최규빈: 저번 요청사항이 accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요? 제 predictor는 accuracy가 91% 그리고 recall은 100% 입니다.\n인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. 여기에서 precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 \\(\\frac{10}{100}\\)이니까 precision이 10%입니다.\n인사팀 속마음: Recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!"
  },
  {
    "objectID": "posts/12wk-046.out.html#d.-f1-score",
    "href": "posts/12wk-046.out.html#d.-f1-score",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "D. F1-score",
    "text": "D. F1-score\n- 느낌: 적중률과 민감도의 균형\n- 정의: Recall과 precision의 조화평균\n- (상황극 시점4) recall, precision을 모두 고려\n\n최규빈: 그런데 recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다.\n최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까?\n인사팀: 그렇다면 둘을 평균내서 F1-score를 계산해서 제출해주세요.\n\n\n속마음: AUC 쓰면 더 좋은뎅.."
  },
  {
    "objectID": "posts/12wk-046.out.html#e.-specificity",
    "href": "posts/12wk-046.out.html#e.-specificity",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "E. Specificity",
    "text": "E. Specificity\n- 느낌: 안나갈 사람을 안나갔다고 찾아낸 비율\n- 정의: \\(\\text{specificity}=\\frac{TN}{FP+TN}=\\frac{\\# X/X}{\\# X/O+\\# X/X}\\)\n- 안중요함"
  },
  {
    "objectID": "posts/12wk-046.out.html#f.-fpr",
    "href": "posts/12wk-046.out.html#f.-fpr",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "F. FPR",
    "text": "F. FPR\n- 느낌: 생사람 잡은 사람의 비율 (오해해서 미안하다, TPR올릴려다 보니까 어쩔수 없었네)\n\nTPR(=recall)을 올리는 가장 편한방법은, 조금만 이상한 징조가 있어도 “너 나갈것 같은데?”라고 몰아가는 것이다. 하지만 이러한 방식은 적중률의 문제가 있다. 또한 멀쩡히 잘 다니고 있는 사람을 생트집 잡아 몰아간다는 문제도 있다.\n\n- 정의: \\(\\text{FPR}=1-\\text{specificity}=\\frac{FP}{FP+TN}\\)=\\(\\frac{\\# X/O}{\\# X/O+\\# X/X}\\)\n- 예시: 아래의 상황을 고려하자.\n\n총 회사원 1000명.\n최규빈 연구원이 “퇴사할것 같은 사람”으로 찍은 직원은 100명.\n실제 퇴사자 10명, 실제퇴사자 10중 최규빈 연구원이 나간다고 한 사람은 8명.\n실제 퇴사자중 최규빈 연구원이 눈치채지 못한 사람은 2명.\n최규빈은 92명의 생사람을 잡음..\n\n정리하면\n\nO/O = 8\nO/X = 2\nX/O = 92\nX/X = 898\n\n\nrecall = TPR = 8/10 # OO/(OO+OX)\n\n\nprecision = 8/100 # OO/(OO+XO)\n\n\nFPR = 92/990 # XO/(XO+XX) # 분모의미: 억울한사람이 되려면 일단 열시미 다니고 있어야함\n\n\nprecision, FPR # 비슷해보이는데 좀 다르죠?\n\n(0.08, 0.09292929292929293)"
  },
  {
    "objectID": "posts/12wk-046.out.html#f.-roc-curve",
    "href": "posts/12wk-046.out.html#f.-roc-curve",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "F. ROC curve",
    "text": "F. ROC curve\n- 휴식: 무빙.. (이영미 교수님[1]이 재밋다해서..)\nhttps://youtu.be/iV6x1HbwU00?si=G93t_vAuMWyJWPGA\n\npredictor: 박병은\n박과장!! 약속과 다르지 않소!! –&gt; 사살 –&gt; 진짜 간첩이었으므로 사살하는 순간 precision/recall 동시에 상승\n동해물과 백두산이… –&gt; 사살 –&gt; 이 사람은 진짜 간첩인지 아닌지 확실하지 않음. 그렇지만 사살함. 따라서 recall은 올라갈 수는 있겠으나 precision 하락 및 FPR 상승의 위험이 있음.\n이런 미친새끼.. 여기서 억울한 휴민트들이 얼마나.. –&gt; 너처럼 계속 classification하면 FPR이 너무 높아진다는 의미\n괜찮아.. 내가 알아서 할게 –&gt; 난 FPR 높아도 괜찮아, 위험감수하고 TPR 올리고 싶어. 이런 내 결정은 내가 알아서 책임질게\n\n- 소감: 총을 얼마나 민감하게 당길지를 결정함에 따라서 FPR과 TRP의 값이 수시로 달라진다.\n- 정의: \\(x\\)축=FPR, \\(y\\)축=TPR 을 그린 커브\n- 의미:\n\n“x=생사람잡은비율” vs “y=이상상황감지율=recall”을 그린 곡선이 ROC커브이다.\n생각해보면 생사람을 잡을 수록 (=\\(x\\)값이 커질수록) 감지율이 높아진다 (=\\(y\\)값도 커진다).\n생사람 잡은 비율이 매우 적은데 (=\\(x\\)값이 작은데) 감지율이 높으면 매우 우수한 predictor 이다.\n따라서 초반부터 ROC curve가 급격하게 상승하면 좋은 predictor 이다."
  },
  {
    "objectID": "posts/12wk-046.out.html#g.-auc",
    "href": "posts/12wk-046.out.html#g.-auc",
    "title": "[STBDA2023] 12wk-046: 평가지표의 이해",
    "section": "G. AUC",
    "text": "G. AUC\n- 정의: ROC curve의 밑면적"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Boram-coco",
    "section": "",
    "text": "Everyday with Coco"
  }
]